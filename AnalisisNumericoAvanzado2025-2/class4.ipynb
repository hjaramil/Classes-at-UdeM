{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPAFv5RCFS+AxeMXfKrSeor"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Revisar error en clase anterior.**\n","\n","\n","**Teorema 1.2.2**: Si $A$ es estrictamente diagonal dominante por filas (columnas) entonces la iteracion Gauss-Seidel\n","\n","$$\\mathbf{x}_{k+1} = T \\mathbf{x}_k + \\mathbf{c} $$\n","\n","Con\n","$T=-(L+D)^{-1} U$ y $L$ triangular inferior con ceros en la diagonal\n","$U$ triangular superior con ceros en la digonal y $D$ diagonal (sin ceros),\n","**converge**.\n","\n","**prueba**:\n","Asumamos que $A$ es diagonalmente dominante por filas. Comenzamos por trasformar la matriz $T$ de la siguiente forma.\n","\n","\\begin{eqnarray}\n","T &=& -(L+D)^{-1} U  \\\\\n","(L + D) T &=& -U   \\\\\n","DT &=& - U - LT \\\\\n","T &=& - D^{-1}(U + LT).\n","\\end{eqnarray}\n","\n","Vamos a probar que $\\| T \\|_{\\infty} < 1$ y por lo tanto $\\rho(T) < 1$\n","\n","Lo vamos a hacer por induccion.\n","Para $i=1$ la primera fila de $L$ tiene solo ceros.  De forma\n","$(LT)_1 = 0$  y la primera componente de $\\| T \\mathbf{x} \\|$ cumple\n","\n","$$| ( T \\mathbf{x} )_1 | =  \\left | \\frac{1}{a _{11}} \\sum_{j=2}^n | a_{1j}| | x_j |   \\right |  \\le \\frac{1}{ | a_{11} |} \\sum_{j=2}^n | a_{1j}  | | x_j |\n","\\le \\frac{1}{ | a_{11} |} \\sum_{j=2}^n | a_{1j} | \\| \\mathbf{x} \\|_\\infty  \\le  \\| \\mathbf{x} \\|_\\infty  \\frac{1}{ | a_{11} |} \\sum_{j=2}^n | a_{1 j} | < \\| x \\|_\\infty  $$\n","\n","\n","\n","\n","\n"],"metadata":{"id":"tpbfWAeSQ5Aq"}},{"cell_type":"markdown","source":["Ahora asumimos para $i$,  $| T(\\mathbf{x})_i | < \\| \\mathbf{x}_\\infty \\|$\n","Probamos que tambien ocurre en $i+1$.\n","\n","\\begin{eqnarray}\n","| (T \\mathbf{x})_{i+1} | &=& \\frac{1}{| a_{i+1 \\; i+1} |} \\left |  \\sum_{j=i+2}^n a_{i+1} x_j + \\sum_{j=1}^i a_{i+1} (T \\mathbf{x})_j \\right |  \\\\\n","&=&  \\frac{1}{| a_{i+1 \\; i+1} |} \\left (  \\sum_{j=i+2}^n | a_{i+1 j} |  | x_j | + \\sum_{j=1}^i | a_{i+1} |  | (T \\mathbf{x})_j | \\right )  \\\\\n","&\\le& \\frac{1}{ | a_{i+1 \\; i+1} | } \\sum_{j=1, j  \\ne i+1}^n |a_{i+1 \\; j} |\n","\\| \\mathbf{x} \\|_\\infty \\\\\n","&<& \\| x \\|_\\infty.\n","\\end{eqnarray}\n","\n","De forma general decimos que las componentes $| T \\mathbf{x}_i  | $\n","est'an acotadas por $\\| \\mathbf{x} \\|_\\infty$ de forma que\n","\n","\n","$$\\| T \\mathbf{x} \\|_\\infty  < \\| \\mathbf{x} \\|_\\infty  \\tag{1} $$\n","\n","Por definicion de norma de una matriz\n","\n","$$\\| T \\|_\\infty = \\sup \\left \\{ \\frac{\\| T \\mathbf{x} \\|}{\\| \\mathbf{x} \\|}    , \\mathbf{x} \\ne 0 \\right \\} = \\sup_{\\| u \\|=1} \\{  \\| T \\mathbf{u} \\|_\\infty \\}  \\tag{2} $$\n","De (1) y (2)\n","\n","$$\\| T \\|_\\infty < 1 $$\n","y como $\\rho(T) < \\| T \\|_\\infty$ entonces\n","$\\rho(T) < 1$ y el metodo de Gauss-Seidel converge para matrices diagonalmente dominantes por fila.\n","\n","\n"],"metadata":{"id":"Bgr4TSFAmF0v"}},{"cell_type":"markdown","source":["## Tecnicas de relajacion (Relaxation) para resolver sistemas de Ecuaciones\n","La idea es buscar un metodo que supere a Jacobi y Gauss-Seidel.\n","\n","Recuerde las iteraciones\n","\n","$$\\mathbf{x}_{k+1} = T \\mathbf{x}_k + \\mathbf{c}. $$\n","\n","La idea de\n","David M. Young en 1950 como parte de su tesis doctoral es que se tienen dos soluciones parciales $\\mathbf{x}_k, \\mathbf{x}_{k+1}$. Sera que **existe** una solucion mejor entre estas dos? o en el camino de ellas.\n","\n","Es decir que existe $\\mathbf{y}$ tal que\n","\n","$$\\mathbf{y} = (1 - \\omega) \\mathbf{x}_k + \\omega \\mathbf{x}_{k+1} . $$\n","\n","Note que si $w=1$, $\\mathbf{y} = \\mathbf{x}_{k+1}$ , si $\\omega = 0$,\n","$\\mathbf{y} = \\mathbf{x}_k$\n","\n","Que valores debe tener $\\omega$?  No debe ser negativo por que?\n","\n","$$\\mathbf{y} = \\mathbf{x}_k + \\omega \\Delta \\mathbf{x}_k $$\n","Si queremos avanzar en la \"direccion correcta\" queremos que $\\omega > 0$,\n","asumiendo que $\\Delta$ esta en la direccion correcta. Creo que en un momento probaremos que $\\omega > 0$.\n","Bamos a ver que $0 < \\omega < 2$. Cuando $\\omega < 1$ se llama \"sub-relajacion\" y cuando $\\omega>1$ se llama \"sobre-relajacion\" (under-relaxation/over-relaxation). Cuando $\\omega=1$ se llama relajaci'on.\n","\n","Veamos el caso de Gauss-Seidel con relajacion.\n","\n","## Metodos de relajacion basados en la iteracion Gauss-Seidel\n","Asumimos $T=T_{GS}$ y escribimos\n","\n","$$\\mathbf{y} = (1 - \\omega) \\mathbf{x}_k + \\omega (T_{GS} \\mathbf{x}_k + \\mathbf{c})$$\n","\n","$$\\mathbf{y} = (1 - \\omega) \\mathbf{x}_k + \\omega T_{GS} \\mathbf{x}_k + w \\mathbf{c}$$\n","Recuerde que es $T_{GS}$.\n","$$\\mathbf{x}_{k+1} = (L + D)^{-1} ( -U \\mathbf{x}_k + \\mathbf{b}) \\tag{3} $$\n","\n","En este caso tenemos que\n","\n","$$\\mathbf{y} = (1 - \\omega) \\mathbf{x}_k  - \\omega (L+D)^{-1} U \\mathbf{x}_k\n","+ \\omega (L+ D)^{-1} \\mathbf{b}$$\n","\n","Pensamos ahora que $\\mathbf{y}=\\mathbf{x}_{k+1}$.\n","\n","$$\\mathbf{x}_{k+1} = (1 - \\omega) \\mathbf{x}_k  - \\omega (L+D)^{-1} U \\mathbf{x}_k$$\n","\n","En componentes tenemos (ver clase anterior)\n","\n","$$x_i^{(k+1)} =  (1 - \\omega) x_i^{(k)} + \\frac{\\omega}{a_{ii}}\n","\\left (  b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(k)} \\right )  $$\n","\n","Vamos a vectorizar esta ecuacion.\n","Multiplicamos por $a_{ii}$ a ambos lados\n","\n","\n","$$a_{ii} x_i^{(k+1)} =  a_{ii} (1 - \\omega) x_i^{(k)} + \\omega\n","\\left (  b_i - \\sum_{j=1}^{i-1} a_{ij} x_j^{(k+1)} - \\sum_{j=i+1}^n a_{ij} x_j^{(k)} \\right )  $$\n","\n","Escribimos esto en forma matricial\n","\n","$$D \\mathbf{x}_{k+1} =  D(1 - \\omega) \\mathbf{x}_k + \\omega \\mathbf{b} - \\omega L \\mathbf{x}_{k+1} - \\omega U \\mathbf{x}_k $$\n","\n","Es decir\n","$$(D + \\omega L) \\mathbf{x}_{k+1}  = [ (1 - \\omega ) D - \\omega U] \\mathbf{x}_k + \\omega \\mathbf{b}  $$\n","\n","O\n","\n","$$\\mathbf{x}_{k+1} = (D + \\omega L)^{-1} [ (1 - \\omega) D - \\omega U)] \\mathbf{x}_k + (D + \\omega L)^{-1} \\omega \\mathbf{b}  $$\n","\n","Tenemos entonces que\n","\n","$$\\mathbf{x}_{k+1} = T_\\omega \\mathbf{x}_k + \\mathbf{c} $$\n","donde\n","\n","$$T_\\omega =  (D + \\omega L)^{-1} [ (1 - \\omega) D - \\omega U)] \\quad, \\quad \\mathbf{c} = (D + \\omega L)^{-1} \\omega \\mathbf{b}   $$\n","\n","Que pasa si $\\omega=1$?. El sistema se reduce a Guass-Seidel. (verificar, ver Ecuacion (3)).                                        \n","\n","\n","Vamos a probar que para convergencia es necesario que $0 < \\omega < 2$.\n","\n","**Teorema 1.3.1**(Kahan): Si $a_{ii} \\ne 0$, $i=1,2 \\cdots ,n$, entonces\n","$\\rho(T_\\omega) > | \\omega - 1 | $. Esto implica (como veremos que para convergencia SOR debemos gener\n","\n","$$ 0 < \\omega < 2. $$\n","\n","de $(AB)^{-1} = B^{-1} A^{-1}$.\n","\n","**Prueba**: Factorizamos $D$ por fuera de la matriz $T_\\omega$.\n","\n","\\begin{eqnarray}\n","T_\\omega &=&  (D + \\omega L)^{-1} [ (1 - \\omega) D - \\omega U)]  \\\\\n","&=& [D( I + \\omega D^{-1} L)]^{-1}   [ (1 - \\omega) D - \\omega U)] \\\\\n","&=& (I + \\omega D^{-1} L)^{-1} D^{-1}  [ (1 - \\omega) D - \\omega U)]  \\\\\n","&=&  (I + \\omega D^{-1} L)^{-1} [(1 - \\omega) I - \\omega D^{-1} U ].\n","\\end{eqnarray}\n","\n","La matriz $D^{1}$ existe por que todos los elementos de la diagonal son distintos de 0. Vamos a tomar el determinante de $T_{\\omega}$.\n","\n","De las dos matrices grandes que multiplican arriba, la primera tiene determinante 1 por que $ \\omega D^{-1} L$ es triangular inferior con 0 en la digonal. De la misma forma la segunda tiene determinante $(1-\\omega)^n$ pues\n","\n","\n","\n","\n","\n"],"metadata":{"id":"yYWrTfnOrBoj"}},{"cell_type":"markdown","source":["$D^{-1} U%$ es triangular superior con 0 en la diagonal.\n","\n","Es decir como $\\det(AB)= \\det(A) \\det(B)$ entonces\n","\n","$$\\det(T_\\omega) = (1 - \\omega)^n $$\n","Recuerde que\n","\n","$$\\rho(T_\\omega)= \\max_{i \\le j \\le n} \\{ | \\lambda_j |: \\lambda_j \\quad \\text{es autovalor de} \\quad  A  \\} $$\n","\n","El determinante de $T_\\omega= \\lambda_1 \\lambda_2 \\cdots \\lambda_n = \\prod_{i=1}^n \\lambda_i \\le (\\lambda_j)^n  $\n","\n","\n","donde $\\lambda_j$ es el maximo $| \\lambda_j | = \\rho(T_\\omega)$\n","\n","Es decir\n","\n","$$(1 - \\omega)^n \\le \\rho(T_\\omega)^n $$\n","\n","Entonces sacando raiz $1/n$ esima\n","\n","$$ | 1 - \\omega  | \\le \\rho(T_\\omega) $$\n","\n","Para convergencia necesitamos que $\\rho(T_\\omega) < 1$\n","De forma que tenemos\n","\n","$$ | 1 - \\omega  | < 1 $$\n","Es decir\n","\n","$$-1 < \\omega - 1 <  1 $$\n","Sumando 1 a ambos lados\n","\n","$$0 <  \\omega <  2$$\n","\n","Esta condicion es necesaria para la convegencia. Ahora Ostrowski probo que es suficiente.\n","\n","**Teorema 1.3.2** (Ostrowski). Asuma que la matriz $A$ del sistema\n","$A \\mathbf{x} = \\mathbf{b}$ es positiva definida y $0 < \\omega < 2$. Entonces la iteraci'on con matriz $T_\\omega$\n","\n","$$ T_\\omega =  (D + \\omega L)^{-1} [ (1 - \\omega) D - \\omega U)]  $$\n","converge.\n","\n","Como $A$ es positiva definida y por ende simetrica, los valores de la diagonal son positivos. Veamos esto. Que sea positiva definida quiere decir que\n","$\\mathbf{x}^T A \\mathbf{x} > 0$, Para todo $\\mathbf{x} \\in \\mathbb{R}$.  En\n","particular ecojamos el vector $\\mathbf{e}_i$ de la base canonica, el cual tiene 1 en la posicion $i$ y 0 en las demas, entonces\n","\n","$$\\mathbf{e}_i^T A \\mathbf{e}_i = a_{ii} $$\n","\n","\n"],"metadata":{"id":"YaQXGeZg7u1z"}},{"cell_type":"markdown","source":["Como $A$ es simetrica $U=L^T$ recuerde ($A = L  + D+ U$)\n","Construimos dos matrices $B$ y $C$ tales que $T_\\omega= B^{-1} C$.\n","\n","Estas son\n","\n","$$B = \\frac{1}{\\omega} (D + \\omega L) \\quad , \\quad C= \\frac{1}{\\omega}[ (1 - \\omega)D - \\omega L^T)]  $$\n","\n","Tenemos entonces que\n","\n","\\begin{eqnarray}\n","T_\\omega &=& B^{-1}  \\frac{1}{\\omega}[ (1 - \\omega)D - \\omega L^T)] \\\\\n","&=& B^{-1} \\left [ \\frac{1}{\\omega} D - D - L^T   \\right ]  \\\\\n","&=& B^{-1} \\left [ \\frac{1}{\\omega} D  + L - L - D - L^T   \\right ]  \\\\\n","&=& B^{-1} \\left [ \\frac{1}{\\omega} (D  + \\omega L) - L - D - L^T   \\right ]  \\\\\n","&=& B^{-1} \\left [ \\frac{1}{\\omega} (D  + \\omega L) - A \\right ] \\\\\n","&=& B^{-1} (B - A) \\\\\n","&=& I - B^{-1} A.\n","\\end{eqnarray}\n","\n","Recuerde que $A=A^T$.\n","\n","\\begin{eqnarray}\n","A - T_\\omega^T A T_\\omega &=&  A - (I - B^{-1} A)^T A (I - B^{-1} A) \\\\\n","&=& A - \\left [  A -  A B^{-1} A - (B^{-1} A)^T A + (B^{-1} A)^T A B^{-1} A   \\right ] \\\\\n","&=& (B^{-1} A)^T A + A B^{-1} A - (B^{-1} A)^T A B^{-1} A \\\\\n","&=& (B^{-1} A)^T I A + (A^T I) B^{-1} A - (B^{-1} A)^T A B^{-1} A \\\\\n","&=& (B^{-1} A)^T B B^{-1} A + A^T (B^{-1})^T B^T B^{-1} A - (B^{-1} A)^T A B^{-1} A \\\\\n","&=& (B^{-1} A)^T B B^{-1} A + (B^{-1} A)^T B^T B^{-1} A - (B^{-1} A)^T A B^{-1} A \\\\\n","&=& (B^{-1} A)^T  (  B + B^T - A  )   B^{-1} A\n","\\end{eqnarray}\n","\n","Ahora bien, miremos el centro\n","\n","\\begin{eqnarray}\n","B + B^T - A &=&\n","\\frac{1}{\\omega} D + L + \\frac{1}{\\omega}D + U - A \\\\\n","&=&\n","\\frac{1}{\\omega} D + L + \\frac{1}{\\omega}D + U - L - D - U \\\\\n","&=&  \\frac{1}{\\omega} (2 - \\omega ) D\n","\\end{eqnarray}\n","\n","\n","Como $0 < \\omega < 2$, entonces $B^T + B - A$ es simetrica positiva definida.\n","Vemos que $A - T_\\omega A T_\\omega$ es tambien simetrica positiva definida.\n","\n","Asuma $\\lambda \\in \\mathbb{C}$ un autovalor de $T_\\omega$\n","\n","Tenemos que\n","\n","$$ \\mathbf{x}^* ( A - T_\\omega^T A T_\\omega) \\mathbf{x} > 0 $$\n","\n","$$\\mathbf{x}^* A \\mathbf{x} > x^* T_\\omega^T A T\\omega \\mathbf{x} = (\\lambda \\mathbf{x})^* A ( \\lambda \\mathbf{x} ) = | \\lambda |^2 \\mathbf{x}^* A \\mathbf{x}  $$\n","\n","De la ecuacion anterior $| \\lambda |^2  < 1$, y por lo tanto\n","\n","$$\\rho(T_\\omega) < 1$$\n","Y por lo tanto el sistema con matriz $T_\\omega$, converge."],"metadata":{"id":"t1zUdFxm_1Si"}}]}