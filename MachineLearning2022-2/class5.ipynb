{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+HfO6/Sb2N9dSgZSjHVzi"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["DEl teorema de SVD, cualquier matriz $A$ se puede factorizar de la forma $A=U \\Sigma V^T$. donde $\\Sigma$ es diagonal con los valores singulares ordenados de mayor a menor $\\sigma_1^2 \\ge \\sigma_2^2 \\ge \\sigma_r^2 \\ge 0 \\ge 0 \\cdots 0$. Si hay cereos es por que $A$ es singular, sino es por que es no-singular (existe $A^{-1}$. Las matrices $U, V$ son ortogonales (es deicr $U^*U=I$ ($m \\times m$), $V^* V=I$ ($ n \\times n$). \n","\n","De aca sacamos una definicion. El **numero de condicion** (condition number)  de una matriz (condition number) es\n","\n","$$ \\kappa = \\frac{\\sigma_1}{\\sigma_r} $$\n","donde $r$ es el indice del mas valor singular mas pequeno, o $\\infty$ si 0 es el valor singular mas pequeno.  Se el numero de condicion es 1 la matriz es super-estable. Si la matriz es ortogonal (ejercicio) entonces el numero de condicion es 1. Si el numero de condicion es grande $10^k$ $k \\gg 1$, sosn malas noticias. La matriz es inestable. Tambien se llama **ill possed**. El problema $Ax = b$ es un problema mal condicionado (ill possed). \n","\n","## Cociente de Rayleigh:\n","Definicion: Dada una matriz $A$ autoadjunta, podemos definir el **coeciente de Rayleigh** de $A$ con respecto al vector $x$ como:\n","\n","$$ R(A, x) =  \\frac{x^* A x}{x^* x} = \\frac{\\langle x, A x \\rangle}{\\| x\\|^2} = \\frac{\\langle Ax, x \\rangle}{\\| x \\|^2} $$\n","\n","Tambien podemos escribir\n","\n","$$R(A,x) =  \\frac{x^*}{\\| x \\|} A \\frac{x}{ \\| x \\|} = u^* A u $$\n","donde $u$ es un vector unitario en la direccion de $x$.\n","\n","Teorema: El autovalor mas grande de una matriz auto-adjunta $n \\times n$ es el valor mas grande del cociente de Rayleigh de la matriz en $\\mathbb{C}^n$. \n","\n","Prueba: Recuerde el teorema de descomposicion espectral. Los autovectores de la matriz $A$ se pueden ver como una base ortornormal \n","\n","$$  \\{ u_1, u_2, \\cdots, u_n \\} $$\n","Usemos la definicion normalizada del cociente de Rayleigh. Sea $u$ un vector unitario. Por definicion de base:\n","\n","$$ u = \\sum_{i=1}^n \\alpha_i u_i \\quad \\quad  (1)  $$\n","\n","$$ R(A, u) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j  $$  (aplicar la distributiva dos veces)\n","\n","Como cada $u_k$ es un autovector de $A$ con autovalor $\\lambda_k$ entonces\n","\n","\n","$$u_i^* A u_j = u_i^* (\\lambda_j u_j) = \\lambda_j u_i^*  u_j = \\lambda_j \\delta_{ij} $$\n","\n","Calculemos ahora $R(A, u)$.\n","\n","\n","$$ R(A, u) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j =  \\sum_{i=1}^n \\sum_{j=1}^n | \\alpha_j |^2 \\lambda_j \\delta_{ij} = \\sum_{i=1}^n | \\alpha_j | \\lambda_i $$ \n","\n","Sin perdida de generalidad asumimos que $\\lambda_1$ es el mayor de todos los autovalores .  Entonces sabemos, por que $u$ es unitario y de la ecaucion (1) que  $u^* u = \\sum | \\alpha | ^2 = 1$. De forma que todos esos $| \\alpha_i |$ son menores o iguales a 1. (por que suman uno y son positivos). Como $\\lambda_1$ es el mas grande, el mayor valor que pueda tomar $R(A, u)$ es cuando $| \\alpha_1 | = 1$ y los demas son 0. \n","Eso me produce que el mayor $R(A, u)= |\\alpha_1 | \\lambda_1 = \\lambda_1$.\n","\n","Otra forma de probar esto es la siguiente. $\\lambda_1 $ es el mas grande de todos los $\\lambda_i$, y es positivo. Al ser el mas grande\n","\n","$$R(A, u )  \\le \\lambda_1 \\sum | \\alpha_i |^2  \\le \\lambda_1 $$.\n","en palabras: El cociente de Rayleigh siempre es menor que el autovalor mas grande. $\\lambda_1$ es una cota para los cocientes de Rayleight. Pero veamos que es igual. No solo es el mas grande, el ocurre dentro todos los cocientes de Rayleight. Cuando?  cuando $\\alpha_1=1$, $\\alpha_i=0$, $i=2,3, \\cdots, n$. Es decir, a lo largo de la direccion $u_1$. \n","\n","Hay un teorema llamado el \n","[minimax](https://en.wikipedia.org/wiki/Min-max_theorem)\n","de Courant establece que se pueden extraer todos los autovalres con cocientes de Rayleigh a lo largo de espacios ortogonales. \n","\n","**Teorema**: Descomposicion **Cholesky**. Si na matriz autoadjunta $A$ es positiva defifnida. (se deriva inmediatamente de la definicion de cociente de Rayleigh que los cocientes de Rayleigh de matrices positivas definidas son positivos). La matriz $A$ tiene una factorizacion \n","\n","$$ A = L L^* $$ \n","donde $L$ es una triangular inferior ($a_{ij} = 0, j>i$). \n","Si $L$ es diagonal entonces $A = L L$ y estamos hablando de que\n","$L$ es la **raiz cuadrada** de $A$. \n","\n","Se puede probar que si $A$ es positiva definida entonces $A^{-1}$ existe y tambien es positiva definidas.\n","\n","Descanso hasta las 8:00 am\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"IljWmvJz0EEC"}},{"cell_type":"markdown","source":["## Calculo matricial\n","En adelante asumimos que las matrices $A$ son reales y orden $m \\times n$.\n","\n","**Definicion:**[Derivada]  Sea $A=A(x)$ un matriz $m \\times n$ con valores $a_{ij}= a_{ij}(x)$ funciones de un vector $x \\in \\mathbb{R}^p$. La **derivada parcial** con respecto a $x_k$ de la matriz $A$ es:\n","\n","$$ \\frac{\\partial A}{\\partial x_k} = \\left [  \\frac{\\partial a_{ij}}{\\partial x_k}\n","    \\right ] \\quad , \\quad k =1 ,2, \\cdots, p \\quad , \\quad i=1,2, \\cdots, m \\quad , \\quad j=1,2, \\cdots, n$$.\n","    Este es un tensor de orden 3."],"metadata":{"id":"pIieB14XIgw4"}},{"cell_type":"markdown","source":["Ej:\n","\n","$$ A = \\begin{pmatrix} x^2 y &  x^2 - y^2 \\\\ 1 & \\exp(x) \\end{pmatrix} $$\n","\n","Entonces\n","\n","$$\\frac{\\partial A}{\\partial x} = \\begin{pmatrix}  2 x y & 2x \\\\\n","0 & \\exp(x)\n","\\end{pmatrix}  $$\n","\n","$$\\frac{\\partial A}{\\partial y} = \\begin{pmatrix}  x^2 & -2y \\\\\n","0 & 0\n","\\end{pmatrix}  $$\n","\n","\n","Notacion $[\\nabla A]_k = [ \\frac{\\partial A}{\\partial x_k} ]$,\n","aca $k$ es la columna. $k$ del gradiente de $A$. El gradiente de $A$\n","es una matriz con respecto a la variable $x_k$. El gradiente es un tensor de rango 3.\n","\n","**Teorema:**[Derivada de una constante] La derivada de una matrix constante $A=C$ es 0.\n","\n","$$ \\frac{\\partial A}{\\partial x_k} = \\left [  \\frac{\\partial a_{ij}}{\\partial x_k} \\right ] = [0] $$\n","\n","Es decir\n","\n","$$ \\frac{\\partial A}{\\partial x_k} = 0 $$\n","\n","**Teorema**[Derivada de un operador lineal]  Si $A$ es una matriz real $m \\times n$, con $x \\in \\mathbb{R}^n$, entonces:\n","\n","$$ \\nabla (A x) = A$$. Es decir:\n","\n","$$ \\frac{\\partial Ax}{\\partial x_k} = \\nabla_k (A x) = A_k $$\n","donde $A_k$ es la columna $k$ de A.\n","\n","*Prueba*: La componente $i$ de $Ax$ es,\n","\n","$$y_i = \\sum_{j=1}^n a_{ij} x_j $$\n","\n","Entonces:\n","\n","$$ \\frac{\\partial y_i}{\\partial x_k} = \\sum_{j=1}^n a_{ij} \\frac{\\partial x_j}{\\partial x_k}  = \\sum_{j=1}^n a_{ij} \\delta_{jk} = a_{ik} $$\n","para $i=1,2, \\cdots, m$ esta es la columna $k$ de la matriz $A$.\n","\n","**Teorema** [producto interno]. Sean $f(x), g(x)$ dos funciones (campos) vectoriales de un vector $x \\in \\mathbb{R}^n$ entonces\n","\n","\n","$$\\frac{\\partial \\langle f, g \\rangle }{\\partial x_i} = \\left \\langle \\frac{\\partial f}{\\partial x_i} , g \\right \\rangle  + \\left \\langle f, \\frac{\\partial g}{\\partial x_i} \\right \\rangle  $$\n","\n","*prueba*. Expandemos el producto interno.\n","\n","$$ \\langle f, g \\rangle = \\sum_{j=1}^n \\overline{f_j(x)} g_j(x) $$\n","\n","Tomamos la derivada sobre los complejos (reales).\n","\n","\\begin{eqnarray}\n","\\frac{\\partial \\langle f, g \\rangle}{\\partial x_i} = \\frac{\\partial }{\\partial x_i} \\sum_{j=1}^n \\overline{f_j(x)} g_j(x) = \n","\\sum_{j=1}^n  \\frac{\\partial \\overline{f_j(x)}}{\\partial x_i} g_j(x) + \n","\\overline{f_j(x)} \\frac{\\partial g_j}{\\partial x_i} =\n"," \\langle \\frac{\\partial f}{\\partial x_i}, g \\left \\rangle  + \\langle f, \\frac{\\partial g}{\\partial x_i} \\right \\rangle \n","\\end{eqnarray}\n","\n","Decimos entonces que\n","\n","$$ \\nabla \\langle f, g \\rangle = \\langle \\nabla f, g \\rangle + \\langle f, \\nabla g \\rangle $$\n","\n","Hay que ser cuidadoso con esta afirmacion. $\\nabla f$ es una matriz. \n","$(\\partial f_i/\\partial x_j)$ y $g$ es un vector. Las unidades no cuadran.\n","Simplemente piense en esto como en tomar la componente $k$ a izquierda y\n","derecha. \n","\n","$$ \\nabla_k \\langle f, g \\rangle = \\langle \\nabla_k f, g \\rangle + \\langle f, \\nabla_k g \\rangle $$\n","\n","**Ejemplo** [minimos cuadrados] Sea $f(x)= \\frac12 \\| Ax - b \\|^2$ con\n","$x \\in \\mathbb{R}^n$ y $A$ es real $m \\times n$. \n","Encuentre el gradiente de $f$.\n","\n","*solucion*: $f(x) = \\frac12 \\langle Ax -b, Ax-b \\rangle $\n","\n","Usamos la notacion $\\nabla_k f = (\\nabla f)_k$ \n","\n","\\begin{eqnarray}\n","\\nabla_k f(x) &=& \\frac12 \\langle \\nabla_k (Ax - b), (Ax - b) \\rangle + \\langle Ax-b , \\nabla_k (Ax - b) \\rangle  \\quad \\text{por commutatibilidad del producto interno ocurre que} \\\\\n","&=& \\langle \\nabla_k (Ax - b),  Ax -b \\rangle  \\\\\n","&=& \\langle \\nabla_k Ax - \\nabla_k b , Ax -b \\rangle \\\\\n","&=& \\langle A_k , Ax-b \\rangle\n","\\end{eqnarray}\n","Dejamos como prueba que $\\nabla_k (A+B) = \\nabla_k A + \\nabla_k B$\n","\n","Es decir, tomando todos los $k=1,2, \\cdots , n$\n","$$\\nabla f(x) = A^*(Ax-b) $$.\n","\n","Este resultado es muy imporante. Si queremos hallar el minimo de la funcion de costo (objetivo) $f(x)$ debemos igular el gradiente a 0.\n","\n","Es decir $\\nabla f(x) =0$. Es decir,\n","\n","$$ A^* (Ax - b) = 0$$\n","Es dcir\n","\n","$$ A^* A x = A^* b  \\quad , \\quad (2)$$.\n","\n","A las ecuaciones (2) se les llama ecuaciones **normales** . Son la base de la solucion del problema de minimos cuadrados que resolvemos en la proxima clase.\n","\n","## Problema de minimos cuadrados regularizado.\n","\n","El problema (mas importante) del algebra lineal es la solucion de $Ax=b$.\n","Si $A$ es no singular, la solucion es posible:\n","\n","$$ x = A^{-1} b$$.\n","\n","Pero en general $A$ ni siquiera es cuadrada. Si $A$ no es cuadrada podemos tener dos tipos de problemas:\n","\n","1. El sistema es **sobredeterminado** (overdetermined). Es decir hay  mas ecuaciones independientes que incognitas. La solucion no existe.\n","El espacio nulo es $\\{0 \\}$ pero el rango no cubre todo el espacio\n","\n","2. El sistema es **subdeterminado** (underdetermined). Existen mas incognitas que ecuaciones. El espacio nulo aca no es $\\{ 0 \\}$.\n","\n","\n","\n"],"metadata":{"id":"1ZLWcA-wKaFM"}}]}