{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMoMd9SCeqHvwCeR/lblI+0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Para hoy tenemos do temas fundamentales.\n","\n","* El teorema espectral de operatores autoadjuntos\n","* El teorema fundamental SVD: Singular Value Decomposition.\n"],"metadata":{"id":"9s7LlYOao0H7"}},{"cell_type":"code","source":[],"metadata":{"id":"4HTj0bwlzQU5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Teorema espectral.\n","\n","**Lema**:  Para caa espacio invariante $W \\ne \\{ 0 \\}$ de una matriz cuadrada $A$, existe (siempre) in autopar $\\lambda, x$.\n","\n","prueba: Com $W$ es finito dimensional tiene una base\n","$\\{x_1, x_2, \\cdots, x_k \\}$ donde $k$ es la dimension de $W$.\n","Recuerden que $A W \\subset W$. Sea $x \\in W$, entonces $Ax \\in W$.\n","$x=\\sum_{i=1}^k \\alpha_i x_i$, para $x \\ne 0$. Puedo escribir\n","\n","$$A x_i = \\sum_{j=1}^k  \\beta_{ji} x_j \\quad (1) $$\n","\n","Queremos probar que existe $\\lambda$ tal que $A x - \\lambda x=0$ para $x \\in W$. \n","Es decir:\n","\n","$$ A x = \\sum_{i=1}^k \\alpha_i A x_i = \\lambda \\sum_{i=1}^k \\alpha_i x_i $$\n","\n","Restamos la izquierda menos centro,\n","\n","\\begin{eqnarray}\n","\\sum_{i=1}^k \\alpha_i A x_i - \\lambda \\sum_{i=1}^k \\alpha_i x_i &=&0  \\quad \\text{restando izquierda de centro} \\\\\n","\\sum_{i=1}^k \\alpha_i \\sum_{j=1}^k \\beta_{ji} x_j - \\lambda \\sum_{i=1}^k \\alpha_i x_i &=&0  \\quad \\text{usando ecuacion (2)} \\\\\n","\\sum_{i=1}^k \\alpha_i \\left ( \\sum_{j=1}^k \\beta_{ji} x_j - \\lambda x_i \\right ) &=& 0  \\quad \\text{factorizamos la suma} \\\\\n","\\sum_{i=1}^k \\alpha_i \\left ( \\sum_{j=1}^k \\beta_{ji} x_j - \\lambda \\sum_{j=1}^k x_j \\delta_{ij} \\right ) &=& 0 \\text{truco del delta   } x_i=\\sum_{j=1}^k \\delta_{ij} x_j \\\\\n","\\sum_{i=1}^k \\alpha_i \\left (  \\sum_{j=1}^k \\beta_{ij} x_j - \\lambda \\delta_{ij} x_j  \\right ) &=& 0   \\quad \\text{recogiendo la segunda suma} \\\\\n"," \\sum_{j=1}^k \\left ( \\sum_{i=1}^k \\alpha_j (\\beta_{ij} - \\lambda \\delta_{ij}) \\right ) x_j &=& 0 \n","\\end{eqnarray}\n","\n","En este punto vemos que los $x_j$ tienen unos pesos en la combinacion lineal los cuales producen 0. Y como los $x_j$ son **linealmente independintes** , entonces los coeficientes de $x_j$ sosn todos cero.\n","\n","Es decir:\n","\n","\\begin{eqnarray}\n"," \\sum_{i=1}^k \\alpha_j (\\beta_{ij} - \\lambda \\delta_{ij}) = 0\n","\\end{eqnarray}\n","\n","Esto de forma matricial es: $(B-\\lambda I) \\alpha = 0$. \n","Pero hay un teorema que dice que $B$ tiene que tener por lo menos un autovalor, y lo tiene yes $\\lambda$. \n","\n","Por lo tanto cada subsepacio de $W$ invariante de $A$ tiene por lo menos una autovector/valor. "],"metadata":{"id":"jDZ3m7NkrkVP"}},{"cell_type":"markdown","source":["### Teorema espectral para matrices adjuntas.\n","Sea $A$ una matriz $n \\times n$ autoadjunta, entonces existe una\n","base base ortonormal.\n","\n","$$\\{ x_1, x_2, \\cdots, x_n \\} $$\n","para la cual la matriz $Q$  (columnas de esta base) satisface la relacion\n","\n","$$ A = Q \\Lambda Q^* $$\n","donde $\\Lambda$ es una matriz diagonal con los autovalores de $A$.\n","Es decir cada entrada de la matriz de $A$ es $\\delta_{ij} \\lambda_j=\\lambda_i$. \n","\n","**Prueba:**  Por induccion.\n","\n","* Se prueba para $k=1$\n","* Se asume que es valido para $k$ y se prueba para $k+1$.\n","Yo llamo a estas pruebas, el efecto domino. "],"metadata":{"id":"0UJ-rIfluzof"}},{"cell_type":"markdown","source":["**Prueba**: \n","*   k=1:  ($k$ es la dimensio de la matriz $A$)\n","Si $k$, La base de esta matriz es $\\{ x_1 \\}$, $ x_1 \\ne 0$. y\n","esta base se puede normalizar $x_1/x_1=1$, $\\{ 1 \\}$. \n","\n","$3 = (1)(3)(1)$. Por defecto y vacuidad $\\{1 \\}$ es otonormal.\n","\n","* Asumimos que el teorema es valido para $k$ y probamos que es valido para $k+1$.\n","Valido para $k$: Existen $k$ vectores $\\{ x_1, x_2, \\cdots , x_k \\}$ que con ortogonales (dos a dos) y tales que $A x_i = \\lambda x_i$.\n","Debemos prabar que existe otro $x_{k+1}$ que es ortogonal a todos los demas. \n","Buscamos el complemento ortogonal del generado de los vectores vasos\n","$\\langle x_1, x_2, \\cdots, x_k \\rangle $, llamemoslo $W$\n","\n","$$ W = \\langle x_1, x_2, \\cdots, x_k \\rangle^{\\perp}$$\n","Por definicion el **complento ortogonal** de un espacio $V$ es el conjunto de todos los $y$ tales que $\\langle y, x \\rangle = 0$, para todo $x \\in V$.\n","\n","Vamos a probar que $W$ es invariante para $A$ y por el teorema anterior tiene por lo mentos un autovalor/vector que no esta en los anteriores. \n","\n","ES decir probalmos que si $y\\in AW$, entonces $y \\in W$.\n","\n","Sea $y \\in AW$, $y=Ax$, $x \\in W$.  Para cada $x_i , i=1,2, , k$\n","\n","$$ \\langle y, x_i \\rangle = \\langle Ax, x_i \\rangle = \\langle x, A^* x_i \\rangle = \\langle x, A x_i \\rangle = \\langle x, \\lambda_i x_i \\rangle = \\lambda_i \\langle x, x_i \\rangle = 0 $$\n","\n","Es decir $c$ es perpendicular a **todos** $x_i$ y por lo tanto al generado, o sea que $x \\in W$. Osea que $W$ es invariante, o sea que\n","$W$ tiene un autovalor/vector que tienen que ser distintos a los\n","$\\lambda_1, \\lambda_2, \\cdots , \\lambda_k$, $x_1, x_2, \\cdots, x_k$\n","y con esto lo podemos nombrar que autovalor $\\lambda_{k+1}$ y autovector $x_{k+1}$.\n","\n","Listo la prueba por inducccion para la construccion de $n$ autvalores y $n$ autovectores ortogonales, (los autovalores pueden tener repetidos). Los podemos normalizar, $x_i/\\| x_i \\|$ y esto \n","me da una base ortonormal. Nos devolvemos a la clase anterior y\n","\n","$$A = Q \\Lambda Q^* $$\n","\n","\n"],"metadata":{"id":"W22k1Gr61bUa"}},{"cell_type":"markdown","source":["Multiplicando a izquierda por $Q^*$ y a derecha por $Q$\n","\n","$$ Q^* A Q = \\Lambda $$"],"metadata":{"id":"WkyPAu601deE"}},{"cell_type":"markdown","source":["Vamos a ver algunas aplicaciones interesantes de este teorema.\n","\n","Habemos de los sistemas quadraticos, en $\\mathbb{R}^n$.\n","Matriz $A$ es $n \\times n$\n","\n","$$\\langle Ax, x \\rangle = x^T A x =  \\sum_{i=1}^n  \\sum_{j=1}^n  a_{ij} x_i x_j $$\n","\n","Como $A$ es simetrica los elementos por fuera la diagonal de $A$ $(n^2 - n)$ estan **acoplados**. $a_{23} x_2 x_3,a_{32} x_3 x_2= 2 a_{23} x_2 x_3$. Vamos a ver como la descomposicion espectral simplifcia el problema.\n","recuerde $A=\n","\n","$$x^T A x = x^T Q \\Lambda Q^T x $$\n","Cambio de variable $y=Q^T x$ entonces se convierte en\n","\n","$$x^T A x = y^T \\Lambda y$$\n","como $\\Lambda$ es diagonal se pierde la doble suma si convierte en\n","$$x^T A x = y^T \\Lambda y  = \\sum_{i=1}^n \\lambda_i y_i^2$$\n"],"metadata":{"id":"TpI4fJH76HFB"}},{"cell_type":"markdown","source":["Teorema para aprender a multiplicar matrices de diversas formas.\n","\n","Teorema: Sea $A$ una matriz $m \\times n$, $B$ una matriz $n \\times p$, $x$ un vector en $\\mathbb{C}^n$, $\\Lambda$ una matriz $n \\times p$ diagonal con valores $\\lambda_i$ en la diagonal y cero por fuera. entonces\n","\n","* $$ AB = [AB_1 | AB_2 | \\cdots | AB_p]$$\n","donde $B_i$ son columnas de la matriz $B$.\n","\n","* $$ Ax = \\sum_{j=1}^n x_j A_j$$\n","donde $A_j$ son columnas de la matriz $A$. Es decir $x \\in \\mathcal{R}(A)$. \n","\n","* Si $p \\ge n$.\n","\n","$$ A \\Lambda = [ \\lambda_1 A_1 | \\lambda_2 A_2 \\cdots | \\lambda_n A_n | 0 | \\cdots | 0  $$\n","donde agregamos $p-n$ columnas de ceros, Si $n>p$\n","\n","$$ A \\Lambda = [ \\lambda_1 A_1 | \\lambda_2 A_2 \\cdots | \\lambda_p A_p] \\quad \\quad (2) $$\n","\n","* Si $u_i$ es un vector columna de una matriz $U$, $m \\times m$ entonces\n","\n","$$ U^* b = \\begin{pmatrix} u_1^* b \\\\ u_2^* b \\\\ \\vdots u_m^* b\n","\\end{pmatrix} $$\n","\n","Nos vemos  las 8: 00 am."],"metadata":{"id":"eZRSIetW-PJ3"}},{"cell_type":"markdown","source":["Probemos el primero.\n","Llamemos $C=AB$, de forma que\n","\n","$$c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj}. $$\n","Tomoemos la columna $j$ del lado derecho. Y de aca la fila i\n","\n","$$ (AB_j)_i =   \\left ( \\sum_{k=1}^n a_{ik} b_{kj} \\right )_i $$\n","\n","Las otras pruebas son **parecidas** y cortas.\n","\n","Que pasa si todos los autovectores no son ortogonales (el caso de matrices no autoadjuntas) pero son linealmente independientes.\n","\n","Es decir, sean estos $P_i$ $A$ es $n \\times n$\n","\n","$$ AP_i = \\lambda_i P_i $$\n","En forma matricial (construyendo $P=[P_1 | P_2 | \\cdots P_n]$)\n","Por las propiedades anteriores (2).\n","\n","$$ AP = P \\Lambda$$. \n","Como los vectores $P_i$ son linealmente indepndientes, $P$ tiene inversa\n","y multiplicamos por $P^{-1}$ primero a izquierda y luego a derecha:\n","\n","$$ P^{-1} A P = P^{-1} P \\Lambda $$\n","a derecha\n","$$ AP P^{-1} = P \\Lambda P^{-1} $$  (como $P P^{-1}=P^{-1} P=I$)\n","\n","$$ \\Lambda = P^{-1} A P $$\n","$$ A = P \\Lambda P^{-1}$$.\n","\n","Esta es la pruebaa del teorema: Si $A \\in \\mathbb{C}^{n \\times n}$ con $n$  autovectores linealmente indepndientes, entonces\n","$A=P \\Lambda P^{-1}$, $\\Lambda = P^{-1} A P$, donde $P$ es la matriz de autovectores (columnas) y $\\Lambda$ es una diagonal con autovalores.\n","A la matriz $A$ se le conoce como **diagonalizable**\n","\n","Miremos como usar esto para elevar una matriz a una potencia.\n","\n","Teorema: Si $A$ es diagonalizable, es decir $A=P \\Lambda P^{-1}$ con $\\Lambda$ diagonal, entonces:\n","\n","$$ A^k = P \\Lambda^K P^{-1}$$.\n","\n","Probemos por induccion:\n","\n","* $k=1$. Nada que hacer.\n","* Asumimos que es valido para $k$ y probamos para $k+1$.\n","\n","\\begin{eqnarray}\n","A^{k+1} &=& A^k A \\quad \\text{recursividad de la potencia}  \\\\\n","&=& P \\Lambda^k P^{-1} P \\Lambda P^{-1}  \\quad  \\text{hipotess} \\\\\n","&=& P \\Lambda^k (P^{-1} P) \\Lambda P^{-1} \\quad \\text{asociativa} \\\\\n","&=& P \\Lambda^k \\Lambda P  \\quad P^{-1} P = P P^{-1} = I  \\\\\n","&=& P \\Lambda^{k+1} P^{-1}\n","\\end{eqnarray}\n","\n","Esto prueba el teorema por inducccion. \n","\n","No tadas las matrices son diagonalizables, para aqueyas que no se aproxima la diagonalizacion en el teorema de [Jordan](https://en.wikipedia.org/wiki/Jordan_normal_form)  (cuando los autovectores no son linealmente independientes.\n","\n","\n","\n","Definicion. Una matriz (autoadjunta0 es **positiva definida** si todos sus autovalres son positivos, **negativa definida** si todos los autovalres son negativos, y **no negativa definida** si todos los autovalres son positivos o 0.\n","Se puede probara que esta definicion es equivalente a:\n","\n","* Positiva definidad: Para todo $x \\in \\mathbb{C}^n$, $x^T A x > 0$.\n","* Negativa  definidad: Para todo $x \\in \\mathbb{C}^n$, $x^T A x < 0$.\n","* no negativa definidad: Para todo $x \\in \\mathbb{C}^n$, $x^T A x  \\ge 0$.\n","\n","Esas definciones son equvalentes (tarea).\n","\n","Teorema (tarea). Si $A$ es positiva definida, $A^{-1}$ es positiva definida.\n","\n","Teorema Estrella: SVD: Singular Value Decomposition (Descomposicion en valores singulares).\n","\n","Sea $A \\in \\mathbb{C}^{m \\times n}$. Existe una matriz ortgonal $u \\in \\mathbb{C}^{m \\times m}$, una matriz ortogonal $V \\in \\mathbb{C}^{n \\times n}$ y una matriz real, diagonal $\\Sigma \\in \\mathbb{R}^{m \\times n}$ tales que\n","\n","$$ A = U \\Sigma V^* $$\n","\n"],"metadata":{"id":"AXvGfPc3G8jx"}},{"cell_type":"markdown","source":["Se usa mucho en ML para reduccion de dimensionalidad (PCA).\n","\n","Prueba (Carnelius Lanczos). Asuma $A \\in \\mathbb{C}^{m \\times n}$\n","y construimos una matriz auxiliar:\n","\n","\n","$$S = \\begin{pmatrix} 0 \\quad  |  \\quad  A \\\\\n","------\\\\\n","A^*  \\quad | \\quad 0  \\quad  \\end{pmatrix}  $$\n","\n","La matriz $S= \\in \\mathbb{C}^{(m+n) \\times (m+n)}$ es autoadjunta. O sea que\n","los autovectores de $S$ son ortogonales y los autovalores son reales.\n","\n","\n","$$ S w_i = \\lambda_i w_i \\quad \\quad (3) $$\n","Pero $w_i$ lo podemos partir como sigue\n","\n","$$ \\begin{pmatrix}  u_i \\\\ v_i \\end{pmatrix} $$, $u_i \\in \\mathbb{C}^m$, $v_i \\in \\mathbb{C}^n$. La ecuacion (3) resulta\n","\n","\\begin{eqnarray}\n","A^* u_i &=& \\lambda_i v_i  \\quad (6)  \\\\\n","A v_i  &=& \\lambda_i u_i   \\quad (7)\n","\\end{eqnarray}\n","Multiplicamos la primera ecuacion por $A$ y la segunda por $A^*$\n","\n","\\begin{eqnarray}\n","A A^* u_i &=& A \\lambda_i v_i = \\lambda_i \\lambda_i u_i = \\lambda_i^2 u_i \\\\\n","A^* A v_i &=& A^* (\\lambda_i u_i) = \\lambda_i A^* u_i = \\lambda^2 v_i\n","\\end{eqnarray}\n","\n","No todos los autovlares son positivos. Sea $r$ el indice del ultimo que no es 0. $\\lambda_1, \\lambda_2, \\cdots, \\lambda_r, 0, 0 , 0 $. \n","\n","Construyamos la matrices $U_r$, y $V_r$ de los autvectores de ($A A^*$ y de $A^* A$ respectivamente, esas matrices tiene como columnas estos autvectores. \n","Llamos $U_0$, $V_0$ las colas de matrices asosciadas con $\\lambda_i=0$. \n","\n","Segun esta construccion: de (6, 2) \n","\n","\\begin{eqnarray}\n","A^* U_r &=& V_r \\Lambda_r  \\quad , \\quad A^* U_0 = 0 \\\\\n","A V_r &=& U_r \\Lambda_r  \\quad , \\quad A V_0 = 0.\n","\\end{eqnarray}\n","\n","Construimos la matriz\n","\n","\n","\n","\n"],"metadata":{"id":"LJ2Z_1LHNUKu"}},{"cell_type":"code","source":["import numpy as np\n","\n","A=np.array( [[1,3,], [-2,4]])\n","\n","dir(np.linalg)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"esSDD92OFEt4","executionInfo":{"status":"ok","timestamp":1662069867002,"user_tz":300,"elapsed":291,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"5f6d8c58-1cfe-42c6-d9d5-9fc48c0a73b3"},"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['LinAlgError',\n"," '__all__',\n"," '__builtins__',\n"," '__cached__',\n"," '__doc__',\n"," '__file__',\n"," '__loader__',\n"," '__name__',\n"," '__package__',\n"," '__path__',\n"," '__spec__',\n"," '_umath_linalg',\n"," 'cholesky',\n"," 'cond',\n"," 'det',\n"," 'eig',\n"," 'eigh',\n"," 'eigvals',\n"," 'eigvalsh',\n"," 'inv',\n"," 'lapack_lite',\n"," 'linalg',\n"," 'lstsq',\n"," 'matrix_power',\n"," 'matrix_rank',\n"," 'multi_dot',\n"," 'norm',\n"," 'pinv',\n"," 'qr',\n"," 'slogdet',\n"," 'solve',\n"," 'svd',\n"," 'tensorinv',\n"," 'tensorsolve',\n"," 'test']"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["np.linalg.eig(A)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nn1zB1p8FOqr","executionInfo":{"status":"ok","timestamp":1662069923925,"user_tz":300,"elapsed":8,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"60f1e75c-f557-4246-adc0-84c4cfdac586"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([2.5+1.93649167j, 2.5-1.93649167j]),\n"," array([[0.77459667+0.j , 0.77459667-0.j ],\n","        [0.38729833+0.5j, 0.38729833-0.5j]]))"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["np.linalg.svd(A)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FwlaPdIUFdLr","executionInfo":{"status":"ok","timestamp":1662069944299,"user_tz":300,"elapsed":6,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"1eb0e9f8-91a0-48ca-8030-0d6a4cc17a8a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(array([[ 0.52573111,  0.85065081],\n","        [ 0.85065081, -0.52573111]]),\n"," array([5.11667274, 1.95439508]),\n"," array([[-0.22975292,  0.97324899],\n","        [ 0.97324899,  0.22975292]]))"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":[],"metadata":{"id":"7m9IoQ5BFqw2"},"execution_count":null,"outputs":[]}]}