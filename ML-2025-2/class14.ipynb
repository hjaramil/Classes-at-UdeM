{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPYmwo6sbRu8FxK6ySxsHCx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Multiplicadores de Lagrange en SVM.\n","Vamos a formular un nuevo problema que se llama el **dual** el cual va a tener ventajas que formulas más adelante.\n","\n","Este problema se formuló inicialmente por Vladimir Vapnik y Alexey Chervonenkis en 1960s. Ma adelante en 1995 se hizo otra publicación de Cortes, y Vapnik.\n","\n","Notación: En vez de $x^{(i)}$, $x_i$, en vez de $y^{(i)}$, $y_i$.\n","\n","En la clase anterior vimos que el problema de SVM consistía en encontrar un hiperplano con \"pendiente\" (vector normal) $w$ e intercepto $b$  que\n","separa los dos conjuntos (pinos vs eucaliptos). Lo llevamos a una función de costo de la forma de multiplicadores de Lagrange\n","\n","$$\\text{minimizar  } J(w) = \\frac{\\| w \\|²}{2} $$\n","$$\\text{sujeto a } y_i(w^T x_i - b ) - 1 = 0 \\quad , i=1,2, \\cdots, m \\tag{0} $$\n","\n","Este es un problema de la forma\n","\n","$$\\min f(\\theta) \\quad , \\quad \\text{sujeta a } $$\n","$$g_i(\\theta) \\ge 0. $$\n","Este problema se conoce como el\n","[KKT](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions)\n","por las siglas de sus inventores. Es una generalización de los multiplicadores de Lagrange. Pero la solución se hace en el mismo sentido.\n","\n","La técnica es la misma. Se construye un Lagrangiano (no el de la mecánica analíitca)\n","\n","$$L(\\theta, \\lambda) = f(\\theta) - \\sum_i \\lambda_i g_i(\\theta) $$\n","\n","En nuestro caso tenemos\n","\n","$$L(w, b, \\lambda) = \\frac12 \\| w \\|²  - \\sum_i \\lambda_i [ y_i(w \\cdot x_i -b) -1 ]  \\tag{1} $$"],"metadata":{"id":"NFXE7V1Bad_o"}},{"cell_type":"markdown","source":["\n","$$\\nabla L(w, b, \\lambda) = \\nabla \\left [ \\frac12 \\| w \\|²  - \\sum_i \\lambda_i [ y_i(w \\cdot x_i -b) -1 ] \\right ]   = 0  $$\n","\n","Lo haremos en dos pasos. Con respecto a $w$ y con respecto a $b$\n","\n","1.\n","$$\\nabla_w L(w, b, \\lambda) = \\nabla_w \\left [ \\frac12 \\| w \\|²  - \\sum_i \\lambda_i [ y_i(w \\cdot x_i -b) -1 ] \\right ]   = 0  $$\n","\n","A su vez consideramos dos partes.\n","\n","* Primero y segundo término.\n","El primer termino es $\\| w \\|²/2 = \\sum_i w_i² /2$.\n","De forma que\n","$$\\frac{\\partial}{\\partial w_j} \\left ( \\frac12 \\sum_i w_i² \\right ) = \\sum_i w_i \\delta_{ij} = w_j \\tag{2} $$\n","\n","* Para el segundo término.\n","\\begin{eqnarray}\n","\\frac{\\partial }{\\partial w_j}\n","\\left (\n","    \\sum_i \\lambda_i [ y_i(w \\cdot x_i -b) -1\n","    \\right )  &=&\n","    \\frac{\\partial }{\\partial w_j}\n","\\left (\n","    \\sum_i \\lambda_i [ y_i \\left ( \\sum_k w_k (x_i)_k  -b \\right ) -1\n","    \\right )  \\\\\n","    &=& \\sum_i \\lambda_i \\left [ y_i \\sum_k \\delta_{kj} (x_i)_k  \\right ] \\\\\n","    &=& \\sum_i \\lambda_i y_i (x_i)_j \\tag{3}\n","\\end{eqnarray}\n","Sumando (2) y (3) encontramos\n","\n","\n","$$\\nabla_w L(w, b, \\lambda) = w_j - \\sum_i \\lambda_i y_i (x_i)_j = 0 $$\n","De acá que\n","\n","$$w = \\sum_i \\lambda_i y_i x_i  \\tag{4} $$\n","\n","2. El gradiente con respecto a $b$ es igual a\n","\n","$$\\frac{\\partial}{\\partial b} \\left (\n","    \\sum_i \\lambda_i [ y_i (w \\cdot x_i - b) -1 ]\n","    \\right )  = -\\sum_i \\lambda_i y_i = 0 $$\n","\n","De forma que\n","\n","$$\\sum_i \\lambda_i y_i = 0. \\tag{5} $$\n","\n","Vamos a reemplazar $w$ y esta ecuacion (me elimina $b$ como veremos) en el Lagrangiano  (1)\n","\n","\\begin{eqnarray}\n","L(w, b, \\lambda) &=& \\frac12  \\left ( \\sum_i \\lambda_i y_i x_i  \\right )\n","\\cdot \\left ( \\sum_j \\lambda_j y_j x_j  \\right )  - \\sum_i \\lambda_i \\left [ y_i\n","    \\left ( \\sum_j \\lambda_j y_j x_j  \\right ) \\cdot x_i -b) -1 \\right]   \\\\\n","    &=&\n","    \\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j\n","    - \\sum_i \\sum_j \\lambda_i y_i x_i \\cdot x_j + \\sum_i \\lambda_i y_i b + \\sum_i \\lambda_i \\\\\n","    &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j x_i \\cdot x_j + 0 + \\sum_i \\lambda_i  \\\\\n","    \\end{eqnarray}\n","\n","O sea que\n","Tenemos el problema **dual**\n","\n","$$L(\\lambda) = -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_i \\lambda_i  $$\n","sujeto a\n","\n","$$\\sum_i \\lambda_i y_i = 0. $$\n","\n","Se puede resolver ya este problema que es convecto (cuadrática) con metodos conocidos. O se puede, también, formular la otra ecuación de multiplicadores de Lagrange\n","\n","$$\\frac{\\partial L}{\\partial \\lambda_i} \\quad   i = 1, \\cdots, m $$\n","Este sistema de $m$ ecuacioes es lineal y se puede resolver con métodos conocidos.\n","\n","Una vea conocidos los multiplicadores de Lagrange $\\lambda_i$, podemos calcular la \"pendiente\" ($w$) de la Ecuación, (4). Se deja como taréa obtener $b$.\n","\n"],"metadata":{"id":"VjmHL39tc-ZJ"}},{"cell_type":"markdown","source":["### Ventajas de la formulación dual.\n","* La importante es que permite usar el **kernel trick** (que se enseña en la segunda parte de esta clase). El cual acelera grandemente el proceso. Esto debido a la cantidad de productos internos de la forma $x_i \\cdot x_j$ en las expresiones.\n","\n","* La solucón en términos de $\\lambda_i$ es rápida en el sentido de que la mayoría de los $\\lambda_i$ son cero. Solo los relacionados a la margen son $\\lambda_i>0$ los demás son 0. El número de $\\lambda_i \\ne 0$ es el número de vectores de soporte.\n","\n","\n","\n"],"metadata":{"id":"IjoEJiAAj1ip"}},{"cell_type":"markdown","source":["# Kernel Trick\n","Hasra 1991 SVM no era tan popular. En 1991 Isabelle Guyon introdujo el **kernel trick** que es supervalioso por que acelera el proceso de separar dos conjuntos de datos.\n","\n","Reurden el criterio de separabilidad.\n","\n","$$y_i \\left (\\sum_j \\lambda_i y_j x_i \\cdot x_j - b   \\right ) \\ge 1  \\tag{6} $$\n","$y_i \\pm $ donde insertamos $w$ en la Ecuación (0).\n","\n","También recordemos el Lagrangiano del problema dual.\n","\n","$$L(\\lambda) = -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_i \\lambda_i  \\tag{7} $$\n","\n","Reucuerden que cuando dos conjuntos no es linealmente separable (que no existe un hiperplano de forma que uno de los conjuntos (pinos) esté a un lado del plano y el otro (eucaliptos) al otro lado) entonces proyectamos hacia atrá (backproject) los datos con una función $\\Phi$. A este $\\Phi$ se le llama **feature mapping** (mapeo de atributo).\n","\n","En este sentido nos vamos de una espacio $\\mathbb{R}^k$ a $\\mathbb{R}^p$ donde $p  > k$. Cuando hacemos este mapeo la Ecuacion (7) se convierte en\n","\n","$$L_\\Phi(\\lambda) = -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j \\Phi(x_i) \\cdot \\Phi(x_j) + \\sum_i \\lambda_i  $$\n","\n","\n","Esta es la que se mete al problema del SVM.\n","Las condiciones de separabilidad de (6) son 2\n","$$\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b \\ge 1 \\quad , \\quad y_i = \\oplus = +1 $$\n","\n","$$\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b < -1 \\quad , \\quad y_i = \\oplus = +1 $$\n","En el intervalo $(-1,1)$ es el margen (gap) \"la carretera\". En $=1$ la berma. Y en los intervalos $(-\\infty, -1)$ y $(1, \\infty)$ está el bosque.\n","\n","\n","De todas estas últimas ecuaciones quiero que observen la cantidad de operaciones del tipo $\\Phi(x_i) \\cdot \\Phi(x_j)$.\n","Muchas veces se tiene que calcular este producto. Isabelle Guyon invento el truco que explicamos a continuación.\n","\n","Se redefine el $k(x,y)$ junto con el \"feature mapping\"\n","\n","$$k(x,y) = \\Phi(x_i) \\cdot \\Phi(x_j) \\tag{8} $$\n","\n","Mediante ejemplos vamos a ilustrar el truco.\n"],"metadata":{"id":"oUTVEOzGkuJ0"}},{"cell_type":"markdown","source":["**Ejemplo 3.3.8**\n","* Tomemos $n=2$ (dos dimensiones o número features)\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^2 &\\to& \\mathbb{R}^3  \\\\\n","(x_1, x_2) &\\mapsto& \\Phi(x_1, x_2)=(x_1² , x_2² , \\sqrt{2} x_1 x_2 )\n","\\end{eqnarray}\n","\n","Vamos a mostrar que\n","$$k(x,y) = (x \\cdot y)² \\tag{9} $$\n","Probamos que $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$, como indica la Ecuacion (8).\n","\n","$$k(x,y) = ( x \\cdot y)^2 = \\left ( \\sum_{i=1}^2 x_i y_i \\right )^2 = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2. $$\n","De otro lado\n","\n","$$\\Phi(x) \\cdot \\Phi(y) = (x_1^2, x_2^2, \\sqrt{2} x_1 x_2) \\cdot (y_1^2, y_2^2 , \\sqrt{2} y_1 y_2 ) = x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 x_2 y_1 y_2 . $$\n","\n","Vemos que $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$.\n","\n","Vamos a ver que es más rapido calcular $k(x,y)$ que $\\Phi(x) \\cdot \\Phi(y)$.\n","\n","Contamos número de operaciones.\n","* $k(x,y)$: De la Ecuación (9). 2 multiplicaciones y una suma.... y un cuadrado.\n","\n","* $\\Phi(x) \\cdot \\Phi(y)$: 4 cuadrados, 4 multiplicaciones, del producto interno 2 multiplicaciones y una suma.\n","\n","Vemos que el no uso de \"kernel trick\" (9), nos arroja mas 3 veces el cómputo.\n","\n","* Para $n=3$. Definimos\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^3 &\\to& \\mathbb{R}^6 \\\\\n","(x_1, x_2, x_3) &\\mapsto& \\Phi(x_1, x_2, x_3) = (x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3 )\n","\\end{eqnarray}\n","\n","Igual que en la Ecuacion (9)\n","$$k(x,y) = (x \\cdot y)²  $$\n","Veamos que $k(x,y)=\\Phi(x) \\cdot \\Phi(y)$\n","\n","* $k(x,y)$:\n","    * Del producto interno: 3 multiplicaciones y 2 sumas\n","    * Un cuadrado.\n","\n","* $\\Phi(x) \\cdot \\Phi(y)$:\n","    * Por la definición de $\\Phi$: 6 cuadrados. 12 productos.\n","    * Por el producto interno 6 productos y 5 sumas.\n","\n","Se observa una clara diferencia entre el uso de $k(x,y)$ vs, $\\Phi(x) \\cdot \\Phi(y)$.\n","\n","**Ejemplo 3.3.8**: Asuma $c \\in \\mathbb{R}$.\n","Piense en en lo siguiente\n","\n","\\begin{eqnarray}\n","k(x,y) &=& (x \\cdot y + c )²  \\\\\n","&=& \\left ( \\sum_{i=1}^3 x_i y_i + c  \\right ) \\left (\n","   \\sum_{i=1}³ x_i y_i + c \\right ) \\\\\n","   &=& \\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j  + 2 c \\sum_{i=1}^3 x_i y_i + c²\n","\\end{eqnarray}\n","\n","Definamos ahora el siguiente feature mapping.\n","\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^3 &\\to& \\mathbb{R}^{10} \\\\\n","(x_1, x_2, x_3) &\\mapsto& \\Phi(x_1, x_2, x_3) = (x_1^2 , x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3, \\sqrt{2} c x_1, \\sqrt{2} c x_2, \\sqrt{2} c x_3, c)\n","\\end{eqnarray}\n","\n","\n","\n"],"metadata":{"id":"IGO0ZMSxuDgr"}},{"cell_type":"markdown","source":["Se puede probar que\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) \\tag{10} $$\n","Cálculos de número de opearaiones. En el aire.\n","El número de operaciones es significatiavente más grande para $\\Phi(x) \\cdot \\Phi(y)$ que para $k(x,y)$.\n","\n","En general, para cualquier $d \\in \\mathbb{N}$, $d \\ge 2$ piense en este kernel\n","\n","$$k(x,y) = (x \\cdot y + c)^d \\quad, \\quad c \\in \\mathbb{R}  $$\n","El kernel satisface\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) $$\n","para algún $\\Phi$ (es tarea).\n","\n","Pistas sobre el $\\Phi$. $\\Phi(x)$ es un feature mapping de $\\mathbb{R}^n$ to\n","$\\mathbb{R}^p$, con\n","\n","$$p = \\binom{n+d}{d} $$\n","Es decir\n","\n","$$\\Phi: \\mathbb{R}^n \\to \\mathbb{R}^p. $$\n","\n","Contar operaciones es interesante. Finjense que $p$ (por el teorema de Sterling) crece de forma exponencial. Entonces el costo de operarar $\\Phi$ es muy alto comparado con el costo de $k(x,y)$ en la Ecuación"],"metadata":{"id":"zf69VqzVzmvN"}},{"cell_type":"markdown","source":["### Kernel Gaussiano:\n","Piense en el siguiente kernel\n","\n","$$k(x_i, x_j) = \\mathrm{e}^{- \\frac{x_i - x_j}{2 \\sigma^2} }. $$\n","\n","Quien es $(x_i - x_j)$\n","donde $x_i, x_j \\in \\mathbb{R}^n$?\n","\n","$$(x_i - x_j)^2 = \\| x_i - x_j \\|^2 = (x_i - x_j)^T (x_i - x_j) = \\langle x_i - x_j, x_i - x_j\\rangle  = (x_i - x_j ) \\cdot (x_i - x_j)  $$\n","\n","Vamos a ser más humildes y pensar en $\\mathbb{R}$. Es decir $x_i, x_j \\in \\mathbb{R}$."],"metadata":{"id":"i-I8cROd1in0"}},{"cell_type":"markdown","source":["Recuerden  la expansion en series de Taylor de la exponencial\n","\n","$$\\mathrm{e}^{\\theta} \\approx 1 + \\theta + \\frac{\\theta^2}{2} + \\cdots + \\frac{\\theta^n}{n!}  \\tag{11} $$\n","\n","La **misión** es encontrar el **feature mapping** correspondiente a este kernel.\n","\n","\n","Usamos (11)\n","\\begin{eqnarray}\n","\\mathrm{e}^{- \\frac{x_i - x_j}{2 \\sigma²} }  &=& \\mathrm{e}^{-\\frac{x_i^2 + x_j^2}{2 \\sigma^2}} \\mathrm{e}^{ \\frac{x_i x_j}{\\sigma^2}} \\\\\n","&=& \\mathrm{e}^{-\\frac{x_i^2 + x_j^2}{2 \\sigma^2}}\n","\\left (\n","    1 +  \\frac{x_i x_j}{\\sigma^2 } +\n","    \\frac{1}{2!}  \\left ( \\frac{x_i x_j}{\\sigma^2} \\right)^2  \n","   +  \n","    \\frac{1}{3!}  \\left ( \\frac{x_i x_j}{\\sigma^2} \\right)^3  \n","    + \\cdots +  \\frac{1}{n!}\n","    \\left ( \\frac{x_i x_j}{\\sigma^2}^n  \\right )\n","    \\right )  \\\\\n","    &=&  \\mathrm{e}^{-\\frac{x_i^2 + x_j^2}{2 \\sigma^2}}\n","    \\left (\n","        (1)(1) + \\frac{x_i}{\\sigma} \\frac{x_j}{\\sigma}  + \\frac{x_i^2}{\\sqrt{2!} \\sigma^2} \\frac{x_j^2}{\\sqrt{2!} \\sigma^2} +\n","        \\frac{x_i^3}{\\sqrt{3!} \\sigma^3} \\frac{x_j^3}{\\sqrt{3!} \\sigma^2} + \\cdots +\n","        \\frac{x_i^n}{\\sqrt{n!} \\sigma^n} \\frac{x_j^n}{\\sqrt{n!} \\sigma^2} +\n","\\right )\n","     \\end{eqnarray}\n"],"metadata":{"id":"b_ckajIu1bx2"}},{"cell_type":"markdown","source":[],"metadata":{"id":"afUA-_oy7Ts4"}},{"cell_type":"markdown","source":["Claramente si\n","\n","$$\n","\\Phi(x) =\n","\\mathrm{e}^{-\\frac{x^2}{2 \\sigma^2} }\n","\\left (\n","    1,  \\frac{x}{\\sigma}, \\frac{x^2}{\\sqrt{2!} \\sigma^2}\n","    , \\frac{x^3}{ \\sqrt{3}! \\sigma^3}, \\cdots, \\frac{x^n}{\\sqrt{n!} \\sigma^n }\n","    \\right )\n","$$\n","\n","Se observa que\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) $$\n","\n","Necesitaríamos un número infinito de operaciones para implementar $\\Phi$. Sin embargo el kernel $k(x,y)$ se implementa con pocas operaciones.\n"],"metadata":{"id":"qjPlWoSj5xWL"}},{"cell_type":"markdown","source":["## algunas propiedades de los Kernels\n","* El kernel es simetrico $k(x,y)=k(y,x)$. Si\n","\n","$$(K)_{ij} = k(x_i, x_j) $$\n","\n","* Recuerden que si una matriz es simétrica entonces se puede diagonalizar.\n","\n","$$K = U \\Lambda U^T \\tag{12}$$\n","Existe un matriz diagonal $\\Lambda$ con autovalores de $K$ y una matriz $U$ con columnas autovectores de $K$ tales que\n","que se cumple la Ecuación (12).\n","\n","Si todos los autovalores son positivos o 0\n","La idea es encontrar el feature mapping $\\Phi$ de este kernel.\n","\n","\\begin{eqnarray}\n","k_{ij} &=&  \\sum_{l=1}^m u_{il} \\lambda_l u_{jl}  \\\\\n","&=& \\sum_{l=1}^m  \\sqrt{\\lambda_l} u_{il} \\sqrt{\\lambda_l} u_{jl} \\\\\n","&=& ( \\sqrt{\\lambda_1} u_{i1}, \\sqrt{\\lambda_2} u_{i2} , \\cdots, \\sqrt{\\lambda_m} u_{im} ) \\cdot\n","\\sqrt{\\lambda_1} u_{j1}, \\sqrt{\\lambda_2} u_{j2} , \\cdots, \\sqrt{\\lambda_m} u_{jm} )\n","\\end{eqnarray}\n","\n"],"metadata":{"id":"yyN486F27IRk"}},{"cell_type":"markdown","source":["De forma que si definimos\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^n &\\to \\mathbb{R}^m \\\\\n","x^{(i)} &\\mapsto& (\\sqrt{\\lambda_1} u_{i1}, \\sqrt{\\lambda_2} u_{i2} , \\cdots, \\sqrt{\\lambda_m} u_{im} )\n","\\end{eqnarray}\n","\n","Entonces\n","\n","$$(K)_{ij} = k(x^{(i)}, x^{(j)}) = \\Phi(x^{(i)}) \\cdot \\Phi(x^{(j)})  $$"],"metadata":{"id":"fTL7-42K81rv"}},{"cell_type":"markdown","source":["Vimos que si la matriz no es negativa definida tenemos la represenaci'on del feaure mapping a partir de $k$.\n","\n","Ahora bien, veamos que si $k(x,y)=\\Phi(x) \\cdot \\Phi(y)$ entonces la matriz $k$ es no negativa definida.\n","\n","Es decir $c \\in \\mathbb{R}^n$\n","\n","\\begin{eqnarray}\n","c^T K C &=& \\sum_i \\sum_j c_i c_j k(x_i, x_j) = \\sum_i \\sum_j c_i c_j \\Phi(x_i) \\cdot \\Phi(x_j) = \\sum_i c_i \\Phi(x_i) \\cdot \\sum_j c_j \\Phi(x_j)  \\\\\n","&=& \\|\\sum_i c_i \\Phi(x_i) \\|^2  \\ge 0.\n","\\end{eqnarray}\n","\n","El analisis de kernels viene del an'alisis funcional en espacios de Hilbert.\n","\n","# Proxima clase: Metodos no supervisados."],"metadata":{"id":"b_reR_Wk9d6M"}}]}