{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZITRCFBP7oinYIw5y5MWT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Teorema espectral para operadores (matrices) autoadjuntos.\n","\n","**Teorema 2.2.2**: Sea $A$ una matriz autoadjunta ($A^* = A$),\n","entonces los autovalores de $A$ son reales y autovectores correspondientes a autovalores distintos son ortogonales.\n","\n","**Prueba**:\n","\n","1. Probemos que los autovalores son reales\n","Asumamos que $\\lambda_j$ es un autovalor de $A_j$ con autovector $u_j$. Sin perdida de generalidad podemos $\\| u_j \\|=1$.\n","Primero calculamos la forma cuadratica $\\langle A u_j, u_j \\rangle$\n","\n","$$\\langle A u_j , u_j \\rangle = \\langle \\lambda_j u_j, u_j \\rangle = \\overline{\\lambda_j} \\langle u_j, u_j \\rangle = \\overline{\\lambda_j} \\| u_j \\|^2 = \\overline{\\lambda_j} \\tag{1}$$\n","\n","Ahora veamos que\n","\n","$$ \\langle A u_j, u_j \\rangle = \\langle u_j, A^* u_j \\rangle  \\tag{2} $$\n","\n","$$\\langle A u_j, u_j \\rangle = (A u_j)^* u_j = u_j^* (A^* u_j) = \\langle u_j, A^* u_j \\rangle $$\n","De aca que combinando (1) con (2)\n","y tambien usamos que $A= A^*$\n","\n","\n","\n","\n","$$\\overline{\\lambda_j} =  \\langle A u_j, u_j \\rangle = \\langle u_j, A^* u_j \\rangle =  \\langle u_j , A u_j \\rangle = \\langle u_j, \\lambda_j u_j \\rangle = \\lambda_j \\langle u_j, u_j \\rangle = \\lambda_j $$\n","\n","De forma que $\\overline{\\lambda_j} = \\lambda_j$ o sea que $\\lambda \\in \\mathbb{R}$.\n","\n","Si $\\overline{a} = a$ es por que $a$ es real. Veamos\n","\n","$$a = b + \\mathrm{i} c $$\n","$$\\overline{a} = b - \\mathrm{i} c $$\n","\n","$$ b + \\mathrm{i}c = b - \\mathrm{i}c$$\n","$$ \\mathrm{i} c = - \\mathrm{i}c $$\n","$c=0$.\n","\n","2. Probamos que si $\\lambda_1 \\ne \\lambda_2$ para $\\lambda_1, \\lambda_2$ autovalores de $A$ correspondientes a autovectores $u,v$ entonces $\\langle u, v \\rangle = 0$.\n","\n","Comenzamos con la forma cuadratica\n","\n","$$ \\langle A u, v \\rangle = \\langle \\lambda_1 u, v \\rangle = \\lambda_1 \\langle u, v \\rangle    \\tag{3} $$\n","\n","Ahora como $A$ es autoadjunto $A = A^*$, y\n","\n","\n","$$\\langle A u, v \\rangle = \\langle u, A v\\rangle = \\langle u, \\lambda_2 v \\rangle = \\lambda_2 \\langle u, v \\rangle \\tag{4} $$ (por (2) extendida)\n","\n","Como (3)=(4)\n","\n","$$\\lambda_1 \\langle u , v \\rangle = \\lambda_2 \\langle u, v \\rangle $$\n","\n","$$(\\lambda_1 - \\lambda_2) \\langle u, v \\rangle = 0 $$\n","Como $\\lambda_1 - \\lambda_2 \\ne 0$\n","entonces\n","$$\\langle u, v \\rangle = 0. $$\n","\n","Este teoremas cierto aun si $\\lambda_1 = \\lambda_2$. Pero la prueba no la hacemos en clase. Esta en las notas.\n","\n","Este hecho es importante por que nos lleva a factorizar la matriz $A$ en tres factores importantes. Veamos como\n","\n","Sean $\\lambda_1, \\lambda_2, \\cdots, \\lambda_n$ autovalores de $A$ (distintos o iguales). Entonces como los autovectores son ortogonales, son linealmente independientes y forman una base para $\\mathbb{C}^n$.  Podemos asumir que los autovectores $x_1, x_2, \\cdots, x_n$ estan normalizados.\n","\n","\n","Ahora bien\n","$$A x_i = \\lambda_i x_i \\quad , \\quad i=1,2, \\cdots, n $$\n","\n","Como son ortogonales\n","$$ x_i^* x_j = \\delta_{ij}  $$\n","\n","Ahora entonces\n","\n","$$ x_i^* A x_j = x_i^* \\lambda_j x_j = \\lambda_j \\delta_{ij} \\tag{5} $$\n","Esto forma la matriz diagonal  $\\Lambda$ con elementos de la diagonal $\\lambda_1, \\lambda_2, \\cdots , \\lambda_n$.\n","\n","Como los $x_i$ son columnas ortogonales, forman una matriz ortogonal $Q$. Y cuando $i,j=1,2, \\cdots, n$ la ecuacion (5)\n","se puede escribir como\n","\n","$$ Q^* A Q = \\Lambda  \\tag{6} $$\n","\n","Multiplicamos la (6) a izquierda por $Q$ y a derecha por $Q^*$\n","\n","$$ Q Q^* A Q Q^*= Q \\Lambda Q^*   $$\n","$$ I A I= Q \\Lambda Q^*   $$\n","$$  A = Q \\Lambda Q^*   $$\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"hZXxwCaPqlDf"}},{"cell_type":"code","source":[],"metadata":{"id":"OYo1FCb2dgI8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Aplicacion directa de este resultado.\n","Considere $x \\in \\mathbb{R}^n$ y la forma cuadratica $(A=A^*)$,\n","asumamos que $A$ es real\n","\n","$$\\langle x, A x \\rangle = x^T A x = \\sum_{i=1}^n \\sum_{j=1}^n x_i a_{ij} x_j = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j  $$\n","\n","El punto es que para calcular este valor se necesitan $2 n^2$\n","multiplicaciones y $n^2-1$ sumas\n","\n","$$a_{11} x_1 x_1 + a_{12} x_1 x_2 + a_{21} x_2 x_1  + a_{22} x_2 x_2  $$\n","\n","Pero, tampoco es asi por que $A$ es simetrica. Enttonces los cruzados $x_1 x_2$ se cuentan como uno solo con coeficientes 2. Por ejmplo $a_{12} x_1 x_2 + a_{21} x_2 x_1 = 2 a_{12} x_1 x_2$.\n","\n","La idea es que este sistema es complicado por los terminos cruzados ($i \\ne j$). Los de la diagonal $i=j$ son cuadrados puros.\n","\n","Se puede simplifcar el problema de la siguiente forma, como $A$ es autoadjunta\n","\n","$$x^T A x = x^T Q \\Lambda Q^T x  \\tag{7}$$\n","Yo puedo cambiar variable (cambio de base)\n","\n","$$y= Q^T x, y^T = x^T Q $$\n","de forma que (7) se puede escribir como\n","\n","$$ y^T \\Lambda y = \\sum_{i=1}^n \\sum_{j=1}^n y_i \\lambda_j \\delta_{ij} y_j =  \\sum_{i=1}^n \\lambda_i y_i^2  $$\n","Esta ultima expresion son $n-1$ sumas y $n$ multiplicaciones\n","teniendo previamente los cuadrados, o $2n$ multiplicaciones teniendo en cuenta los cuadrados). Pero de $2n^2$ a $2n$ hay mucha ganancia.\n","\n"],"metadata":{"id":"m7_VwwHWc1l8"}},{"cell_type":"markdown","source":["Recuerde que en ML casi todo es multiplicacion de matrices. Hemos visto diferentes formas de multiplicar matrices, pero necesitamos mas.\n","\n","**Terema 2.2.5**: Sea $A$ un matriz $m \\times n$, $B$ es una matriz $n \\times p$, $x$ es un $n$ vector y $\\Lambda$ es una matriz $n \\times p$ con valores $\\lambda_i$ en la diagonal y ceros en las otras componentes. Entonces:\n","\n","1.\n","$$AB = [ AB_1 | AB_2 | \\cdots | AB_p] \\tag{8} $$\n","donde $B_i$, $i=1,2, \\cdots, p$ son las columnas de $B$\n","Esta propiedad (8) es importante por que vemos que no necesitamos calcular todo el producto $AB$ para encontra la columna $j$ de $AB$.\n","\n","2.\n","$$ Ax = \\sum_{j=1}^n x_j A_j \\tag{9}  $$\n","donde $A_j$ es la columna $j$ de $A$.\n","En palabras: El producto de una matriz por un vector es la suma ponderada de las columnas de la matriz donde los pesos son las componentes del vector.  En este sentido el producto $Ax$ nos genera el rango la matriz.\n","\n","3. Si $p \\ge n$\n","\n","$$ A \\Lambda  = [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_n A_n | 0_{n+1} | \\cdots | 0_p] $$\n","donde $A_i$ es la columna $i$ de la matriz $A$.\n","Si $p \\le n$\n","\n","$$ A \\Lambda  = [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_n A_n ] \\tag{10} $$\n","\n","4. Si $u_i$ son vectores columna de una matriz $m \\times m$  $U$ y $b \\in \\mathbb{C}^n$ entonceso\n","\n","$$U^* b = \\begin{pmatrix} u_1^* b \\\\ u_2^* b \\\\ \\vdots \\\\ u_m^* b  \\end{pmatrix}  \\tag{11} $$\n","\n","**Prueba**:\n","1.  Probamos (8).\n","Sea $C=AB$, expandimos los dos lados de (8)\n","\n","El de la izquierda. (definicion de producto de matrices)\n","$$c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj} $$\n","Ahora veamos el de la derecha. Tomomos el elemento $i$ de la derecha. Dado $j=1,2, \\cdots, n$\n","\n","$$(AB_j)_i=  \\sum_{k=1}^n a_{ik} b_{kj} $$\n","pero esto coincide con $c_{ij}$ como esta arriba.\n","\n","2. La ecuacion (9)\n","Tomamos la componente $i$ de esta ecuacion\n","\n","$$(A x)_i = \\sum_{j=1}^n a_{ij} x_j = \\left ( x_j A_j \\right )_i $$\n","de forma que $Ax = \\sum_{j=1}^n x_j A_j$ pues esto es cierto para $i=1,2, \\cdots, n$.\n","\n","3. Asumamos que $p \\le n$\n","Piense que $\\Lambda = [\\Lambda_1 | \\Lambda_2  | \\cdots, | \\Lambda_p ]$\n","\n","Utilizando (8)\n","\\begin{eqnarray}\n","A \\Lambda = [A \\Lambda_1 | A \\Lambda_2 | \\cdots | A \\Lambda_p]\n","\\end{eqnarray}\n","Ahora usamos la (9)\n","\\begin{eqnarray}\n","A \\Lambda = [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_p A_p]\n","\\end{eqnarray}\n","\n","que pasa si $p>n$, entonces se generan columnas adicionales de puros ceros, por que las columnas de $\\Lambda$ despues de la columna $n$ son todas 0\n","\n","4. La fila $i$ de $U^* b$ esta dada por $(U^* b)_i$\n","\n","$$(U^* b)_i = \\sum_{j=1}^m \\overline{u_{ji}} b_j = \\langle u_i, b \\rangle  = u_i^* b $$\n","donde $u_i$ son vectores columna de $U$.\n","De forma que\n","\n","$$U^* b= \\begin{pmatrix}\n","u_1^* b \\\\\n","u_2^* b \\\\\n","\\vdots  \\\\\n","u_m^* b \\\\\n","\\end{pmatrix}  $$"],"metadata":{"id":"aauDvOEJgX8w"}},{"cell_type":"markdown","source":["En esta parte del curso queremos \"factorizar\" (descomponer) la matriz $A$ en tres partes importantes\n","\n","1. Espectral para autoadjuntas\n","$$ A = Q \\Lambda Q^*$$\n","2. Luego del descanso probamos que si los autovectores de $A$ son linealmente independientes, formamos la matriz $P$ de autovectores y escribimos\n","\n","$$A = P \\Lambda P^{-1} $$\n","\n","3. SVD: Singular Value Decomposition\n","\n","$$ A = U \\Sigma V^T $$\n","explico luego que son $U, \\Sigma$ y $V^T$\n","En este $A$ es cualquier matriz $m \\times n$.\n","\n","7:40 pm\n","\n","Asuma que la matriz $A$ , $n \\times n$  tiene $n$ autovectores linealmente independientes (no tiene que se auto-adjunta).\n","Entonces,  esos vectores son $P_i$, $i=1,2, \\cdots, n$ satisfacen\n","\n","$$A P_i = \\lambda_i P_i $$\n","Podemos escribir la matriz\n","\n","Usando, la propiedad 8 y  la (10) con $p=n$.\n","$$ AP = P \\Lambda $$\n","\n","Como las columnas de $P$ son linealmente independientes entonces $P$ es no singular, existe $P^{-1}$.\n","\n","Multiplicamos por $P^{-1}$ a izquierda.\n","\n","$$ P^{-1} AP = P^{-1} P \\Lambda $$\n","como $P^{-1} P= I$\n","\n","$$\\Lambda = P^{-1} A P  \\tag{12} $$\n","A esto se le conoce como **diagonalizacion** de la matriz $A$.\n","Tambien se puede usar la diagonalizacion para encontrar los ejes principales de la elipse (cambio base).\n","\n","\n","Multiplicamos la ecuacion (12) a izquierda por $P$ y a derecha por $P^{-1}$\n","\n","\n","\n","$$P \\Lambda P^{-1} = P P^{-1} A P P^{-1}    $$\n","$$A = P \\Lambda P^{-1}      $$\n","Esta es otra \"factorizacion\" (descomposicion de $A$ como el producto de tres matrices importantes.\n","\n","**Teorema 2.2.6**(Diagonalizacion). Si $A \\in \\mathbb{C}^{n \\times n}$ tiene $n$ autovectores linealmente independientes, entonces existte una matriz $P$, donde las columnas $P_i$ son estos autovectores y una matriz $\\Lambda$ diagonal con los autovalores de $A$ tal que\n","\n","$$ A = P \\Lambda P^{-1} \\tag{13}$$\n","$$ \\Lambda = P^{-1} A P$$\n","\n","Que pasa si las columnas de $A$ no son linealmente independientes? El teorema se puede extender, pero ahora la matriz $\\Lambda$ no es diagonal, es \"casi\" diagonal. Es una matriz de\n","[Jordan](https://es.wikipedia.org/wiki/Forma_can%C3%B3nica_de_Jordan).\n","\n","Que ventaja puede tener esta representacion. La ventaja es que se puede exponenciar la matriz facilmente.\n","\n","**Teorema 2.2.7** (exponenciacion de matrices). Asuma que $A$ es diagonalizable, es decir\n","\n","$$A = P \\Lambda P^{-1} $$\n","donde $\\Lambda$ es una matriz $n \\times n$ diagonal.\n","Entonces\n","\n","$$A^{k} = P \\Lambda^k P^{-1} $$\n","donde las entradas de $\\Lambda^k$ son $\\lambda^k$.\n","Esto simplifica enormemente la exponenciacion de matrices.\n","\n","La prueba la hago por induccion. Es como el domino effect\n","\n","* Tumba la primera ficha $k=1$\n","* Necesito que al tumbar $k$ empuje a la $k+1$ (que la distancia entre $k$ y $k+1$ sea menor que la altura del domino.\n","\n","[induccion matematica](https://en.wikipedia.org/wiki/Mathematical_induction).\n","\n","* Probemos que se cumple para $k=1$.\n","\n","\n","$$A^{1} = P \\Lambda^1 P^{-1} $$\n","Esta es la identidad (13).\n","\n","* Probemos que si es valido para $k$ tambien es para $k+1$.\n","\n","Es decir que si\n","$$A^{k} = P \\Lambda^k P^{-1} $$\n","entonces\n","$$A^{k+1} = P \\Lambda^{k+1} P^{-1} $$\n","\n","Veamos\n","\n","\\begin{eqnarray}\n","A^{k+1} &=& A^k A \\\\\n","&=& P \\Lambda^k (P^{-1} P) \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^k \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^{k+1}  P^{-1}.\n","\\end{eqnarray}\n","\n","**Definicion 2.2.3** (Positiva definida, negativa definida, no negativa definida). Una matriz autoadjunta es **positiva definida** si todos sus autovalores son positivos. Es **negativa definida** si todos sus autovalores son negativos, **no negativa definida** si todos sus autovalores son positivos o 0.\n","\n","Se puede probar la siguiente equivalencia (que es usada en la literatura como definicion de positiva definida).\n","\n","**Teorema 2.2.8**: (positiva definida). Si $A$ es positiva definida entonces esto equivale a que\n","\n","$$x^* A x > 0  $$\n","donde $A \\in \\mathbb{C}^{n \\times n}$ , $x \\in \\mathbb{C}^n$ es arbitrario.\n","Ademas $A$ tiene inversa y $A^{-1}$ tambien es positiva definida.\n","La nonegativa definida tambien se llama **positiva semidefinida**. En este caso\n","\n","$$x^* A x \\ge 0  \\tag{100} $$\n","Se puede probar, que si es positiva semidefinida entonces es equivalente a que todos los autovalres son 0 o positivos y equivalente a (100).\n","\n","## Teorema SVD: Singular Value Decomposition.\n","**Teorema 2.3.2** (SVD): Dada cualquier matriz $A \\in \\mathbb{C}^{m \\times n}$, existen dos matrices\n","$U \\in \\mathbb{C}^{m \\times m}$,\n","$V \\in \\mathbb{C}^{n \\times n}$,\n","y una matriz diagonal $\\Sigma \\in \\mathbb{R}^{m \\times n}$ con entradas $\\sigma_{ij}>0$ llamados **valores singualares** tal que\n","\n","$$A = U \\Sigma V^*. $$\n","\n","Esta prueba que vamos a hacer se debe al Hungaro Cornelius Lanczos (tengo la referencia)\n","\n","\n"," C. Lanczos. Linear Differential Operators. Classics in Applied Mathematics. Society\n","for Industrial and Applied Mathematics, 1997.\n","\n","\n","**Prueba**:\n","Construimos una matriz auxiliar $S$\n","\n","$$S = \\begin{pmatrix} 0 & | & A  \\\\ A^* &|& 0\\end{pmatrix}  $$\n","donde $S \\in \\mathbb{C}^{(m+n) \\times (m+n)} \\tag{14}$\n","Usamos el teorema espectral, pues $S$ es autoadjunta $S=S^*$\n","entonces sus autovectores son ortogonales y sus autovalres son reales.  Asuma que los autovalores de $S$ son $\\lambda_i$ con autovectores $w_i$.\n","\n","$$ S w_i = \\lambda_i w_i $$\n","$w_i$ lo escribimos como\n","\n","$$w_i = \\begin{pmatrix} u_i \\\\ v_i \\end{pmatrix}  $$\n","\n","Mirando a $S$ (14) vemos entonces que esto genera las siguientes dos ecuaciones\n","\n","\\begin{eqnarray}\n","A^* u_i = \\lambda_i v_i \\tag{15} \\\\\n","A v_i = \\lambda_i u_i \\tag{16}\n","\\end{eqnarray}\n","\n","Ahora multiplicamos la primera de las dos ecuaciones por $A$ y la segunda por $A^*$\n","\n","\\begin{eqnarray}\n","A A^* u_i = \\lambda_i A v_i  = \\lambda_i \\lambda_i u_i = \\lambda_i^2 u_i\\\\\n","A^* A v_i = \\lambda_i A^* u_i = \\lambda_i \\lambda_i v_i = \\lambda_i^2 v_i  \\\\\n","\\end{eqnarray}\n","\n","Vemos que las columnas de $U$, llamelas $u_i$ son autovectores de la matriz $A A^* \\in \\mathbb{C}^{m \\times m}$ con autovalores $\\lambda_i^2$ y que\n"," las columnas de $V$, llamelas $v_i$ son autovectores de la matriz $A^* A \\in \\mathbb{C}^{n \\times n}$ con autovalores $\\lambda_i^2$.\n","\n","Es posible que hallan autovectores 0 de la matriz $U$.\n","A la matriz de autovectores que no son 0 se le llama $U_r$,\n","y a la parte de 0 se le llama $U_0$.  Igualmente $V_r$ a la matriz con columnas no 0 y $V_0$ la parte de la matriz con puros 0. Asumimos que los autovalores que no son 0 son $r$\n","$\\lambda_1, \\lambda_2, \\cdots, \\lambda_r \\ne 0$,\n","$\\lambda_{r+1}, \\lambda_{r+2} \\cdots, \\lambda_{p} = 0$,\n","$p \\min {m, n}$. Los $\\lambda_i$ forman la matriz diagonal $\\Lambda_r$.\n","Con esto tenemos que\n","De las ecuaciones (15) y (16) y la (10)\n","\n","$$A^* U_r = V_r \\Lambda_r  \\quad, \\quad A^* U_0 = 0$$\n","$$A V_r = U_r \\Lambda_r  \\quad, \\quad A V_0 = 0$$\n","\n","\n","Si definimos\n","\n","$$\\Sigma = \\begin{pmatrix} \\Lambda_r &|& 0 \\\\ 0 &|& 0  \\end{pmatrix} $$\n","\n","Entonces encotramos que\n","$$AV = U \\Sigma \\tag{17}$$,\n","probemos esto.\n","\n","\n","\n","\n","\n","\n","\n","propios\n"],"metadata":{"id":"4L_GCGyDqrAb"}},{"cell_type":"markdown","source":["Ojo: Revisar multiplicacion de matrices por bloques:\n","$$ AV = A [ V_r | V_0 ] = [A V_r | A V_0] = [ U_r \\Lambda_r | 0 ] =[ U_r \\Lambda_r + U_0 \\times 0 | U_r \\times 0 + U_0 \\times 0] = [U_r | U_0] \\; \\Sigma  $$\n","\n","En la ecuacion (17), multiplicamos por $V^*$ a derecha.\n","$U,V$ son matrices ortogonales (por el teorema espectral).\n","Entonces $VV^*=I$\n","y\n","\n","\n","$$AVV^* = U \\Sigma V^* $$\n","como $V V^*=I$\n","entonces\n","\n","$$A = U \\Sigma V^*. $$"],"metadata":{"id":"1B3lKp4G6h6O"}},{"cell_type":"markdown","source":["Los autovalores $\\lambda_i^2$ son tales que\n","$\\lambda_i=\\sqrt{\\lambda_i^2}=\\sigma_i$\n","y los $\\sigma_i$ se llaman **valores singulares**.\n","Realmente $\\sqrt{\\lambda_i^2} = | \\lambda |$. Pero como $A^*A$ es positiva semidefinida, entonces $\\lambda>0$ y $\\sqrt{\\lambda^2} = \\lambda_i =\\sigma_i$."],"metadata":{"id":"FJWAVwRN7yrd"}},{"cell_type":"markdown","source":["**Ejemplo 2.3.1**: Encuentre la SVD de la siguiente matriz\n","\n","$$A = \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4 \\end{pmatrix} $$\n","\n","**Solucion**:\n","* Encuentre $A^* A$ y  $A A^*$.\n","* Encuentre autovalores y autovectores de estas matrices. Los autovalores no cero son los mismos. Una de estas (la mas grande, $3 \\times 3$) tiene un autovalor 0.\n","* Los valores singulares son las raices cuadradas de los autovalores de $A^*A$ y $A A^*$.\n","* Ordene los valores singulares de mayor a menor y ordene los autovectores de la misma forma.\n","\n","* Construya $U,V$ como las columnas en el orden el punto anterior $u_i, v_i$.\n","\n","1. Observe que A^* A$ es $3 \\times 3$ y que $A A^*$ es $2 \\times 2$.\n","\n","\n","Consejo: Primero calcular la mas pequenha y de ahi los autovalores,\n","pues en el caso de las $2 \\times 2$ el polinomio caracteristico es\n","cuadratico, sino tendria que resolver un cubico. Aunque a decir verdad, uno de los autovalores es 0 y tambien se reduce a una cuadratica.\n","\n","$$A A^* = \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4\\end{pmatrix}\n","\\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\\\ 3 & 4 \\end{pmatrix} =\n","\\begin{pmatrix} 10 & 13 \\\\ 13 & 21\\end{pmatrix}    $$\n","\n","Los autovalores de $A A^*$ cumplen\n","\n","$$p(\\lambda) = 0 $$\n","Resolviendo este polinomio (no lo muestro)\n","\n","$$\\lambda_1^2 = \\frac12 ( 31 + \\sqrt{797}) \\approx 29.62 $$\n","$$\\lambda_2^2 = \\frac12 ( 31 - \\sqrt{797}) \\approx 1.38 $$\n","El otro es $\\lambda_3=0$\n","\n","Los sigmas son las raices cuadradas.\n","\n","$$\\sigma_1 = \\sqrt{\\lambda_1^2} \\approx 5.44 $$\n","$$\\sigma_2 = \\sqrt{\\lambda_2^2} \\approx 1.17 $$\n","\n","\n","2. Calculemos $A^* A$\n","\n","$$A^* A = \\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\\\ 3 & 4\\end{pmatrix}\n","\\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4 \\end{pmatrix}=\n","\\begin{pmatrix} 2 & 2 & 7 \\\\ 2 & 4 & 8 \\\\ 7 & 8 & 25 \\end{pmatrix}  $$\n","Si resuelve el polinomio caracteristico de esta matriz le da una cubica, pero una de las soluciones es $\\lambda_3=0$ y las otras dos ya las hallo.\n","\n","Autovectores de $A A^*$\n","\n","$$U_1 = \\begin{pmatrix} 0.55 \\\\ 0.83 \\end{pmatrix} $$\n","$$U_2 = \\begin{pmatrix} -0.83 \\\\ 0.55 \\end{pmatrix} $$\n","\n","Los autovectores de $A^* A$ son\n","\n","\n","$$V_1 = \\begin{pmatrix} 0.25 \\\\ 0.31 \\\\ 0.92 \\end{pmatrix} $$\n","$$V_2 = \\begin{pmatrix} -0.24 \\\\ 0.94 \\\\ 0.24 \\end{pmatrix} $$\n","$$V_3 = \\begin{pmatrix} 0.92 \\\\ -0.24 \\\\ 0.31 \\end{pmatrix} $$\n","\n","Se construyen $U, V, \\Sigma$,\n","\n","$$A = U \\Sigma V^T =\n","\\begin{pmatrix}\n","0.54 & -0.83 \\\\\n","0.83 & 0.55\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","5.44 & 0 & 0 \\\\\n","0 & 1.17 & 0\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","0.25 & 0.31 & 0.92 \\\\\n","-0.24 & 0.94 & 0.24 \\\\\n","0.92 & -0.24 & 0.31\n","\\end{pmatrix}\n","$$\n","\n","\n"],"metadata":{"id":"tAak1Dhq-zN8"}},{"cell_type":"markdown","source":[],"metadata":{"id":"cxSAej6eDu16"}}]}