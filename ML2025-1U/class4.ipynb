{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEQSp+wAMN6J2kZ49SEYbO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Singular Value Decomposition (SVD): Descomposicion en valores singulares\n","Hasta el momento hemos visto dos formas de \"factorizar\" (descomponer) una matriz  como el producto de otras tres matrices. Por conveniencia.\n","\n","1. El teorema de descomposicion espectral:\n","$$ A = Q \\Lambda Q^T$$\n","donde $Q$ es ortogonal con las columnas los atuvectores de $A$ y $\\Lambda$ diagonal con los autovalores de $A$. Restriccion: $A$ es autoadjunta (cuadrada)\n","\n","2. El teorema de diagonalizacion\n","$$ A = P \\Lambda P^{-1}$$\n","Donde $P$ es una matriz con columnas los autovectores de $A$ y $\\Lambda$ diagonal con los autovalores. Restriccion: $A$ debe tener autovectores linealmente independientes.  Que pasa si los autovectores de $A$ no son linealmente independientes. Entonces\n","\n","$$ A = P J P^{-1}$$\n","donde $J$ se llama la matriz de\n","[Jordan](https://en.wikipedia.org/wiki/Jordan_normal_form).\n","o forma canonica de Jordan.\n","\n","\n","Ahora\n","3.0 La Matriz $A$ para SVD puede ser **cualquiera**. Restricciones ninguna.\n","\n","3. El SVD: Esta clase\n","## Teorema SVD:\n","Singular Value Decomposition.\n","\n","**Teorema 2.3.1** (SVD)\n","Dada una matriz $A \\in \\mathbb{C}^{m \\times n}$, exiten matrices ortogonales $U \\in \\mathbb{C}^{m \\times m}$, $V \\in \\mathbb{C}^{n \\times n}$ y una matriz diagonal $\\Sigma \\in \\mathbb{R}^{m \\times n}$ con componentes $\\sigma_{ii} \\ge 0$, como es diagonal, las componentes por fuera de la diagonal son 0, y podemos renombrar los elementos de la diagonal $\\sigma_i$, $i=1, \\cdots, \\min \\{m,n\\}$\n","donde\n","$$A = U \\Sigma V^* $$\n","\n","**Prueba**: Esta prueba se debe a Cornelius Lanczos (Hungaro)\n","bibliografia en la pagina de la UVirtual.\n","\n","Primero escriba una matriz extendiada $S$\n","\n","$$S = \\begin{pmatrix} 0 &|& A \\\\ - &-&-\\\\ A^* &|& 0  \\end{pmatrix}  $$\n","\n","Donde $S \\in \\mathbb{C}^{m \\times n}$.\n","Ahora $S$ es autoajunta (pruebelo) por lo tanto usando el teorema de descomposicoin espectral\n","\n","\n","$$S w_i = \\lambda_i w_i $$\n","con\n","\n","$$ w_i = \\binom{u_i}{v_i} $$\n","donde $u_i \\in \\mathbb{C}^m$ y $v_i \\in \\mathbb{C}^n$.\n","Ademas $w_i$ son **ortogonales** (por el teorema de descomposicion espectral).\n","\n","Multiplicar por bloques\n","$$S w_i =  \\binom{A v_i }{A^* u_i} $$\n","\n","De forma que\n","\n","$$A v_i = \\lambda_i u_i \\tag{1}  $$\n","$$A^* u_i = \\lambda_i v_i  \\tag{2}  $$\n","\n","Los $\\lambda_i$ son reales (TDE).\n","Multiplicamos la primera de estas dos a izquierda por $A^*$ y la segunda por $A$\n","\n","$$A^* A v_i = \\lambda_i A^* u_i  $$\n","$$A A^* u_i = \\lambda_i A v_i  $$\n","\n","$$A^* A v_i = \\lambda_i \\lambda_i v_i  = \\lambda_i^2 v_i  $$\n","$$ A A^* u_i = \\lambda_i \\lambda_i u_i = \\lambda_i^2 u_i $$\n","\n","Es decir $v_i$ son autovectores de $A^* A$\n","Es decir $u_i$ son autovectores de $A A^*$\n","y que $\\lambda_i$ son autovalores de ambos $A^*A$ y de $A A^*$.\n","\n","Tenemos dos matrices $A^* A \\in \\mathbb{C}^{n \\times n}$\n"," $A A^* \\in \\mathbb{C}^{m \\times m}$\n","Los autovalores $\\lambda_1, \\lambda_2 , \\cdots, \\lambda_p$ con\n","$p = \\min \\{ m, n \\}$ no estan garantizados positivos.  Pueden haber algunos que sean 0 . Sea $r$ el ultimo autovalor que no es 0. Precisamente los autovalores que son 0 noquean (knock-down) esa dimension.  \n","\n","Vamos a escribir la matrice $U$ como los autovectores de $A^*A$,\n","la matrice $V$ como los autovectores de $AA^*$\n","Definamos la matriz $U_r$ como aquellas columnas que pertenecen a autovalores no 0.\n","Definamos la matriz $U_0$ como aquellas columnas que pertenecen a autovalores iguales  0.\n","\n","Definamos la matriz $V_r$ como aquellas columnas que pertenecen a autovalores no 0.\n","Definamos la matriz $U_r$ como aquellas columnas que pertenecen a autovalores iguales  0.\n","Sea $\\Lambda_r$ la matriz $r \\times r$ de autovalores no cero.  \n","Es decir, de (2) y teorema clase anterior.\n","\n","$$A^* U_r = V_r \\Lambda_r $$\n","$$A^* U_0 = 0 $$\n","\n","Igualmente de (1)\n","$$A V_r = U_r \\Lambda_r $$\n","$$A V_0 = 0 $$\n","\n","Aca recuerdo la multiplicion por bloques (tarea de lectura)\n","\n","$$A V = A [ V_r | V_0 ] = [A V_r | A V_0] = [U_r \\Lambda_r | 0] = [ U_r \\Lambda_r + U_0 \\times 0 | U_r \\times 0 + U_0 \\times 0 ] = U \\Sigma  $$\n","donde\n","$$\\Sigma = \\begin{pmatrix} \\Lambda_r & | & 0  \\\\ --&-&-- \\\\ 0 &|& 0  \\end{pmatrix}  $$\n","\n","De forma que si\n","\n","$$AV = U\\Sigma $$\n","y como $V$ es ortogonal multiplicamos a ambos lados (a derecha) por $V^*$ y obtenemos\n","\n","$$ A = U \\Sigma V^* $$\n","La matriz $\\Sigma$ tiene la submatriz $\\Lambda_r$ que es cuadrada de orden $r \\times r$. A los autovalores (positivos) son $\\lambda_i = \\sqrt{\\lambda_i^2}$.\n","Pregunta por que no $| \\lambda_i |$.  Definicion $\\lambda_i = \\sqrt{\\lambda_i^2} = \\sigma_i$ se llaman **valores singulares**.\n","\n","**Ejemplo 2.3.1**: Encuentre la SVD de la siguiente matriz\n","\n","$$A = \\begin{pmatrix} 1 & 0 & 3 \\\\  1 & 2 & 4 \\end{pmatrix} $$\n","Redondee los resultados a dos cifras decimales.\n","\n","**Algoritmo**:\n","1. Encontrar $A A^*$, $A^* A$\n","2. Encontrar los autovalores/vectores de estas dos matrices\n","3. Los $\\sigma_i$ son $\\sqrt{\\lambda_i^2}$ donde $\\lambda_i^2$ son los autovalores de $A^*A$ o $A A^*$.\n","4. Los autovectores de $A A^*$ son las columnas de $U$\n"," Los autovectores de $A^* A$ son las columnas de $V$\n","\n","Ojoooo.  Establezca dos criterios\n","1. Los valores singulares son positivos o 0\n","2. Los valores singulares se ordenan de mayor a menor y los vectores correspondientes ($u_i, v_i$) se ordenan en ese orden.\n","\n","**Solucion**:\n","1. Encontremos $A A^*$\n","\n","$$A A^* = \\begin{pmatrix}  1 & 0 & 3 \\\\ 1 & 2 & 4\\end{pmatrix}\n","\\begin{pmatrix} 1 & 1 \\\\ 0 &2 \\\\ 3 & 4 \\end{pmatrix} =\n","\\begin{pmatrix} 10 & 13 \\\\ 13 & 21 \\end{pmatrix}  $$\n","\n","Los autovalores de $ A A^*$ satisfacen el polinomio caracteristico $p(\\lambda) = \\det ( A A^* - \\lambda I)$. Estos son (trabajo no hecho en clase, el polinomio caractestico es de oden 2)\n","\n","$$ \\lambda_1^2 = \\frac12 ( 31 + \\sqrt{797}) \\approx 29.62  $$\n","$$ \\lambda_2^2 = \\frac12 ( 31 - \\sqrt{797}) \\approx 1.38  $$\n","\n","Los autovectores normalizados de $A^* A$.\n","Se encuentra\n","\n","$$U_1 = \\binom{0.55}{0.83} \\quad , \\quad u_2 = \\binom{-0.83}{0.55} $$\n","\n","Ahora la matriz $A^* A$\n","\n","$$A^* A = \\begin{pmatrix}\n","1 & 1 \\\\ 0 & 2 \\\\ 3 & 4\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","1 & 0 & 3 \\\\ 1 & 2 & 4\n","\\end{pmatrix}\n","=\n","\\begin{pmatrix}\n","2 & 2 & 7 \\\\ 2& 4 & 8 \\\\ 7 & 8 & 25\n","\\end{pmatrix} $$\n","Los autovalores son los mismos de arriba mas un autovalor $\\lambda_3=0$\n","\n","Los autovectores, normalizados\n","\n","$$V_1 = \\begin{pmatrix} 0.25 \\\\ 0.31 \\\\ 0.92 \\end{pmatrix}  \\quad , \\quad\n","V_2 = \\begin{pmatrix} -0.24\\\\ 0.94 \\\\ -0.24 \\end{pmatrix}   \\quad , \\quad\n","V_3 = \\begin{pmatrix} 0.92 \\\\ -0.24 \\\\ 0.31 \\end{pmatrix}  $$\n","\n","Los sigmas, las raices cuadradas de los $\\lambda_i^2$\n","\n","$$\\sigma_1 = \\sqrt{\\lambda_l^2} = \\sqrt{29.62} \\approx 5.44 $$\n","$$\\sigma_2 = \\sqrt{\\lambda_2^2} = \\sqrt{1.38} \\approx 1.17 $$\n","\n","El resultado de las descomposicion es\n","\n","$$A = U \\Sigma V^T =\n","\\begin{pmatrix}\n","0.55 & -0.83  \\\\ 0.83 & 0.55\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","5.44 & 0 & 0 \\\\ 0 & 1.17 & 0\n","\\end{pmatrix}\n","\\begin{pmatrix}\n","0.25 & 0.31 & 0.92 \\\\\n","-0.24 & 0.94 & -0.24 \\\\\n","-0.94 & -0.16 & 0.31\n","\\end{pmatrix}\n"," $$\n","Se deja como ejercicio verifcar este triple producto produce $A$ (con algun error).\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"EXaCrF7uYy7e"}},{"cell_type":"code","source":[],"metadata":{"id":"xI3uutlEVzJ_"},"execution_count":null,"outputs":[]},{"source":["import numpy as np\n","\n","U = np.array([[0.55, -0.83], [0.83, 0.55]])\n","Sigma = np.array([[5.44, 0, 0], [0, 1.17, 0]])\n","V_T = np.array([[0.25, 0.31, 0.92], [-0.24, 0.94, -0.24], [-0.94, -0.16, 0.31]])\n","\n","\n","result = np.dot(np.dot(U, Sigma),V_T)\n","print(result)"],"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnuIRdyuV0EE","executionInfo":{"status":"ok","timestamp":1740759331377,"user_tz":300,"elapsed":26,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"e0a6ab5e-0e46-4c46-85af-32798b6ce166"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.981064 0.014686 2.985704]\n"," [0.97436  2.004602 3.999544]]\n"]}]},{"cell_type":"code","source":["# preguntemmosle el SVD de A\n","A =np.array ( [[ 1,0,3],[1,2,4]])\n","\n","# find the SVD (Singular Value Decomposition of A)\n","U, Sigma, VT = np.linalg.svd(A)\n","print(U)\n","print(Sigma)\n","print(VT)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CxVbPetMV1ek","executionInfo":{"status":"ok","timestamp":1740759479563,"user_tz":300,"elapsed":24,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"bb630077-8f00-4cb0-9ea7-599180ca361d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["[[-0.552431   -0.83355863]\n"," [-0.83355863  0.552431  ]]\n","[5.44202115 1.17660775]\n","[[-0.25468288 -0.30634156 -0.91721943]\n"," [-0.23893064  0.93902322 -0.2472803 ]\n"," [-0.93704257 -0.15617376  0.31234752]]\n"]}]},{"cell_type":"markdown","source":["Hay muchas interpretaciones de SVD. Por ejemplo si se tiene un cuerpo (coordenadas) el cuerpo puede sufrir (sin transladar) el SVD hace lo siguiente\n","\n","$$ y = Ax = U \\Sigma V^T x $$\n","el cuerpo es todos los datos en $x$.\n","1. $V^T x$, toma el cuerpo $x$ y le hace una transformacion ortogonal. Rotacion, o un flip (reflexion) o una combinacion de ambos.\n","\n","2. $\\Sigma V^T x$\n","Esto toma el resultado de $V^T x$ y lo deforma a lo largo de los ejes principales, estirandolo o comprimiendolo o desapareciendo dimensiones (aplastandolo como en los dibjuos animados).\n","\n","3. $Ax = U \\Sigma V^T$\n","es una ultima transformacio ortogona: rotacion, o un flip, o una combinacion de ambos.\n","\n"],"metadata":{"id":"8vY3KvDLWbQp"}},{"cell_type":"markdown","source":["**Definicion 2.3.1** (condition number, numero de condicion). Al valor\n","\n","$$ \\kappa = \\frac{\\sigma_1}{\\sigma_n} $$\n","se la llama **numero de condicion**. Este numero es muy importante e indica la estababilidad de un sistema lineal. De hecho si $\\sigma_n=0$ el numero de condicion es  infinito y esto quiere decir que la matriz $A$ no tiene inversa.\n","Si el numero de condicion es muy grande $\\lambda_n \\ll 1$, entonces el sistema\n","$Ax=b$ es inestable y cambios muy pequenos en los datos $b$, puede producir cambios muy grandes en la solucion $x$.\n","\n","## El cociente de Rayliegh.\n","Se usa en PCA como lo explicare mas adelante.\n","**Definicion 2.3.2**(Cociente de Rayleigh)\n","Sea $A$ una matriz auto-adjunta el **cociente de Raylegih** de $A$ se define como\n","\n","$$R(A, x) = \\frac{x^* A x}{x^* x} = \\frac{\\langle x,  A x \\rangle }{\\| x \\|^2}= \\frac{\\langle Ax, x \\rangle }{ \\| x \\| } $$\n","\n","Tambien, se puede usar que $u=x/\\| x\\|$ entonces\n","\n","$$ R(A,x) = \\frac{x^*}{\\| x \\| } A \\frac{x}{\\| x \\|}= u^* A u $$\n","donde $\\| u \\|= 1$ (esfera de radio 1).\n","\n"],"metadata":{"id":"UmrvjOnJYt4G"}},{"cell_type":"markdown","source":["**Teorema 2.3.2**: (cociente de Rayleigh): El mas grande autovalor de $A$ (autoadjunta) corresponde con el maximo valor de los cocientes de Rayleigh de la matriz $A$..\n","\n","**Prueba**: Asuma que $u \\in \\mathbb{C}^n$ es un vector unitario. Cada vector del espacio se puede escribir como una combinacion lineal de los vectores base (ortogonales, autovectores de $A$) $\\{ u_1, u_2, \\cdots, u_n \\}$\n","de forma que\n","\n","$$ u= \\sum_{i=1}^n \\alpha_i u_i $$\n","donde $ \\| u \\| = \\sqrt{ \\sum_{i=1}^n {\\alpha_i}^2}= 1 $\n","\n","Por definicion de cociente de Rayleigh  y aplicando la distributiva\n","\n","\n","$$R(A, u) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j $$\n","\n","Ahoora como cada $u_k$ es autovector de $A$ con autovalor $\\lambda_k$ entonces\n","\n","$$ u_i^* A u_j = u_i^* ( \\lambda_j u_j) = \\lambda_j u_i^* u_j = \\lambda_j \\delta_{ij}  $$\n","de forma que\n","\n","$$R(A, u) = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j = \\sum_{i=1}^n | \\alpha_i |^2 \\lambda_i = \\sum_{j=1}^n | \\alpha_j |^2 \\lambda_j  \\tag{4} $$\n","\n","Algunos hechos\n","* La suma anterior es un promedio ponderado.\n","* $\\lambda_1$ es el mas grande de todos autovalores (asi los escojo, por capricho mio)\n","* El cociente de Rayleigh mas grande es cuando escojo\n","\n","$$u=(1, 0, 0, \\cdots, 0) $$\n","por que le da mayor peso al primero $\\lambda_1$ y ningun peso a los demas.\n","De forma que\n","\n","El mas grande de los cocientes de Rayleigh es $\\lambda_1$.\n","\n","Si $\\lambda_2$ es el mas grande de los autovalores, yo escojo\n","$u=(0,1,0 \\cdots , 0)$ y la ecuacion (4) produce\n","\n","\n","$$R(A, u) = \\lambda_2  $$\n","En las notas de clase hay otra forma al final para probar esto. Lo dejo de lectura.\n","\n","**Teorema 2.3.3.** (Cholesky). Si $A$ es positiva definida y auto-auto adjunta, entonces exiten $L$ tal que\n","\n","$$A = L L^* $$\n","Si $L$ es autoadjunta entonce $L=L^*$ y $A=L^2$ de forma que\n","$L$ es la raiz cuadrada de $A$. No lo usaremos en el curso.\n","\n","Tarea de lectura: Leer la seccion 2.4 de las notas de clase:\n","Applications of eigenvalues and eigenvectors.\n","\n","# Proxima clase: Calculo matricial."],"metadata":{"id":"-7XqCaKdcFgP"}}]}