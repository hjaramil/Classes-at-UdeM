{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM8B9FuNbFOLUUA5jux81NC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["\n","\n","### comentarios acerca de SVD y la proxima tarea\n","## Numero de condicion (condition number)\n","Numero condicion indica que tan estable es la matriz, o que tan seguro\n","sea resolver $Ax=b$. Si el numero de condicion es muy grande, son malas noticias, y toca **regularizar** . Como se calcula\n","\n","\n","$$\\kappa = \\frac{\\sigma_1}{\\sigma_n} .$$\n","donde $\\sigma_1$ es el primer (mayor) valor caracteristico y $\\sigma_n$ el ultimo (menor). Si $\\sigma_n=0$ la matriz es **ill conditioned**.\n","Si $\\kappa \\approx 1$ el sistema $Ax=b$ es muy estable.\n","\n","**Definicion: Cociente de Rayleigh** (util cuando veamos PCA)\n","Asuma una matriz $A$ $n \\times n$ autoadjunta. El cociente de Rayleigh se define como\n","\n","\n","$$R(A, x) = \\frac{x^* A x}{x^* x} = \\frac{\\langle x, Ax \\rangle}{\\| x\\|^2} = \\frac{\\langle Ax,  x \\rangle }{\\| x \\|^2}.$$\n","\n","Ahora, podemos usar $\\| x\\|^2 = \\langle x,  x \\rangle$\n","\n","$$R(A, x) = \\frac{x^*}{ \\| x \\| } A \\frac{x}{\\| x \\|} = u^* A u .$$\n","donde $u$ es un vector unitario.\n","\n","**Teorema**: El mayor autovalor de la matriz $A$ (autoadjunta)  es el mayor cociente de Rayleigh.\n","\n","\n","Prueba: Del teorema espectral sabemos que podemos usar una base ortonormal\n","$\\{ u_1, u_2, \\cdots, u_n \\}$ que son autovectores de la matriz $A$.\n","Cada vector $u \\in \\mathbb{C}^n$ se puede representar como\n","\n","\n","$$ u = \\sum_{i=1}^n \\alpha_i u_i .$$\n","donde $\\alpha_i$ los coeficientes en la base de los $u_i$.\n","\n","Hallemos el cociente de Rayleigh para este $u$\n","\n","\n","$$ R(A, u ) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j  .$$\n","\n","Ahora como los vectores son autovectores para autovalores $\\lambda_k$, entonces\n","\n","\n","$$ u_i^* A u_j = u_i^* (\\lambda_j u_j) = \\lambda_j u_i^* u_j = \\lambda_j \\delta_{ij} .$$\n","\n","$$R(A, u) = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j  = \\sum_{i=1}^n \\sum_{j=1}^n  \\overline{\\alpha_i} \\alpha_j \\lambda_j \\delta_{ij} = \\sum_{i=1}^n | \\alpha_i |^2 \\lambda_i .$$\n","\n","Tenemos que $\\sum_{i=1}^n | \\alpha_i|^2 = 1$ , por que el vector $u$ es unitario y estos son los coeficientes de $u$ en la base ortonormal $u_i$.\n","\n","Asumamos, sin perdida de generalidad que $\\lambda_1$ es el mas grande de todos los autovalores.  La ultima suma es una suma ponderada  de los autovalores.\n","El mayor valor que esta suma pueda tener es para $| \\alpha_1 |^2=1$ y\n","$ | \\alpha_i | = 0$, $i=2,3, \\cdots, n$. Con esto vemos que\n","\n","\n","$R(A, u) = \\lambda_1$.\n","\n","**Teorema: Cholesky decomposition**: Si $A$ es positiva definida, autoadjunta,\n","entonces que existe una matriz $L$ triangular inferior (que todos los elementos de la diaognal para arriba son 0), tal que\n","\n","$$ A = L L^* .$$\n","Si $L$ es diagonal, entonces escribimos \"$L=\\sqrt{A}$\", pues $L^2=A$.\n","De alguna la descomposicoin de Cholesky nos da la raiz cuadrada de la matriz $A$.\n","\n","\n","## Calculo (diferencial) matricial.\n","En esta seccion asumimos $A \\in \\mathbb{R}^{m \\times n}$.\n","\n","**Definicion de derivada de una matriz**: Si $A=A(x)$ es una matriz $m \\times n$\n","donde los elementos $a_{ij}=a_{ij}(x)$ son funciones $x \\in \\mathbb{R}^p$\n","entonces para $A=(a_{ij})$,\n","\n","$$ \\frac{\\partial A}{\\partial x_{k}} = \\left (  \\frac{\\partial a_{ij}}{\\partial x_k}  \\right ) .$$\n","\n","\n","**Teorema :** La matriz de derivadas parciales (**gradiente**) de una matriz constante $A$ es 0. La notacion de **gradiente** es $\\nabla$.\n","\n","La prueba es :\n","\n","$$ \\frac{\\partial A}{\\partial x_k} = \\left (  \\frac{\\partial a_{ij}}{\\partial x_k} \\right ) = (0_k^{ij}) = 0  \\quad , \\quad  i=1, \\cdots m \\quad , \\quad j=1, \\cdots m \\quad , \\quad k=1, \\cdots p.$$\n","\n","**Teorema: **: Si $A$ es $m \\times n$, real y $x \\in \\mathbb{R}^n$, entonces\n","\n","$$\\nabla (A x ) = A .$$, $A$ es una matriz constante.\n","\n","Prueba: Tomemos la componente $i$ del producto $Ax$\n","\n","$$ y_i = \\sum_{j=1}^n a_{ij} x_j .$$\n","\n","Entonces\n","\n","$$ \\frac{\\partial y_i}{\\partial x_k} = \\sum_{j=1}^n a_{ij} \\frac{\\partial x_j}{\\partial x_k} = \\sum_{j=1}^n a_{ij} \\delta_{jk} = a_{ik}.$$\n","\n","Lo que esto nos dice es que la derivada $k$ esima de $Ax$ extrae la columna $k$.\n","\n","\n","Escribios, en general con simbolos $\\nabla Ax = A$.\n","\n","**Teorema**: Sean $f(x)$, $g(x)$ funciones vectoriales de un vector $x \\in \\mathbb{R}^n$, entonces (la regla del producto)\n","\n","$$ \\frac{\\partial \\langle f , g \\rangle }{ \\partial x_i} = \\langle \\frac{\\partial f}{\\partial x_i}, g \\rangle + \\left \\langle f, \\frac{\\partial g}{\\partial x_i} \\right \\rangle .$$\n","\n","Prueba: Expandimos el producto interno\n","\n","$$ \\langle f, g  \\rangle = \\sum_{j=1}^n \\overline{f_j(x)} g_j(x) .$$\n","Aplicamos la derivada a esta suma\n","\n","\n","\\begin{eqnarray}\n"," \\frac{\\partial \\langle f, g \\rangle}{\\partial x_i} &=&\n"," \\frac{\\partial }{\\partial x_i} \\sum_{j=1}^n \\overline{f_j(x)} g_j(x) \\\\\n"," &=& \\sum_{j=1}^n  \\overline{\\frac{\\partial f_j(x)}{\\partial x_i}} g_j(x)\n"," + \\overline{f_j(x)} \\frac{\\partial g_j(x)}{\\partial x_i}\n","\\end{eqnarray}\n","Esto se puede escribir como\n","\n","\n","$$  \\frac{\\partial \\langle f, g \\rangle}{\\partial x_i}  =\n","\\left \\langle \\frac{\\partial f}{\\partial x_i}, g \\right\\rangle +\n","\\left \\langle f , \\frac{\\partial g}{\\partial x_i}\n","\\right \\rangle  .$$\n","\n","En notacion de gradiente podemos escribir\n","\n","$$ \\nabla \\langle f, g \\rangle  = \\langle \\nabla f, g \\rangle + f , \\nabla g \\rangle   .$$\n","\n","Ojo!! tenemos que tener cuidado. El operador $\\nabla f$ es una matriz\n","(la matriz de coordenadas $\\partial f_i/\\partial x_j$, mientras que $f,g$ son vectores. Entonces que sentido tiene el producto interno de una matriz con un vector?\n","\n","\n","Lo que vamos a hacer para que halla dimensionalidad correcta es usar la notacion $\\nabla_k f$.\n","\n","\n","$$ \\nabla_k \\langle f, g \\rangle  = \\langle \\nabla_k f, g \\rangle + f , \\nabla_k g \\rangle   . \\tag{1} $$\n","\n","\n","**Ejemplo**: sea $f(x) = \\frac12 \\| Ax - b  \\|^2$ con $x \\in \\mathbb{R}^n$ y $A$ es una matriz real $m \\times n$.  Encuentre $\\nabla f$.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"f-SRrpEm6S5a"}},{"cell_type":"markdown","source":["Escrimos $f$ usando la notacion de producto interno\n","\n","\n","$$f(x) = \\frac12 \\langle Ax - b, Ax - b \\rangle  .$$\n","\n","Usamos la notacion $\\nabla_k f = (\\nabla f)_k$\n","Pensamos $A_k$ como la columna $k$ de la matriz $A$.\n","\n","Veamos la prueba, usamos la ecuacion (1)\n","\n","\n","\\begin{eqnarray}\n","\\nabla_k f(x) &=& \\frac12 [ \\langle \\nabla_k (Ax - b), (Ax - b) \\rangle + \\langle Ax-b, \\nabla_k( Ax - b) \\rangle ]   \\\\\n","&=& \\langle \\nabla_k (Ax - b), Ax-b) \\quad \\text{el producto interno de reales comunta} \\\\\n","&=& \\langle A_k , Ax - b \\rangle \\quad \\text{teoremas de arriba}\n","\\end{eqnarray}\n","\n","Es decir, cuando $k=1,2, m$,\n","\n","$$\\nabla f(x) = A^*(Ax - b) . \\tag{2}$$\n","\n","Este resultado es muy importante. La Ecuacion (2) se llama ecuaciones normales, se pueden escribir (usando la distributiva), si $\\nabla f = 0$,\n","\n","$$ A^*(Ax - b) = 0 .$$\n","$$ A^*Ax  = A^* b  \\quad  \\text{             ecuaciones normales}.$$\n","\n","Una forma nemotecnica de llegar a las ecuaciones normales es.\n","Queremos resolver\n","\n","$$ Ax = b . \\tag{3}$$\n","Multiplicamos (3) por $A^*$ a izquierda\n","\n","$$ A^*Ax = A^* b . \\tag{4} $$\n","\n","El sistema (3) casi nunca tiene solucion (si $m \\ne n$) o si $m=n$ pero $A^{-1}$ no existe.\n","\n","El sistema (4) tiene solucion si $\\mathcal{N}(A) =0 $. Ademas $A^*A$ es una matriz cuadrada (de orden $n+m \\times n+m$.\n"],"metadata":{"id":"rUqHnY-X3xz3"}},{"cell_type":"markdown","source":["En la proxima clase vemos que pasa cuando (4) no es soluble y introducimos el tema de **regularizacion y minimos cuadrados**."],"metadata":{"id":"P62wVEk264ji"}}]}