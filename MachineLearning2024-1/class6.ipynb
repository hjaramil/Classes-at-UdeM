{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOLU6fKnQGXld6/RJCFSlX/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Nota sobre producto interno y producto punto.\n","\n","**Producto punto**: Si $a=(a_i),b=(b_i) \\in \\mathbb{C}^n$ entonces\n","$$ a \\cdot b = \\sum_{i=1}^n a_i b_i .$$\n","$$ \\langle a , b \\rangle = \\sum_{i=1}^n \\overline{a_i} b_i .$$\n","Un ejemplo donde no son iguales.\n","$a=\\mathrm{i}, b=1$. $a \\cdot b = \\mathrm{i}$ pero $\\langle a , b \\rangle = -\\mathrm{i}$.\n","\n","Por que se define el producto interno de esta forma? Un ejemplo simple.\n","$z = a + \\mathrm{i} b$, $a,b \\in \\mathbb{R}$,\n","\n","$$z \\cdot z  = (a+ \\mathrm{i}b)(a + \\mathrm{i}b) = a^2 + 2 \\mathrm{i} a b -b^2 .$$\n","$$\\langle z , z \\rangle   = \\overline{(a+ \\mathrm{i}b)}(a + \\mathrm{i}b) = (a - \\mathrm{i} b) ( a + \\mathrm{i}b ) = a^2 +b^2 = | z |  .$$\n","\n","## Matrices Positivas Definidas\n","Definicion: Una matriz autoadjunta es **positiva definida** si todos sus\n","autovalores son positivos, es **negativa definida** si todos sus autovalres son negativos, **no negativa definida** si todos sus autovalres son 0 o positivos.\n","\n","En wikipedia encuentra otra defincion (busquela).\n","Una matriz $A$ es positiva definida si $\\forall u \\in \\mathbb{R}^n$\n","$$ u^T A u > 0$$.\n","Una matriz $A$ es negativa definida si $\\forall u \\in \\mathbb{R}^n$\n","$$ u^T A u < 0$$.\n","Una matriz $A$ es no negativa definida si $\\forall u \\in \\mathbb{R}^n$\n","$$ u^T A u \\ge 0 $$.\n","\n","Veamos como mi definicion implica la de wikipedia.\n","\n","Asumamos que $A$ es positiva definida, o sea todos los autovalores son positivos, Entonces, sea $u$ un autovector\n","\n","\n","$$ u^T A u = u^T (\\lambda u) = \\lambda u^T u = \\lambda \\| u \\|^2 > 0  .$$\n","por que $\\lambda > 0$, $u \\ne 0$. Ahora bien $u$ es un autovector de $A$ no\n","un vector arbitrario, que probar para cualquier vector arbitrario. Cualquier vector arbitrario es una combinacion lineal de autovectores. Queda en veremos esta parte de la prueba.\n","\n","Teorema: Si una matriz es positiva definida, su inversa existe y tambien es positiva definida.\n","\n"],"metadata":{"id":"fkhlhqjQYTP8"}},{"cell_type":"markdown","source":["# SVD: Singular Value Decomposition\n","Introduccion. Hemos probado dos teoremas importantes.\n","\n","* Si $A$ es autoadjunta, existe $Q$ ortogonal tal que $A=Q \\Lambda Q^T .$\n","con $\\Lambda$ diagonal con los autovalores de $Q$ y las columnas de $Q$ son los autovectores de $A$.\n","\n","* Si los autovectores de $A$ son linealmente independientes existe $P$ (matriz con columnas formadas por los autvectores) tal que\n","\n","$$ A = P \\Lambda P^{-1} .$$\n","\n","Pero esos teoremas existen hipotesis. El teorema SVD (Singuar Value Decomposition) no exige practicamente nada.\n","\n","El teorema de SVD dice que $A$ ($ m \\times n$) se puede escribir\n","como\n","\n","$$  A = U \\Sigma V^*.$$\n","Donde $U$ es $m \\times m$ con columnas ortogonales,\n","$\\Sigma$ es una matriz diagonal $m \\times n$, con valores positivos que se llaman **valores caracteristicos** y finalmente $V$ es una matriz\n","$n \\times n$ con columnas ortogonales.\n","\n","El uso de este teorema en ML lo vemos cuando estudiemos PCA:Principal Component Analysis y reduccion de dimensionalidad.\n","\n","La prueba de este teorema se basa en el trabajo Cornelius Lanzcos.\n","\n","Construimos una matriz extendida.\n","\n","\n","\\begin{equation*}\n","S = \\left [  \\begin{array}{c:c} 0 & A \\\\ \\\\\n","\\hdashline \\\\ A^* & 0  \\end{array}   \\right ]\n","\\end{equation*}\n","\n","La matriz $S$ es autoadjunta, y es $(m+n \\times m+n)$.\n","Usando el teorema espectral\n","\n","existen vectores ortogonales $w_i$ tales que\n","\n","$$S w_i = \\lambda_i w_i .$$\n","estos $w_i$ son ortogonales.\n","Escribamos los\n","\n","$$ w_i = \\binom{u_i}{w_i} .$$\n","Cuando hacemos la multiplicacion $S w_i$,\n","\n","$$ A^* u_i = \\lambda_i v_i .$$\n","$$ A v_i = \\lambda_i u_i .$$\n","\n","Multiplicamos la primera por $A$ y la segunda por $A^*\n","\n","$$ A A^* u_i = \\lambda_i A v_i = \\lambda_i^2 u_i   .$$\n","$$ A^* A v_i = \\lambda_i A^* u_i  = \\lambda_i^2 v_i.$$\n","\n","\n","Los autovalores de $A^*A$ y $A A^*$ son todos positivos o 0.\n","Vamos a agrupar los autovectores correspondientes a autovalores no 0\n","como $U_r$ para los $u_i$ y $V_r$ para los $v_i$, donde $\\lambda_i^2 >0$,\n","$i=1,2, \\cdots , r$, $\\lambda_i=0$, $i=r+1, r+2, \\cdots m+n$.\n","\n","Construimos la matriz diagonal $\\Lambda_r$ (de orden $r \\times r$ formada\n","por todos los $| \\lambda_i |$.  Entonces\n","\n","Usando las ecuaciones de productos de matrices de la clase anterior, tenemos\n","las siguientes identidades\n","\n","\\begin{eqnarray}\n","A^* U_r = V_r \\lambda_r \\quad , \\quad A^* U_0 = 0 \\\\\n","A V_r= U_r \\Lambda_r \\quad , \\quad AV_0 = 0.\n","\\end{eqnarray}\n","Estas identidades se pueden escribir en bloque como\n","$$ AV = U \\Sigma . \\tag{1} $$\n","donde\n","\n","\\begin{equation*}\n","\\Sigma =\n","\\left [\n","    \\begin{array}{c:c}\n","    \\Lambda_r & 0  \\\\\n","    \\hdashline\n","    0 & 0  \n","    \\end{array}\n","    \\right ]\n","\\end{equation*}\n","\n","Veamos como las cuatro ecuaciones de arriba me generan la identidad (1)\n","\n","\n","$$A V = A [ V_r | V_0 ] = [A V_r | A V_0 ] =[ U_r \\Lambda_r | 0 ] = [ U_r \\Lambda_r + U_0 \\times 0 | U_r \\times 0 + U_0 \\times 0 ] =  U \\Sigma.$$\n","\n","De la ecuacion (1) multiplicando derecha por $V^*$\n","\n","$$ AVV^* = U \\Sigma V^* .  $$\n","pero $V V^* = I_n$ entonces tengo que\n","\n","$$ A = U \\Sigma V^* .  $$\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"FPFD46kFYz6m"}},{"cell_type":"code","source":["import numpy as np\n","\n","A = np.array([[1,3],[2,0]])\n","np.linalg.svd(A)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-15lyWdAVqs","executionInfo":{"status":"ok","timestamp":1708642181103,"user_tz":300,"elapsed":11,"user":{"displayName":"Herman Jaramillo","userId":"11327667299349438387"}},"outputId":"513c5f33-3bfe-4029-ff8e-be3bfb7ae3ff"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SVDResult(U=array([[-0.95709203, -0.28978415],\n","       [-0.28978415,  0.95709203]]), S=array([3.25661654, 1.84240298]), Vh=array([[-0.47185793, -0.8816746 ],\n","       [ 0.8816746 , -0.47185793]]))"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["**Ejemplo:** Encuentre la descomposicion SVD de la matriz\n","\n","$$A = \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4\\end{pmatrix} $$"],"metadata":{"id":"bSNRUAC4Y3T-"}},{"cell_type":"markdown","source":["El algoritmo para encontrar el SVD es el siguiente:\n","\n","1. Construir las matrices $A^*A$ y $A A^*$\n","2. Encontrar los autovalres de $A^*A$ que son los mismos de $A A^*$.\n","3. Encontrar los autvectores de $A A^*$ (los $u_i$) y de $A^* A$ (los $v_i$).\n","4. Con las raices cuadradas **positivas** delos $\\lambda_i$, que llamamos $\\sigma_i$ , valores singulares, construimos la matriz $\\Sigma$. Con los autovectores $u_i$ la matriz $U$ y con los autovectores $v_i$ la matriz $V$.\n","\n","Veamos\n","\n","1. $A^*A$ es $3 \\times 3$, $A A^*$ es $2 \\times 2$. Comencemos por\n","$A A^*$ (por que es la mas pequena y cuando hallemos los autovalores tenemos que resolver una cuadratica).\n","\n","$$ A A^* =  \\begin{pmatrix} 10 & 13 \\\\ 13 & 21 \\end{pmatrix} .$$\n","\n","Calculamos los autvalores. El polinomio caracteristico es\n","\n","$$ p(\\lambda) =  \\det \\begin{pmatrix} 10 - \\lambda & 13 \\\\ 13 & 21 - \\lambda \\end{pmatrix} .$$\n","\n","$$ p(\\lambda) = (10 - \\lambda)(21 - \\lambda) - 169 = 210- 31 + \\lambda^2 - 31.$$\n","Cuando resolvemos la cuadratica $p(\\lambda)=0$ encontramos las raices\n","\n","\n","$$\\lambda_1^2 = \\frac12 ( 31 + \\sqrt{729}) \\approx 28.62 \\quad , \\quad \\lambda_2^2 = \\frac12 ( 31 - \\sqrt{797})  \\approx 1.38 .$$\n","\n","Los autovectores normalizas (de $A A^*$)\n","\n","$$U_1 = \\binom{0.55}{0.83}  \\quad U_2 = \\binom{-0.83}{0.55} .$$\n","\n","Sigue la matriz $A^* A$,\n","\n","$$A^* A = \\begin{pmatrix} 1 & 1 \\\\ 0 &2 \\\\ 3&4 \\end{pmatrix} \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4\\end{pmatrix} = \\begin{pmatrix} 2 & 2 & 7 \\\\2 & 4 & 8 \\\\ 7 & 8 & 25 \\end{pmatrix} .$$\n","\n","Los autovalores de $A^*A $ y $A A^*$ son los mismos, solo que como $A^*A$ es $3 \\times 3$ el poliomio caracteristico es cubico, pero tiene una raiz 0.\n","\n","\n","En este caso las raices del polinomio caracteristico de orden 3 son:\n","\n","$$\\lambda_1^2 = \\frac12 ( 31 + \\sqrt{729}) \\approx 28.62 \\quad , \\quad \\lambda_2^2 = \\frac12 ( 31 - \\sqrt{797})  \\approx 1.38 \\quad , \\quad \\lambda_3^2 = 0 .$$\n","\n","\n","Recuerden que la matriz $\\Sigma$ se construye con las raices cuadradas positivas de los $\\lambda_i$.\n","\n","$$ \\sigma_1 = \\sqrt{\\lambda_1^2} = \\sqrt{28.62} \\approx 5.44 .$$\n","$$ \\sigma_2 = \\sqrt{\\lambda_2^2} = \\sqrt{1.38} \\approx 1.17 .$$\n","\n","Se pueden encontrar los autovectores de $A^* A$ (los $v_i$) normalizados.\n","\n","$$V_1 = \\begin{pmatrix} 0.25 \\\\ 0.31 \\\\ 0.92 \\end{pmatrix}  \\quad , \\quad V_2 = \\begin{pmatrix} -0.24 \\\\ 0.94 \\\\ -0.24 \\end{pmatrix} \\quad , \\quad V_3 = \\begin{pmatrix} 0.92 \\\\ -0.24 \\\\ -0.31\\end{pmatrix}.$$\n","\n","Con estos elementos construimos $A= U \\Sigma V^T$.\n","La matriz $\\Sigma$ es $2 \\times 3$.\n","\n","\\begin{eqnarray}\n","A = U \\Sigma V^T = \\begin{pmatrix} 0.55 & -0.83 \\\\ 0.83 & 0.55 \\end{pmatrix}\n","\\begin{pmatrix} 5.44 & 0 & 0 \\\\ 0 & 1.17 & 0  \\end{pmatrix}\n","\\begin{pmatrix} 0.25 & 0.31 & 0.92 \\\\ -0.24 & 0.94 & -0.24 \\\\ 0.92 & -0.24 & -0.31 \\end{pmatrix}\n","\\end{eqnarray}\n","\n","\n"],"metadata":{"id":"6QqRHC2VBEMv"}}]}