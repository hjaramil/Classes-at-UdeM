{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP0QAVlpN/DRuBqmfJXZgAK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# The Kernel Trick\n","La idea del Kernel Trick es de\n","[Isabelle Guyon](https://en.wikipedia.org/wiki/Isabelle_Guyon)\n","y consiste en acelear el SVM de forma que pueda ser eficiente cuando los datos sean muchos. En espacios con altas dimensiones.\n","\n","Discutimos la idea aca.\n","Dos clases antes de esta vimos la restriccion para el SVM\n","\n","$$y_i ( w^T x_i - b) \\ge 1  \\tag{1} .$$\n","La idea era que $x_i$ estaba en un lado para $y_i=0$ y en el otro\n","para $y_i=1$. Cuando hicimos los multiplicadores de Lagrange (clase anterior) encontramos la restriccion $w=\\sum_i \\lambda_i y_i x_i$.  Reemplazando esto en la desigualdad (1)\n","\n","$$y_i \\left ( \\sum_j \\lambda_j y_j x_i \\cdot x_j -b   \\right )  \\ge 1 \\tag{2} .$$\n","\n","Recuerden que para poder subir los datos a un espacio de alta dimension donde sean linealmente separables necesitamos un \"mapping\" (clase anterior).\n","\n","Si aplicamos el mapeo $\\Phi$ tanto al Lagrangiano (clase anterior) como a la expression\n","obtenemos\n","\n","\n","$$ L(\\lambda) = -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j \\Phi(x_i) \\cdot \\phi(x_j) + \\sum_i \\lambda_i .$$\n","\n","Con restricciones\n","\n","\n","\\begin{eqnarray}\n","\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b \\ge 1 \\quad, \\quad y_i = +1 \\\\\n","\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b < -1 \\quad, \\quad y_i = -1 \\\\\n","\\end{eqnarray}\n","\n","Que pasa, que el operador $\\Phi$ esta siendo aplicado a muchos datos. Y se ve constantemente la expression\n","\n","$$\\Phi(x_i ) \\cdot \\Phi(x_j) .$$\n","\n","$$k(x_i, x_j) = \\Phi(x_i) \\cdot \\Phi(x_j) .$$\n","Como esto puede ser muy costoso para un numero de puntos grande (millones).\n","\n","Para el truco disenenhamos unos pocos ejemplos.\n","\n","**Ejemplo 1**:\n","\n","Tomemos dos dimensiones $n=2$. Defininamos el mapeo a 3 dimensiones\n","\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^2 &\\to& \\mathbb{R}^3 \\\\\n","(x_1, x_2) &\\mapsto& \\Phi(x_1, x_2) = (x_1^2, x_2^2, \\sqrt{2} x_1 x_2)\n","\\end{eqnarray}\n","\n","Verificamos que $k(x,y)= (x \\cdot y)^2$ entonces\n","$k(x,y) = \\Phi(x) \\cdot \\Phi(y). $\n","\n","Tome $x=(x_1, x_2)$ y $y=(y_1, y_2)$, entonces\n","\n","$$k(x,y) = (x \\cdot y)^2 = \\left ( \\sum_{i=1}^2 x_i y_i  \\right )^2  = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2.$$\n","\n","Ahora comparemos con\n","\n","$$\\Phi(x) \\cdot \\Phi(y) = (x_1^2, x_2^2, \\sqrt{2} x_1 x_2) \\cdot (y_1^2 , y_2^2, \\sqrt{2} y_1 y_2) = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + y_1^2 y_2^2 $$\n","\n","\n"],"metadata":{"id":"h7SHcPDFsNK0"}},{"cell_type":"markdown","source":["Contemos operaciones:\n","\n","* $\\Phi(x) \\cdot \\Phi(y)$:\n","    * 4 cuadrados\n","    * 8 multiplicaciones\n","    * 2 sumas\n","\n","* $k(x,y)$:\n","    * 2 multiplicaciones\n","    * 1 suma\n","    * un cuadrado."],"metadata":{"id":"_veWzYKLMfmm"}},{"cell_type":"markdown","source":["**Ejemplo 2**:\n","Consideremos $n=3$ (3D) el mapeo\n","\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^3 &\\to& \\mathbb{R}^6 \\\\\n","(x_1, x_2, x_3) &\\mapsto& (x_1^2, x_2^2, x_3^3, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3 , \\sqrt{2} x_2 x_3)\n","\\end{eqnarray}\n","Definimos $k(x,y) = (x \\cdot y)^2$\n","\n","\n","Verificamos que $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$.\n","\n","$$k(x,y) = (x \\cdot y)^2 = \\left (  \\sum_{i=1}^3 x_i y_i  \\right )^2 = \\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j  .$$\n","\n","Ahora bien\n","\n","$$\\Phi(x) \\cdot \\Phi(y) = (x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3) \\cdot (y_1^2, y_2^2, y_3^2 , \\sqrt{2} y_1 y_2, \\sqrt{2} y_1 y_3, \\sqrt{2} y_2 y_3) = \\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j  .$$\n","\n","De forma que $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$.\n","\n","Contemos operaciones.\n","\n","\n","* $\\Phi(x) \\cdot \\Phi(y) $ :\n","    * 6 cuadrados\n","    * 12 +6 productos = 18 productos\n","    * 5 sumas\n","\n","* $k(x,y)$:\n","    * 1 cuadrado\n","    * 3 productos\n","    * 1 suma\n","\n","\n","\n","\n"],"metadata":{"id":"P39niO0CM_Fe"}},{"cell_type":"markdown","source":["**Ejemplo 3**:\n","\n","Sea $c \\in \\mathbb{R}$\n","observemos\n","\n","3 dimensiones de $x,y$\n","\\begin{eqnarray}\n","k(x,y) &=& (x \\cdot y  + c )^2 \\\\\n","&=& \\left ( \\sum_{i=1}^3 x_i y_i + c  \\right ) \\left ( \\sum_{i=1}^3 x_i y_i + c \\right ) \\\\\n","&=& \\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j + 2 c \\sum_{i=1}^3 x_i y_i + c^2\n","\\end{eqnarray}\n","\n","Definamos el mapeo $\\Phi$\n","\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^3 & \\to & \\mathbb{R}^{10} \\\\\n","(x_1, x_2, x_3) &\\mapsto& (x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3, \\sqrt{2} c x_1, \\sqrt{2} c x_2, \\sqrt{2} c x_3, c)\n","\\end{eqnarray}\n","\n","Se puede probar que\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) .$$\n","\n"],"metadata":{"id":"2XX15sSAPj4z"}},{"cell_type":"markdown","source":["Hagamos el conteo\n","\n","* $\\Phi(x) \\cdot \\Phi(y) $:\n","    * 6 cuadrados\n","    * 10 + 10 multiplicaciones = 20 multiplicaciones\n","    * 1 raiz cuadra\n","    * 9 sumas\n","\n","* $k(x,y) $:\n","    * 3 multiplicaciones\n","    * 3 Sumas\n","    * 1 cuadrado.\n"],"metadata":{"id":"p_C3gKD2RJv0"}},{"cell_type":"markdown","source":["In general para $d \\in \\mathbb{N}$, $d \\ge 2$\n","encontramos que\n","\n","$$k(x,y) = (x \\cdot y + c)^d \\quad , \\quad c \\in \\mathbb{\n","    R}.$$\n","\n","Este kernel satisface\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) .$$\n","\n","Donde\n","\n","$$ \\Phi: \\mathbb{R}^n \\to \\mathbb{R}^p .$$\n","\n","donde\n","\n","$$ p = \\binom{n+d}{d} .$$\n","\n","$$ 5!/(3! 2!)= \\frac{5 \\times 4}{2} = 10$$\n","\n","Como tarea dejo encontrar el mapping $\\Phi$."],"metadata":{"id":"jkSL5HZRR2Nk"}},{"cell_type":"markdown","source":["El ultimo ejemplo es\n","**Ejemplo 4**: El kernel Gaussiano.\n","\n","Yo comienzo con el kernel $k(x,y)$ y encuentro el mapping $\\Phi$\n","\n","Es buena estrategia para buscar mappings a partir de kernels."],"metadata":{"id":"ZB_h93VVsQ_G"}},{"cell_type":"markdown","source":["El kernel Gaussiano esta definido por\n","\n","$x_i, y_j \\in \\mathbb{R}^n$, $i=1, \\cdots, m$, $j=1, \\cdots, m$\n","\n","$$k(x_i,y_i) = \\mathrm{e}^{- \\frac{(x_i - x_j)^2}{2 \\sigma^2}}  . $$\n","\n","Aca\n","\n","$$(x_i - x_j)^2 = \\| x_i - x_j \\|^2 = (x_i -x_j)^T (x_i - x_j) = \\langle x_i -x _j , x_i - x_j \\rangle  = ( x_i - x_j) \\cdot (x_i - x_j) .$$"],"metadata":{"id":"9bsOrsXuTU0-"}},{"cell_type":"markdown","source":["Vamos a usar a siguiente serie de Taylor para la exponencial\n","\n","\n","$$\\mathrm{e}^{\\theta} = 1 + \\theta + \\frac{\\theta^2}{2!} + \\cdots + \\frac{\\theta^n}{n!} + \\cdots .$$\n","mas un error\n","\n","\n","Asumamos, por simplicidad que $x_i, x_j \\in \\mathbb{R}^2$.\n","\n","\n","Veamos una extrategia para extraer el $\\Phi$.\n","\n","\n","\\begin{eqnarray}\n","\\mathbb{e}^{- \\frac{(x_i - x_j)^2}{2 \\sigma^2}}\n","&=&  \\mathrm{e}^{- \\frac{x_i^2 + x_j^2}{2 \\sigma^2}} \\mathrm{e}^{\\frac{x_i x_j}{ \\sigma^2}} \\\\\n","&=&  \\mathrm{e}^{- \\frac{x_i^2 + x_j^2}{2 \\sigma^2}}  \\left (  (1)(1) +  \n","\\frac{x_i x_j}{\\sigma^2} + \\frac{1}{2!} \\left ( \\frac{x_i x_j}{\\sigma^2}  \\right )^2 + \\frac{1}{3!} \\left ( \\frac{x_i x_j}{\\sigma^2}  \\right )^3 + \\cdots + \\frac{1}{n!} \\left ( \\frac{x_i x_j}{\\sigma^2}  \\right )^n \\right ) \\\\\n","&=& \\mathrm{e}^{- \\frac{x_i^2 + x_j^2}{2 \\sigma^2}}\n","\\left (\n","    (1)(1) + \\frac{x_i}{\\sigma} \\frac{x_j}{\\sigma} +\n","    \\frac{x_i^2}{\\sqrt{2!}^2 \\sigma^2}\n","    \\frac{x_j^2}{\\sqrt{2!}^2 \\sigma^2} +\n","    \\frac{x_i^3}{\\sqrt{3!}^2 \\sigma^3}\n","    \\frac{x_j^3}{\\sqrt{3!}^2 \\sigma^3} + \\cdots\n","    + \\frac{x_i^n}{\\sqrt{n!} \\sigma^n}\n","     \\frac{x_j^n}{\\sqrt{n!} \\sigma^n}  \n","    \\right )\n","\\end{eqnarray}\n","\n","Esto sugiere el $\\Phi$\n","\n","\n","$$\\Phi(x) = \\mathrm{e}^{-\\frac{x^2}{2 \\sigma^2}}  \n","\\left ( 1  , \\frac{x}{\\sigma} , \\frac{x^2}{\\sqrt{2!} \\sigma^2},\n","\\frac{x^3}{\\sqrt{3!} \\sigma^3}, \\cdots , \\frac{x^n}{\\sqrt{n!} \\sigma^n} \\right ) $$\n","\n","Se puede probar que\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y).$$.\n","\n","\n","\n","\n"],"metadata":{"id":"AiuZkO2kUCfj"}},{"cell_type":"markdown","source":["En general los kernels satisfacen propiedades.\n","\n","* Son simetricos $k(x,y) = k(y,x)$.\n","\n","* Dado que el kernel es simetrico es diagonalizable. Recuerden el teorema. Es decir la matriz $(K)_{ij}=(k(x_i, x_j))$ es diagonalizable. Existe una matriz diagonal $\\Lambda$, tal que\n","\n","$$ K = U \\Lambda U^T .$$\n","donde $U$ es una matriz donde las columnas son los autovectores de $K$, es ortogonal.\n","\n","\n","Entonces\n","\n","\\begin{eqnarray}\n","k_{ij} &=& \\sum_{l=1}^m u_{il} \\lambda_k u_{jl} \\\\\n","&=& \\sum_{i=1}^m \\sqrt{\\lambda_l} u_{il} \\sqrt{\\lambda_l} u_{jl} \\\\\n","&=& (\\sqrt{\\lambda_1} u_{i1}, \\sqrt{\\lambda_2} u_{i2} , \\cdots, \\sqrt{\\lambda_m} u_{im} ) \\cdot\n","(\\sqrt{\\lambda_1} u_{j1}, \\sqrt{\\lambda_2} u_{j2} , \\cdots, \\sqrt{\\lambda_m} u_{jm} ) \\cdot\n","\\end{eqnarray}\n","Esto sugiere el mapping $\\Phi$\n","\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^n & \\to & \\mathbb{R}^m \\\\\n","x^{(i)} &\\mapsto& ( \\sqrt{\\lambda_1} u_{i1}, \\sqrt{\\lambda_2} u_{i2} , \\cdots , \\sqrt{\\lambda_m} u_{im} )\n","\\end{eqnarray}\n","\n","De forma que\n","\n","\n","$$(K)_{ij} = k( x^{(i)}, x^{(j)}) = \\Phi(x^{(i)}) \\cdot \\Phi(x^{(j)}) .$$\n","\n","La matriz $K$ se llama\n","[Gram matrix](https://en.wikipedia.org/wiki/Gram_matrix).\n","\n","Veamos que es no negativa definida.\n","\n","Sea $c \\in \\mathbb{R}^m$,\n","\n","$$c^T K c = \\sum_i \\sum_j c_i c_j k(x_i , y_j) = \\sum_i \\sum_j c_i c_j \\Phi(x_i) \\cdot \\Phi(x_j) = \\sum_i c_i \\Phi(x_i) \\cdot \\sum_j c_j \\Phi(x_j) = \\|  \\sum_{i} c_i \\Phi(x_i) \\|^2  \\ge 0 .$$"],"metadata":{"id":"syqME-v6ZU8O"}},{"cell_type":"markdown","source":["Resumen de lo visto hoy\n","\n","Los kernels\n","\n","* **Lineal**\n","\n","$$ k(x,y) = x \\cdot y  + c $$\n","\n","* **polinomico**\n","$$k(x,y) = (x \\cdot y + c)^d .$$\n","\n","* **Gaussiano**\n","$$k(x,y) = \\mathrm{e}^{- \\frac{\\| x - y \\|^2}{2 \\sigma^2}} .$$\n","\n","Que sigue?\n","\n","* Decision trees (arboles de decision)\n","* Random Forests (arboles aleatorios)\n","\n","* Deep learning\n","    *CNN: Convolutional neural networks\n","    * NLP: Natural language processing\n","        * Redes recursivas RNN\n","        * LSTM: Long Short Term memory\n","    * Generative methods\n","        * GAN: Generativa adeversary Netoworks\n","        * GPT: Generative Preprocessed Transformers\n"],"metadata":{"id":"0gcXinUibzQw"}},{"cell_type":"markdown","source":["# Proxima clase\n","## Metodos no supervisados."],"metadata":{"id":"rHOXBeZVc4Rv"}}]}