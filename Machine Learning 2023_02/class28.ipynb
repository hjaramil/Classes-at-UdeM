{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOObkfPVU4gFyJ87RlnZM/e"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Regularizacion en SVM\n","De la clase anterior vimos que\n","\n","$$\\min \\frac{\\| w \\|^2}{2} \\quad \\text{sujeto a} \\quad y^{(i)}(w^T x^{(i)} - b ) \\ge 1  \\quad i=1,2, \\cdots m \\tag{1} .$$\n","\n","Podemos escribir una variacion de la ecuacion anterior como\n","la minimizacion de la funcion objetivo\n","\n","\n","$$ J(w) = \\frac{\\|w \\|^2}{2} + C \\sum_{i=1}^m (1 - y^{(i)}[w^T x^{(i)}-b)].$$\n","\n","\n"],"metadata":{"id":"J15Ddw3YUnvh"}},{"cell_type":"markdown","source":["Si $x^{(i)}$ esta en el lado correcto lo correspondiente dentro la suma es negativo. Como queremos minimizar esta suma no representa un penalti.\n","De otro lado si $x^{(i)}$ es positivo entonces lo que esta dentro la suma es positivo y toca reducirlo al minimo. Hay penalti para incrementar el \"gap\" de las \"bermas\".\n","\n","Los negativos no nos interan por que esos minimizan sin ningun esfuerzo. A todos los negativos los podemos aproximar hasta 0 y los positivos dejarlos como tal. Podemos cambiar el sumando por\n","\n","$$\\max( 0, 1- y^{(i)}( w^T x^{(i)} - b)) .$$\n","y dejar como funcion objetivo\n","\n","$$J(w, b) = \\frac{\\| w \\|^2}{2 } + C \\sum_{i=1}^m  \\max( 0, 1 - y^{(i)}(w^T x^{(i)} - b )) .$$\n","$C$ es un paramatro de regularizacion, actua como $1/\\lambda$ en Tikhonov.\n","\n","## Multiplicadores de Lagrange.\n","La ecuacion (1) es tipica del tema de multiplicadores de Lagrange.\n","La idea es crear una sola ecuacion donde se usa la funcion objetivo sumando (restando) todas las restricciones pesadas con $\\lambda_i$. En Este caso, convertimos la ecuaicion (1) en:\n","\n","\n","$$ L(w, b , \\lambda) = \\frac12 \\| w \\|^2 - \\sum_i \\lambda_i [ y_i(w \\cdot x_i - b ) - 1]. \\tag{1b}$$\n","\n","La minimizacion de esta funcion objetivo se hace con  el gradiente\n","\n","$$\\nabla_w  L(w, b , \\lambda) = \\nabla_w \\left [ \\frac12 \\| w \\|^2 - \\sum_i \\lambda_i [ y_i(w \\cdot x_i - b ) - 1] \\right ].$$\n","\n","Calcuemos primer las derivadas del primer termino. $\\| w \\|^2/2 = \\sum_i w_i^2/2$, entonces\n","\n","\n","$$ \\frac{\\partial}{w_j} \\left ( \\frac12 \\sum w_i^2\n","    \\right )  = \\sum_i w_i \\delta_{ij} = w_j .$$\n","\n","Para el segundo termino.\n","\n","\\begin{eqnarray}\n","\\frac{\\partial }{\\partial w_j} \\left (\\sum_i \\lambda_i [ y_i(w \\cdot x_i - b ) - 1] \\right ) &=& \\sum_i \\lambda_i\n","\\left [ y_i  \\left (  \\sum_k \\delta_{kj} (x_i)_k   \\right )      \\right ]  \\\\\n","&=& \\sum_i \\lambda_i y_i (x_i)_j\n","\\end{eqnarray}\n","\n","Encontramos que\n","\n","$$\\nabla_w L(w, b, \\lambda) =w_j - \\sum_i \\lambda_i y_i x_i  = 0.$$\n","de donde\n","\n","\n","$$ w = \\sum_i \\lambda_i y_i x_i  \\tag{2} .$$\n","\n","Pero ojo, que no hemos hecho las derivadas con respecto a $b$.\n","\n","$$ \\frac{\\partial }{\\partial b} \\left (  \\sum_i \\lambda_i [ y_i(w \\cdot x_i - b) - 1]\n","    \\right )\n","= -\\sum_i \\lambda_i y_i = 0\n","    .$$\n","\n","De donde\n","\n","$$ \\sum_i \\lambda_i y_i = 0 .$$\n","\n","Estos resultados los usamos en la ecuacion (1b).\n","\n","\n","\\begin{eqnarray}\n","L(\\lambda) &=& \\frac12 \\left ( \\sum_i \\lambda_i y_i x_i  \\right )\n","\\left ( \\sum_j \\lambda_j y_j x_j \\right ) - \\sum_i \\lambda_i \\left [\n","    y_i \\left ( \\sum_j \\lambda_j y_j x_j \\cdot x_i -b \\right ) - 1\n","    \\right ] \\\\\n","    &=& \\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j\n","    - \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_i \\lambda_i y_i b + \\sum_i \\lambda_i \\\\\n","    &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_{i} \\lambda_i\n","\\end{eqnarray}\n","sujeto a $\\sum_i \\lambda_i y_i = 0$.\n","\n","A este problema se le llama el \"dual\" por que otro multiplicadores de Lagrange, pero ahora en la variable $\\lambda$ en vez de $w$.\n","\n","Una vez resolvamos este problema, obtenemos $\\lambda$ y de la ecuacion (2) obtenemos $w$. Falta $b$, esto se deja como ejercicio al estudiante.\n","\n","\n","\n","\n"],"metadata":{"id":"7NAgMVAdttJl"}},{"cell_type":"markdown","source":["## El \"kernel trick\"\n","\n","El [Kernel Trick](https://en.wikipedia.org/wiki/Kernel_method#:~:text=The%20kernel%20trick%20avoids%20the,inner%20product%20in%20another%20space%20.)\n","fue introducido por\n","[Isabel Guyon](https://en.wikipedia.org/wiki/Isabelle_Guyon)\n","en los 90s.\n","\n","Recuerden dos cosas:\n","\n","* Usamos kernels para levantar los puntos a un espacio de altas dimensiones para convertilos de un conjunto que no es linealmente separable en otro que si lo es (ver clase anterior).\n","\n","* En el proceso de resolver las ecuaciones de SVM por el metodo de Lagrange, dual, se requiren muchos productos de la forma $x_i \\cdot x_j$.  Y la evalaucion de estos con un mapeo $\\Phi$ a un numero mayor de dimensiones es costoso. Vamos a ver ejemplos de como el \"kernel trick\" ayuda a simplicar los calculos de una forma abismal.\n","\n","Sea\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^n &\\to* \\mathbb{R}^k \\\\\n","x_i &\\mapsto&  \\Phi(x_i)\n","\\end{eqnarray}\n","donde $k > m$\n","\n","Recordemos las ecuacines de Lagrange (dual)\n","\\begin{eqnarray}\n","L(\\lambda) &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_{i} \\lambda_i\n","\\end{eqnarray}\n","Cuando hacemos el mapeo de los puntos terminamos en algo como\n","\n","\n","\n","\\begin{eqnarray}\n","L(\\lambda) &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j \\Phi(x_i) \\cdot \\Phi(x_j) + \\sum_{i} \\lambda_i\n","\\end{eqnarray}\n","\n","Las condiciones serian\n","$$\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b \\ge 1  \\quad y_i=+1.$$\n","$$\\sum_j \\lambda_j y_j \\Phi(x_j) \\cdot \\Phi(x_i) - b < -1  \\quad y_i=-1.$$\n","\n","Como los productos $\\Phi(x_i) \\cdot \\Phi(x_j)$ estan por todas partes\n","definimos el kernel como\n","\n","\n","$$k(x_i,x_j) = \\Phi(x_i) \\cdot \\Phi(x_j) .$$\n","A $\\Phi$ se le llama el **feature map**.\n","\n","Veamos ejemplos de la construccion de kernels que nos permiten evaluar los calculos de forma rapida.\n","\n","\n","Ejemplos:\n","\n","* Tomemos $n=2$, y definamos:\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^2 &\\to& \\mathbb{R}^3 \\\\\n","(x_1, x_2) &\\mapsto& \\Phi(x_1, x_2) = (x_1^2, x_2^2, \\sqrt{2} x_1, x_2).\n","\\end{eqnarray}\n","    Para este caso el **kernel trick** es\n","\n","$$k(x,y)= (x \\cdot y)^2. \\tag{3}$$\n","Vamos a probar que\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y) \\tag{4} .$$\n","Veamos la prueba de esto. Sea $x=(x_1, x_2)$, $y=(y_1, y_2)$, entonces\n","de la ecuacion (3)\n","$$k(x,y)= (x \\cdot y)^2 = \\left ( \\sum_{i=1}^2 x_i y_i  \\right )^2\n","x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + y_1^2 y_2^2  \\tag{5}.$$\n","\n","Independientemente evaluemos (4).\n","\n","$$\\Phi(x) \\cdot \\Phi(y) = (x_1^2, x_2^2 , \\sqrt{2} x_1 x_2) \\cdot (y_1^2 , y_2^2,  \\sqrt{2} y_1 y_2) = x_1^2 y_1^2 + x_2^2 y_2^2 + 2 x_1 x_2 y_1 y_2  \\tag{6} .$$\n","y probamos que $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$.\n","\n","\n","Profe, donde esta la ganancia?\n","\n","* En el caso de la ecuacion (6). Que necesitamos\n","    * 4 cuadrados\n","    * 4 multiplicaciones antes del producto\n","    * 3 multiplicaciones para el producto\n","    * 2 sumas\n","\n","* En el caso de la ecuacion (5)\n","    * 3 multiplicaciones y dos sumas (el producto interno)\n","    * 1 cuadrado.\n","\n","\n","* Tomemos el caso de $n=3$.\n","Definimos\n","\n","\\begin{eqnarray}\n","\\Phi: \\mathbb{R}^3 &\\to& \\mathbb{R}^6 \\\\\n","(x_1, x_2, x_3) &\\mapsto& \\Phi(x_1, x_2, x_3) = (x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3)\n","\\end{eqnarray}\n","\n","El \"kernel trick\" en este caso es\n","\n","$$k(x,y) = (x \\cdot y)^2 .$$\n","\n","Veamos esto rapidamente, $x=(x_1, x_2, x_3)$, $y=(y_1, y_2, y_3)$\n","\n","\n","$$k(x,y)= (x \\cdot y)^2 = \\left ( \\sum_{i=1}^3 x_i y_i  \\right )^2\n","=\\sum_{i=1} \\sum_{j=1}^3 x_i x_j y_i y_j.$$\n","\n","De otro lado\n","\n","\n","$$\\Phi(x) \\cdot \\Phi(y) = (x_1^2, x_2^2, x_3^3, \\sqrt{2} x_1, x_2, \\sqrt{2} x_1, x_3, \\sqrt{2} x_2, x_3)  \\cdot\n","  (x_1^2, x_2^2, x_3^3, \\sqrt{2} x_1, x_2, \\sqrt{2} x_1, x_3, \\sqrt{2} x_2, x_3) =\\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j.$$\n","\n","\n","* La exponencial (Gaussian)\n","\n","Piense en este kernel\n","\n","$$k(x_i, x_j) = \\mathrm{e}^{- \\frac{(x_i-x_j)^2}{2 \\sigma^2}} .$$\n","\n","Donde podemos interpretar\n","\n","\n","$$(x_i - x_j)^2 = \\| x_i - x_j \\|^2 = (x_i - x_j)^T (x_i - x_j) = \\langle x_i - x_j, x_i - x_j \\rangle = (x_i - x_j) \\cdot (x_i - x_j) .$$\n","\n","\n","Para simplificar vamos a asumir que $x_i, x_j \\in \\mathbb{R}$.\n","\n","\n","\n"],"metadata":{"id":"hNhTWi6Ekgo2"}},{"cell_type":"markdown","source":["Usamos las siguiente expansion de Taylor\n","\n","$$\\mathrm{e}^{\\theta} = 1 + \\theta + \\frac{\\theta^2}{2} + \\cdots + \\frac{\\theta^n}{n!}  \\quad, n \\to \\infty  .$$\n","\n","Usando esta ecuacion escribimos\n","\n","  \\begin{eqnarray*}\n","        \\mathrm{e}^{-\\frac{(x_i - x_j)^2}{2 \\sigma^2}} &=&\n","         \\mathrm{e}^{-\\frac{x_i^2 + x_j^2}{2 \\sigma^2}}  \\mathrm{e}^{\\frac{x_i x_j}{ \\sigma^2}} \\\\\n","          &=&\\mathrm{e}^{-  \\frac{x_i^2 + x_j^2}{2 \\sigma^2}}\n","          \\left ( 1 + \\frac{x_i x_j}{ \\sigma^2} + \\frac{1}{2 !} \\left ( \\frac{x_i x_j}\n","          { \\sigma^2}  \\right )^2  + \\frac{1}{3!} \\left ( \\frac{x_i x_j}{ \\sigma^2}\n","          \\right )^3   + \\cdots + \\frac{1}{n!} \\left ( \\frac{x_i x_j}{ \\sigma^2} \\right )^n\n","      \\right )  \\\\\n","        &=& \\mathrm{e}^{-  \\frac{x_i^2 + x_j^2}{2 \\sigma^2}} \\left (\n","          (1) (1)  + \\frac{x_i}{\\sigma} \\frac{x_j}{\\sigma}\n","        + \\frac{x_i^2}{\\sqrt{2!} \\sigma^2} \\frac{x_j^2}\n","        {\\sqrt{2!} \\sigma^2} +   \\frac{x_i^3}{\\sqrt{3!}\\sigma^3}\n","        \\frac{x_j^3}{\\sqrt{3!} \\sigma^3} + \\cdots + \\frac{x_i^n}{\\sqrt{n!} \\sigma^{n}}  \n","        \\frac{x_j^n}{\\sqrt{n!} \\sigma^{n}} \\right ) \\,.\n","\\end{eqnarray*}\n","\n","Esto nos sugiere el siguiente mapping:\n","\n","\n","$$\\Phi(x) = \\mathrm{e}^{-\\frac{x^2}{2 \\sigma^2} }\n","\\left (  1, \\frac{x}{\\sigma} , \\frac{x^2}{\\sqrt{2!} \\sigma^2}, \\frac{x^3}{\\sqrt{3!} \\sigma^3}, \\cdots, \\frac{x^n}{\\sqrt{n!} \\sigma^n}   \\right ) \\,, $$\n","\n","\n","El kernel en este caso es igual\n","\n","$$k(x,y) = \\Phi(x) \\cdot \\Phi(y)  .$$\n","\n","Esto implica que el **kernel trick** es:\n","\n","\n","$$k(x,y) =  \\mathrm{e}^{-\\frac{(x_i - x_j)^2}{2 \\sigma^2}}.$$\n","\n"],"metadata":{"id":"8yn9MfSKsq8l"}}]}