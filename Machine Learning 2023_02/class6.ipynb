{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMhCJyvmeIcGVfSFrsmTKNV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Teorema de descomposicion espectral para matrices autoadjuntas\n","Si $A$ es una matriz $n \\times n$ autoadjunta entonces existe una base ortonormal $\\{ x_1, x_2, \\cdots, x_n \\}$\n","con la cual podemos construir una matriz $Q$ (las columnas de la base) y existe una matriz diagonal $\\Lambda$, tal que\n","\n","\n","$$A = Q \\Lambda Q^* .$$\n","la diagonal $\\Lambda$ tiene en su diagonal los autovalores de $A$.\n","\n","**Prueba**: Ya este teorema lo probamos cuando todos los $\\lambda_i$ son diferentes. Ahora puede algunos sean repetidos.\n","\n","El teorema lo probamos por el principio de induccion matematica.\n","Que es el principio de induccion matematica?\n","\n","Piense en un juego de domino. Para que todas las fichas se caigan necesita dos pasos:\n","\n","1. Tumbar la primera\n","2. Que si se cae la ficha $k$ se cae entonces la $k+1$, $k>1$. Es decir la distancia entre dos fichas seguidas es menor que la altura.\n","\n","Matematicamente esto pasa con los enteros. Se cumple para el primero $k=1$, y si se cumple para $k$ implica que se cumple para $k+1$ entonces todos son vasidos.\n","\n","Digamos que $k$ es la dimension de la matriz $A$.\n","* Si $k=1$. Cualquier matriz $A$ tiene un autovalor, y en particular existe $\\lambda_1, x_1$ tal que $A x_1 = \\lambda_1 x_1$. Es $\\{ x_1 \\}$ (normalizado) ortonormal? Si por defecto (default).\n","\n","\n","Tarea: entontrar la descomposicion de $A$ en $Q \\Lambda Q^*$\n","cuando $A \\in \\mathbb{C}$.\n","\n","* Asumamos que el teorema es valido para $k>1$. Entonces tenemos\n","una base ortonormal $\\{ x_1, x_2, \\cdots, x_k \\}$ que me generan\n","la matriz $Q$ ortogonal tal que $A = Q \\Lambda Q^*$.\n","\n","Necesitamos encontrar que el teorema es valido para $k+1$.\n","\n","Definamos el espacio $W$ como el complemento ortogonal del generado $\\langle x_1, x_2, \\cdots, x_k \\rangle$. Es decir\n","\n","$$ W = \\langle x_1, x_2, \\cdots, x_k \\rangle^{\\perp}. $$\n","Defino: el complemento ortogonal de un conjunto $W$ es el conjunto $V$ tal que para todo $x \\in w$ y $y \\in V$, $\\langle x, y \\rangle = 0$.\n","\n","vamos a probar que $W$ es invariante para la matriz $A$. Si esto es cierto tiene un vector $v \\in W$ tal que $\\langle v, x \\rangle = 0$, $x \\in W$. Este $v$ no puede estar en $W$ por que ortogonal a $W$, si estuviera $\\langle x, x \\rangle = 0= \\| x \\|^2$, y bueno\n","entonces el vector $x = 0$.  \n","\n","Probemos que $W$ es invariante para $A$. Para probar que es invariante hay que probar que $A W \\subset W$. Es decir que si tomamos un vector $y \\in AW$ este vector lo encotraremos en $W$.\n","Al estar $y \\in AW$, $y = Ax$, para algun $x \\in W$.\n","\n","Comencemos:\n","$$ \\langle y , x_i \\rangle = \\langle A x , x_i \\rangle = \\langle x, A^* x_i \\rangle = \\langle x, A x_i \\rangle =\\langle x, \\lambda_i x_i \\rangle = \\lambda \\langle x, x_i \\rangle  = 0   .$$\n","Pues $x \\in W$, $x_i \\in W^{\\perp}$.\n","En conclusion $y \\perp x_i$, y esto para $i=1,2, \\cdots, k$, ,o sea que $y \\in \\langle x_1, x_2, \\cdots, x_k \\rangle ^{\\perp} $\n","o sea que $y \\in W$ y probamos que $A W \\subset W$. Por lo tanto\n","$W$ es invariante para $A$ y tiene un vector $x_{k+1}$ que es autovector y no  esta en el conjunto $\\{x_1, x_2, \\cdots, x_k \\}$.\n","La hipotesis de induccion se cumple.\n","La nueva base es $\\{ x_1, x_2, \\cdots, x_k, x_{k+1} \\}$.\n","\n","En resumen existe una base $\\{x_1, x_2, \\cdots, x_n \\}$\n","La construccion $A=Q \\Lambda Q^*$ se hace tal y como se hizo la clase anterior.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"NtvX0W8eGuml"}},{"cell_type":"markdown","source":["Aplicaciones:\n","\n","* Veamos como se pude simplificar una forma cuadratica usando este teorema. Asuma que $A$ ($n \\times n$) es una matriz simetrica real $A=A^T$.\n","La forma cuadratica para un vector $x \\in \\mathbb{R}^n$\n","es\n","\n","$$ x^T A x = \\sum_{i=1}^n \\sum_{j=1}^n a_{ij} x_i x_j .$$\n","se dela la prueba de esto al estudiante.\n","\n","Cuantos terminos hay aca? Hay $n^2$. Son muchos. De estos terminos $n^2 - n$ estan acoplados. (los diagonales).\n","Ver ejemplo a mano alzada.\n","\n","\n","Veamos como esta ecuacion se puede simplicar.\n","Si $A= Q \\Lambda Q^T$ entonces\n","\n","$$ x^T A x = x^T Q \\Lambda Q^T x  .$$\n","\n","Hacemos un cambio de variable. $y= Q^T x$,  entonces\n","$y^T = x^T Q$ y encontramos la simplificacion\n","\n","\n","$$ x^T A x = y^T \\Lambda y = \\sum_{i=1}^n \\lambda_i y_i^2  .$$\n","El numero de terminos paso de $n^2$ a $n$.\n","\n","* Hay otra simplificacion a procesos que es elevar una matriz a una potencia .\n","\n","\n","Pero esto lo vemos mas adelante (tal vez hoy), cuando veamos lo que se llama **diagonalizacion**.\n","\n","\n","Un de los objetivos fuertes del algebra lineal en esta clase es el teorema de SVD: Singular Value Decomposition (otros objetivos son el problema de minimos cuadrados, y el de la pseudoinversa).\n","\n","Antes de hablar del problema SVD. Vamos a ver unas reglas de multiplicacion de matrices que nos ahorran trabajo.\n","\n","\n","Teorema: Si $A$ es una matriz $m \\times n$, $B$ es una matriz $n \\times p$, $x$ es un vector $\\in \\mathbb{C}^n$, $\\Lambda$ es una matriz $n \\times p$ diagonal (todos todos los valores por fuera de la diagonal son 0), entonces encontramos las siguientes propiedades:\n","\n","1.\n","$$ AB = \\left [ AB_1 | AB_2 | \\cdots | AB_p \\right ].$$\n","donde $B_i$ es la columna $i$ de $B$.\n","\n","2.\n","\n","$$ A x = \\sum_{j=1}^n x_j A_j .$$\n","donde $A_j$ es la columna $j$ de $A$.\n","O sea que para construir el $\\mathcal{R}(A)$ podemos multiplicar\n","$A$ por todos los vectores $x \\in \\mathbb{C}^n$.\n","\n","3. Si $p \\ge n$, ,\n","\n","$$ A \\Lambda = \\left [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_n A_n | 0_{n+1} | \\cdots | 0_p      \\right ] .$$\n","donde $A_i$ es la columna $i$ de $A$ y $0_k$ es un vector de ceros, en la columna $k$.\n","\n","Si $n>p$,\n","\n","$$ A \\Lambda = \\left [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_n A_n  \\right ] .$$\n","\n","4. Si $u_i$ son columnas de una matriz $m \\times m$, $U$, y $b \\in \\mathbb{C}^m$, entonces\n","\n","\n","\\begin{eqnarray}\n","U^* b  =\n","\\begin{pmatrix}\n","u_1^* b \\\\\n","u_2^* b \\\\\n","\\vdots \\\\\n","u_m^* b\n","\\end{pmatrix}\n","\\end{eqnarray}\n","\n","\n","\n","\n"],"metadata":{"id":"cmRDWEYF7QrD"}},{"cell_type":"markdown","source":["\n","Prueba:\n","\n","1.\n","\n","$$ AB = \\left [ AB_1 | AB_2 | \\cdots | AB_p \\right ]. \\tag{1}$$\n","\n","Lo podemos hacer por componente por componente.\n","Segun la definicion de producto de matrices, si $C=AB$\n","\n","$$ c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj} . \\tag{2} $$\n","\n","Ahora tomemos la $j$ columna del lado derecho de la ecuacion (1).\n","De esta $j$ columna tomamos la fila $i$\n","\n","$$ (A B_j)_i = \\sum_{j=1}^n a_{ij} b_j  $$\n","pero esto es igual a la ecuacion (2).\n","\n","2.\n","$$ A x = \\sum_{j=1}^n x_j A_j . \\tag{3}$$\n","\n","Tomemos la componente $i$ en ambos lados de esta ecuacion\n","\n","$$(A x)_i = \\sum_{j=1}^n a_{ij} x_j = \\left ( \\sum_{j=1}^n x_j A_j  \\right )_i .$$\n","Al comparar las componentes $i$ en cada lado encontramos (3).\n","\n","\n","3.\n","Debemos probar esto si $p>n$.\n","$$ A \\Lambda = \\left [\\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_n A_n | 0_{n+1} | \\cdots | 0_p      \\right ] .$$\n","\n","Por ahora ignoremos si $p$ es mayor o menor que $n$.\n","\n","\n","\\begin{eqnarray}\n","A \\Lambda &=& \\left [  A \\Lambda_1 | A \\Lambda_2 | \\cdots | A \\Lambda_p   \\right ] \\quad \\text{ por la propiedad (1)} \\\\\n","&=& \\left [  \\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_p A_p \\right ] \\tag{4}.\n","\\end{eqnarray}\n","\n","Si $p >n$, todas las columnas, entonces los $\\lambda_i$, $n < i < p$ son 0. Y esto me introduce las columnas 0 que necesitamos.\n","\n","4. Finalmente\n","\n","Debemos probar esto:\n","\\begin{eqnarray}\n","U^* b  =\n","\\begin{pmatrix}\n","u_1^* b \\\\\n","u_2^* b \\\\\n","\\vdots \\\\\n","u_m^* b\n","\\end{pmatrix}\n","\\end{eqnarray}\n","\n","\n","La fila $i$ de $U^*b$ , es decir $(U^* b)_i$, se calcula\n","asi\n","\n","\n","$$ (U^*b )_i = \\sum_{j=1}^m \\overline{u_{ji}} b_j = \\langle u_i, b  \\rangle = u_i^* b$$\n","\n","\n","\n"],"metadata":{"id":"cPHHqBovBP9u"}},{"cell_type":"markdown","source":["## Diagonalizacion de matrices.\n","\n","Asumamos que todos los auto-vectores de $A$, $n \\times n$, son linealmente independientes. $A$ no tiene que ser autoadjunta. Asumamos que los autovectores son $P_1, P_2, \\cdots, P_n$. Tenemos entonces por definicion de autovector(valor) que para cada uno de estos existe $\\lambda_i$ tal que\n","\n","\n","$$ AP_i = \\lambda_i P_i .$$\n","\n","Si construicmos esto en matrices, usando la ecuacion (1) y la ecuacion (4)\n","\n","$$ AP = P \\Lambda . \\tag{5} $$\n","donde $P$ es la matriz de columnas $P_i$ y $\\Lambda$ es una matriz diagonal\n","\n","\n","Esta descomposicion es interesante. Podemos multiplicar a izquierda (5) por $P^{-1}$ ($P$ tiene inversa por que es $n \\times n$ y todos sus columnas son linealmente independientes)\n","\n","$$  P^{-1} A P_i = P^{-1} P \\Lambda.$$\n","$$  P^{-1} A P_i = I \\Lambda.$$\n","$$  P^{-1} A P_i =  \\Lambda.$$\n","\n","Esta es la diagonalizacion por que la matriz de la derecha es diagonal.\n","\n","En vez de multiplicar (5) por $P^{-1}$ a izquierda lo hacemos a derecha.\n","\n","\n","\\begin{eqnarray}\n"," AP &=& P \\Lambda  \\\\\n"," AP P^{-1} &=& P \\Lambda P^{-1} \\\\\n"," A I  &=& P \\Lambda P^{-1} \\\\\n"," A   &=& P \\Lambda P^{-1}\n","\\end{eqnarray}\n","Vemos que la matriz $A$ se puede \"factorizar\" (en el sentido de escribir como un producto de factores) en una matriz invertible, una diagonal y la inversa de la matriz invertible.\n","\n","Probamos el siguiente teorema:\n","Teorema (diagonalizacion) si $A \\in \\mathbb{C}^{n \\times n}$ tiene $n$ autovectores linealmente independientes, entonces existe una matriz invertible $P$ y otra $\\Lambda$ diagonal con autovalores de $A$ en la diagonal tal que $A=P \\Lambda P^{-1}$ y\n","$\\Lambda = P^{-1} A P$. Decimos que $A$ es **diagonalizable**.\n","\n","Aplicacion:\n","Elevar una matriz a un potencia.\n","Asumamos que $A$ es diagonalizable, entonces $A^k = P \\Lambda^k P^{-1}$.\n","\n","Hagamos la prueba por induccion.\n","* $k=1$, tenemos la hipotesis $A = P \\Lambda P^{-1}$.\n","\n","* Asumamos que es valido para $k$ es decir\n","$$ A^k = P \\Lambda^k P^{-1}. $$\n","\n","Veamos que pasa para $k+1$.\n","\n","\\begin{eqnarray}\n","A^{k+1} &=& A^k A \\\\\n","&=&  P \\Lambda^k P^{-1} (P \\Lambda P^{-1}) \\\\\n","&=& P \\Lambda^k (P^{-1} P) \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^k I \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^k  \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^{k+1}   P^{-1} \\\\\n","\\end{eqnarray}\n","Que es lo queremos probar.\n","\n","\n"],"metadata":{"id":"5pFHm74TGcXn"}},{"cell_type":"markdown","source":["# Proxima clase:\n","matriz positiva definida\n","SVD: Singular Value Decomposition.\n"],"metadata":{"id":"hl2vU6YILD2U"}}]}