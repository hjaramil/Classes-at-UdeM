{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP2PxfB6uzdTa3rxBA7vq2C"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Definicion: (**numero de condicion**, condition number)\n","Dada una matriz $A$ el numero de condicion es el radio\n","\n","$$\\kappa = \\frac{\\sigma_1}{\\sigma_n} .$$\n","Note que $\\sigma_1$ es el mayor de todos los valores singulares\n","y $\\sigma_n$ es el mas pequenho. Obvio que si la matriz no es cuadrada o es singular (no tiene inversa) $\\sigma_n=0$ y este numero es infinito. El numero de condicion para matrices no cuadradas, o para matrices singulares es $\\infty$.\n","\n","El valor mas pequenho que pueda tener $\\kappa$ es $1$. En este caso todos los valores singualres son iguales y la matriz es ortogonal. El valor $\\kappa > 1$\n","\n","Cuadra ejemplos simples:\n","\n","\\begin{eqnarray}\n","\\begin{pmatrix}\n","1 & 0  \\\\\n","0 & 1\n","\\end{pmatrix}\n","\\end{eqnarray}\n","Verificar que $\\kappa = 1$.\n","\n","\\begin{eqnarray}\n","\\begin{pmatrix}\n","10 & 0  \\\\\n","0 & 10\n","\\end{pmatrix}\n","\\end{eqnarray}\n","Verificar que $\\kappa = 1$.\n","\n","\n","\\begin{eqnarray}\n","\\begin{pmatrix}\n","8 & 0  \\\\\n","0 & 1\n","\\end{pmatrix}\n","\\end{eqnarray}\n","Verificar que $\\kappa = 8$.\n","Ejemplos mas interesantes son los del ejemplo de la clase anterior.\n","\n","\\begin{eqnarray}\n","A =\n","\\begin{pmatrix}\n","1 & 0 & 3 \\\\ 1 & 2 & 4\n","\\end{pmatrix}\n","\\end{eqnarray}\n","En este caso $\\kappa = 5.4/1.17 \\approx 4.61$.\n","\n","\n","El numero de condicion nos dice que tan estable\n","es un sistema $Ax=b$. Si el numero de condicion es cercano a 1, el sistema es muy estable. Si el numero de condicion es $\\gg 1$, el sistema es inestable y decimos que la matriz $A$ es \"ill conditioned\" (malamente condicionada), si $\\kappa$ esta cercano a 1 decimos que es \"well conditioned\" (bien condicionada).\n","Esto lo que quiere decir que cambios pequenhos en $b$ inducen cambios pequenhos en $x$."],"metadata":{"id":"J2WY5EylF8-C"}},{"cell_type":"markdown","source":["# Cociente de Rayliegh (Rayleigh quotient).\n","Asumimos que $A$ es autoadjunta, definimos el cociente de Rayleigh para $A$ con respecto a un vector $x$, $R(A,x)$ como\n","\n","\n","$$R(A,x) = \\frac{x^* A x}{x^* x} = \\frac{\\langle x, A x \\rangle}{\\| x \\|^2} = \\frac{\\langle Ax, x \\rangle }{\\| x \\|^2} .$$\n","Tambien lo podemos escribir como\n","\n","\n","$$R(A, x) = \\frac{x^*}{\\| x \\|} A \\frac{x}{\\| x\\|} = u^* A u .$$\n","donde $\\| u \\| = 1$, con $u = x / \\| x \\|$.\n","Vamos a ver mas adelante que el cociente de Rayleigh mide la dispersion de datos cuando se proyectan a lo largo de la recta con direccion $u$.\n","\n","Teorema (Cociente de Rayleigh): El maximo valor propio de una matriz autoadjunta $n \\times n$ es el cociente de Rayliegh mayor, maximo cociente de Rayleigh.\n","\n","**Prueba:**: Asuamamos que $u$ (noramalizado) es un vector unitario.\n","Cada autovector de $A$ se puede escribir como una combinacion lineal de vectores base $\\{u_1, u_2, \\cdots, u_n \\}$. Entonces\n","\n","\n","$$ u = \\sum_{i=1}^n \\alpha_i u_i .$$\n","entonces\n","\n","$$ R(A, u) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j  \\tag{1} .$$\n","Estos vectores base si son autvectores. (por el teorema espectral, y son ortonormales)\n","Como cada $u_j$ es autovector de $A$ con autovalor $\\lambda_j$ entonces\n","\n","\n","$$ u_i^* A u_j = u_i^* ( \\lambda_j u_j) = \\lambda_j u_i^* u_j = \\lambda_j \\delta_{ij}  \\tag{2} .$$\n","Insertamos (2) en (1).\n","\n","$$ R(A, u) =\n","\\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j u_i^* A u_j\n","= \\sum_{i=1}^n \\sum_{j=1}^n \\overline{\\alpha_i} \\alpha_j \\lambda_j \\delta_{ij} =  \\sum_{i=1}^n | \\alpha_i |^2 \\lambda_i.\n","$$\n","\n","Note que $\\sum | \\alpha_i |^2 = 1$, por que $\\alpha_i$ son los coeficientes de $u$ en la base de los $u_i$. Es decir $1=\\| u \\|^2 = \\sum_{i=1}^n | \\alpha _i |^2$.o\n","\n","Asumamos, sin perdida de generalidad que $\\lambda_1$ es el mas grande de todos los autovalores (son reales). El valor mas grande que pueda tomar $R(A, u)$ es cuando $\\alpha_1 =1$, $\\alpha_i=0$, $i=2,3, \\cdots, n$.\n","\n","Es decir\n","$$\\max_{u} R(A, u) = \\lambda_1 .$$\n","\n","En las notas de clase tengo otra prueba adicional de esto. La dejo para que la miren.\n","\n","Teorema: **Descomposicion Cholesky**: Si $A$ es positiva definida, autoadjunta, se puede escribir como el producto de dos matrices $L L^*$ donde $L$ es triangular inferior con entradas positivas.\n","\n","$$A = L L^* .$$\n","Si $L$ es autoadjunta entonces $L=L^*$ y tneemos que $A= L^2$ y\n","$L$ seria la **raiz cuadrada** de la matriz  $A$.\n","La prueba se deja al estudiante.\n","\n","\n","# Calculo matricial:\n","Pensemos en matrices reales $A$ de orden $m \\times n$. donde cada entrada es una funcion de un vector. Es decir  $a_{ij} = a_{ij}(x)$ donde $x \\in \\mathbb{R}^n$.  \n","\n","Definicion (**derivada parcial de una matriz**): Sea $A= A(x)$, una matriz $m \\times n$, cuyos elementos $a_{ij}$ son funciones de $x \\in \\mathbb{R}^p$\n","entonces\n","\n","$$ \\frac{\\partial A}{\\partial x_k} = \\left [ \\frac{\\partial a_{ij}}{\\partial x_k} \\right ] \\quad , \\quad k=1,2, \\cdots, p \\quad, \\quad i=1,2 , \\cdots , m \\quad , \\quad j=1,2, \\cdots, n$$\n","Ejemplo a mano alzada.\n","\n","Teorema: Si $A$ es una matriz constante (que no depende de $x$)\n","entonces la derivada de $A$ es la matriz $0_{m \\times n}$\n","\n","\n","$$\\frac{\\partial A}{\\partial x_k} = \\left [\n","    \\frac{\\partial a_{ij}} {\\partial x_k} \\right ] =\\left [ 0 \\right ].$$\n","\n","Es decir\n","\n","$$ \\frac{\\partial A}{\\partial x_k}= 0_{m \\times n} .$$\n","\n","Teorema: Si $A$ es $m \\times n$ constante, y $x \\in \\mathbb{R}^n$, entonces\n","\n","\n","$$ \\nabla (A x) = A .$$\n","donde $\\nabla $ es gradiente, o sea vector de derivadas parciales.\n","Es decir\n","\n","$$ \\frac{\\partial (Ax)}{\\partial x_k} = \\nabla_k (Ax) = A_k .$$\n","donde $A_k$ es la columna $k$ de la matriz $A$.\n","\n","**Prueba:**:\n","La componente $y_i$ de $Ax$ se escribe como\n","\n","$$y_i = \\sum_{j=1}^n a_{ij} x_j .$$\n","Entonces, como la derivada es lineal\n","(usamos $\\partial x_i / \\partial x_k = \\delta_{ik}.$\n","\n","$$ \\frac{\\partial y_i}{\\partial x_k}= \\sum_{j=1}^n a_{ij} \\frac{\\partial x_j}{\\partial x_k} = \\sum_{j=1}^n a_{ij} \\delta_{jk} = a_{ik}.$$\n","para $k$ fijo esa es la columna $k$ de $A$, o $A_k$.\n","\n","\n","Teorema: **producto interno**. Sean $f(x)$, $g(x)$ funciones vectoriales en la variable $x \\in \\mathbb{R}^n$, podemos definir su producto interno (para cada $x \\in \\mathbb{R}^n$). De forma que\n","\n","$$\\frac{\\partial \\langle f, g \\rangle }{\\partial x_i} = \\left \\langle \\frac{\\partial f}{\\partial x_i}, g  \\right \\rangle + \\left \\langle f, \\frac{\\partial g}{\\partial x_i}\\right \\rangle.$$\n","\\\n","Esta es la regla del producto para productos internos\n","\n","**Prueba**; Usemos la siguiente definicion\n","\n","$$\\langle f, g \\rangle = \\sum_{j=1}^n \\overline{f_j(x)} g_j(x) .$$\n","\n","Entonces aplicamos lo que sabemos del la derivada del producto, y usamos la linealidad de la derivada\n","\n","$$ \\frac{\\partial \\langle f, g \\rangle }{\\partial x_i} = \\sum_{j=1}^n \\frac{\\partial }{\\partial x_j} \\overline{f_j(x)} g_j(x) =\n","\\sum_{j=1}^n \\frac{\\partial \\overline{f_j(x)}}{\\partial x_i} g_j(x) + \\overline{f_j(x)} \\frac{\\partial g_j(x)}{\\partial x_i} .$$\n","\n","Es decir\n","\n","\n","$$\\frac{\\partial \\langle f, g \\rangle }{\\partial x_j} =\n","\\left \\langle \\frac{\\partial f}{\\partial x_j} , g \\right \\rangle\n","+ \\left \\langle f , \\frac{\\partial g}{\\partial x_j} \\right \\rangle . $$\n","\n","Usemos la notacion $\\nabla$\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"pQxzfQdN8aZM"}},{"cell_type":"markdown","source":[],"metadata":{"id":"zaRdM_Rbpt8v"}},{"cell_type":"markdown","source":[],"metadata":{"id":"Jdta_D_jpvfY"}},{"cell_type":"markdown","source":["$$ f(x) = \\frac12 \\langle Ax - b , Ax-b \\rangle  .$$\n"],"metadata":{"id":"NETbRMiHM8hN"}},{"cell_type":"markdown","source":["# En la proxima encontramos $\\nabla f$.\n","\n","\\begin{eqnarray}\n","\\nabla_k f(x) =\n","\\end{eqnarray}"],"metadata":{"id":"ubos_kwdNgb2"}},{"cell_type":"markdown","source":["$$\\nabla \\langle f, g \\rangle = \\langle \\nabla f, g \\rangle + \\langle f, \\nabla g \\rangle  .$$\n","**Ojoooo**  Esta expresion no tiene sentido.  Pues $\\nabla f$ es matriz y $g$ es vector, entonces realmente lo que debemos decir es que la componente $k$ a ambos lados es la misma y escribir\n","\n","\n","$$\\nabla \\langle f, g \\rangle_k = \\langle \\nabla_k f, g \\rangle + \\langle f, \\nabla_k g \\rangle  \\tag{4} .$$\n","\n","\n","El siguiente teorema es la base de los **minimos cuadrados**.\n","Es un ejemplo\n","\n","Ejemplo. Sea $f(x) = \\frac12 \\| Ax - b \\|^2$, con $x \\in \\mathbb{R}^n$, $A$ es una matriz $m \\times n$, real Encuentre $\\nabla f$.\n","\n","Recuerde que\n","\n","\n","$$ f(x) = \\frac12 \\langle Ax - b , Ax - b \\rangle.$$\n","De forma que\n","\n","\\begin{eqnarray}\n","\\nabla_k f(x) &=& \\frac12 \\left [  \\langle \\nabla_k(Ax-b), (Ax, -b) \\rangle + \\langle Ax - b, \\nabla_k(Ax - b)  \\right ] \\quad, \\text{por  } (4) \\\\\n","&=& \\langle \\nabla_k(Ax-b), (Ax -b) \\rangle   \\\\\n","&=& \\langle A_k, Ax - b \\rangle\n","\\end{eqnarray}\n","Si llevamos esto a forma matricial encontramos que\n","\n","$$ \\nabla f(x) = A^* (Ax - b) = A^*A  x - A^* b .$$`\n","note que si queremos que $\\nabla f(x) = 0$ (que es lo que se hace para hallar extremos) entonces\n","\n","$$ A^*A  x = A^*b .$$\n","\n","Esta sistema es muy importante y se le conoce como las **ecuaciones normales** (normal equations). Son las que se usan\n","para resolver el problema de **minimos cuadrados** que abordamos en este momento.\n"],"metadata":{"id":"wbnSEiPtpx5Q"}}]}