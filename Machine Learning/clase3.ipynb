{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"clase3.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMMJyP3gfR1eGpVJEH86C+N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Un teorema importante sobre multiplicacion de matrices.\n","\n","$A$ es una matriz $m \\times n$, $B$ es $n \\times p$, $x$ es un vector $x \\in \\mathbb{R}^n$, $\\Lambda$ es diagonal $n \\times p$ donde en la diagonal se encuentran los valores. \n","\n","* $$AB = [ A B_1 | AB_2 | \\cdots | A B_p]  \\quad (1)$$\n","donde $B_i$ es la columna $i$ de la matriz $B$.\n","\n","\n","* $$Ax =\\sum_{j=1}^n x_j A_j \\quad , \\quad  (2) $$\n","$A_j$ es la columna $j$ de $A$.\n","\n","* Si $p \\ge n$ entonces\n","\n","$$A \\Lambda = [ \\lambda_1 A_1 | \\lambda_2 A_2 \\cdots | \\lambda_n A_n | 0_{n+1} | \\cdots | 0_p] \\quad \\quad (3)  $$\n","\n","Aca se agrean $p-n$ columnas de ceros. Si $n>p$,\n","\n","$$A \\Lambda = [ \\lambda_1 A_1 | \\lambda_2 A_2 \\cdots | \\lambda_n A_p ]   $$\n","\n","* Si $u_i$ es la columan $i$ de la matriz $m \\times m$ $U$ entonces\n","\n","$$ U^* b = \\begin{pmatrix} u_1^* b \\\\\n","u_2^* b \\\\ \\vdots \\\\ u_m^* b \\end{pmatrix} \\quad , \\quad (4) $$\n","\n"],"metadata":{"id":"0urZDRl697jb"}},{"cell_type":"markdown","source":[""],"metadata":{"id":"Zy1P9tzMEFhi"}},{"cell_type":"markdown","source":["**Prueba**.\n","\n","Probemos (1).\n","\n"," $$AB = [ A B_1 | AB_2 | \\cdots | A B_p]  \\quad (1)$$\n","\n"," Una forma de probar es tolar la componente $ij$ en ambos lados y ver es la misma.\n","\n"," Llamemos $C=AB$, entonces\n","\n"," $$ c_{ij} = \\sum_{k=1}^n a_{ik} b_{kj} $$.\n"," Tomemos ahora la columna $j$ del lado derecho de la ecuacion (1) . El elemento $i$ de esta columna\n","\n"," $$(A B_j)_i = \\sum_{k=1}^n a_{ij} b_{jk}  $$\n","\n"," Vamos para (2).\n","\n"," $$Ax =\\sum_{j=1}^n x_j A_j \\quad , \\quad  (2) $$\n","$A_j$ es la columna $j$ de $A$.\n","\n","Tomemos la componente $i$ a la izquierda\n","\n","$$ (Ax)_i = \\sum_{j=1}^n a_{ij} x_j = \n","\\left ( \\sum_{j=1}^n x_j A_j \\right)_i$$\n","\n","\n","Veamos la (3) \n","\n","$$A \\Lambda = [ \\lambda_1 A_1 | \\lambda_2 A_2 \\cdots | \\lambda_n A_n | 0_{n+1} | \\cdots | 0_p] \\quad \\quad (3)  $$\n","\n","\n","\n","\\begin{eqnarray}\n","A \\Lambda &=& [ A \\Lambda_1 | A \\Lambda_2 | \\cdots A \\Lambda_p ]  \\quad \\text{por la propiedad (2)} \\\\\n","&=& [ \\lambda_1 A_1 | \\lambda_2 A_2 | \\cdots | \\lambda_p A_p ]\n","\\end{eqnarray}\n","\n","Si $p$ es mayor que $n$ (solo hay $\\lambda_1 , \\lambda_2, \\cdots \\lambda_n$)."],"metadata":{"id":"piq0vp27AhLF"}},{"cell_type":"markdown","source":["El ultimo resultado. El (4) \n","\n","$$ U^* b = \\begin{pmatrix} u_1^* b \\\\\n","u_2^* b \\\\ \\vdots \\\\ u_m^* b \\end{pmatrix} \\quad , \\quad (4) $$\n","\n","Prueba: Fijemos $i$\n","\n","$$ ( U^* b)_i = \\sum_{j=1}^m \\overline{u_{ji}} b_j = \\langle u_i, b \\rangle = u_i^* b  \\quad , \\quad i=1,2, \\cdots , n$$\n"],"metadata":{"id":"Ey4umuoB-RaN"}},{"cell_type":"markdown","source":["## repaso del final de la clase anterior.\n","\n","Recordemos que el teorema espectral representa una matriz autoadjunta como el producto de tres matrices , $A= Q \\Lambda Q^*$\n","\n","Quisieramos tener propiedades parecidas para todas las matrices. Pero esto no se puede.\n","\n","Un caso interesante (de la clase pasada) asuma que todos autovectores son linealmente independientes. \n","\n","Tenemos las siguientes ecuaciones:\n","\n","$$ AP_i = \\lambda_i P_i $$\n","\n","donde $P_i$ es el $i$ autovector linealmente independiente. \n","\n","$$ [AP_1 | AP_2 | \\cdots | AP_n ] = [\\lambda_1 P_1 | \\lambda_2 P_2 | \\cdots | \\lambda_n P_n ] $$\n","\n","$$ AP = P \\Lambda$$\n","\n","$P$ es invertible por que las columnas de $P$ son linealmente independientes (Algebra lineal I).\n","Multipliquemos a ambos lados por $P^{-1}$\n","\n","$$ A P P^{-1} = P \\Lambda P^{-1} $$\n","$$ A  = P \\Lambda P^{-1} $$\n","\n","Teorema: Si los autovectores de $A$ son linealmente independientes, la matriz $A$ es **diagonalizable**\n","\n","Repaso. Esto puede servir para elevar potencias.\n","\n","Aplicacion: $A^k$.\n","\n","Teorema: $A^k$ es (si la matriz es diagonalizable) esta dado por\n","\n","$$A^k = P \\Lambda^k P^{-1} $$\n","\n","Prueba (por induccion). Para $k=1$, \n","$A = P \\lambda P^{-1}$ (esto lo acabamos de probar.\n","\n","Asumamos que $A^k = P \\Lambda^k P^{-1}$ para $k>1$\n","entonces:\n","\n","\\begin{eqnarray}\n","A^{k+1} &=& A^k A \\\\\n","&=& (P \\Lambda^k P^{-1}) (P \\Lambda P^{-1}) \\\\\n","&=& P \\Lambda^k ( P^{-1} P ) \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^k \\Lambda P^{-1} \\\\\n","&=& P \\Lambda^{k+1} P^{-1} \n","\\end{eqnarray}\n","\n","Elevar $A$ a una potencia con esta representacion es algo simple. Por que toca hallar $P, P^{-1}$, \n","Se eleva solo $\\Lambda^k$. Lo otros son dos productos uno de ellos muy simple. \n","\n","\n","Definicion de **positiva definida**, **no-negativa definida**\n","\n","En Wikipedia la definicion es una matriz $A$ tal que\n","$x^T A x > 0$, para todo $x \\in \\mathbb{C}^n$. \n","\n","Para nosotros **positiva definida** es una matriz donde todos sus autovalores son positivos ($A$ es cuadrado). \n","\n","Las dos definiciones (Wikipedia y la mia) son equivalentes. \n","\n","**no negativa definida** es que los autovalores son 0 o positivos. \n","\n","Teorema: Si $A$ es positiva definida entonces $A^{-1}$ existe. (tarea).\n","\n","\n","\n","\n","\n"],"metadata":{"id":"nuqFuz_7FAo9"}},{"cell_type":"markdown","source":["## Torema de descomposicion de valores singulares (SVD: Singular Value Decomposition).\n","\n","Dada una matriz $A \\in \\mathbb{C}^{m \\times n}$. Existe una matriz ortogonal $U \\in \\mathbb{C}^{m \\times m}$, una matriz ortogonal $V \\in \\mathbb{n \\times n}$ y una matriz diagonal $\\Sigma \\in \\mathbb{R}^{m \\times n} $ con $\\sigma_i \\in \\mathbb{R^+} \\cup \\{ 0 \\}$ tal que\n","\n","$$ A = U \\Sigma V^* $$\n","\n","Prueba se debe a Cornelius Lanczos.\n","\n","Comencemos: Definamos $S$\n","\n","\\begin{eqnarray}\n","S = \\begin{pmatrix}  0_{m \\times n}  & | & A  \\\\\n","-- & -- & --  \\\\\n","A^* & | & 0_{n \\times m} \n","\\end{pmatrix}\n","\\end{eqnarray}\n","La dimension de $S$ es $S \\in \\mathbb{C}^{(m+n) \\times (m+n)} $\n","La matriz $S$ es **autoadjunta**. Podemos aplicar el teorema de descomposicion espectral donde dice que existen $w_1, w_2, \\cdots, w_{m+n}$ autovectores ortogonales . $w_i \\in \\mathbb{C}^{n + m}$.\n","\n","$$ S w_i = \\lambda w_i  \\quad , \\quad (1) $$\n","\n","Partimos $w_i$ en dos partes.\n","\n","$$ \\begin{pmatrix} u_i \\\\ v_i  \\end{pmatrix} $$\n","$u_i \\in \\mathbb{C}^m$, $v_i \\in \\mathbb{C}^n$. \n","De (1) \n","\n","\\begin{eqnarray}\n","A^* u_i &=& \\lambda_i v_i  \\quad , \\quad (2)  \\\\\n","A v_i &=& \\lambda_i u_i  \\quad , \\quad (3)\n","\\end{eqnarray}\n","\n","Multiplicamos la primera de estas ecuaciones por $A$ y la seguna por $A^*$\n","\n","\n","\\begin{eqnarray}\n","A A^* u_i &=& \\lambda_i A v_i = \\lambda_i (\\lambda_i u_i) = \\lambda_i^2 u_i   \\\\\n","A^*A v_i &=& \\lambda_i A^* u_i = \\lambda_i \\lambda_i v_i = \\lambda_i^2 v_i \n","\\end{eqnarray}\n","\n","Resultado: Los $u_i$ son autovectores de $A A^*$ con autovalores $\\lambda_i$, y los $v_i$ son autovectores de $A^* A$ con autovalores $\\lambda_i$\n","(los $\\lambda_i$ son autovalores comunes a $A^*A$ y  a $A A^*$.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"gPtDHYctJEH0"}},{"cell_type":"markdown","source":["Construimos las matrices $U=[u_1 | u_2 | \\cdots | u_m ]$, $V=[v_1 | v_2 | \\cdots | v_n ]$.\n","\n","Es posible que algunos autovalres $\\lambda_i^2$ sen cero. Asumamos que el **ultimo** autovalor no cero es el $r$. Es decir $\\lambda_1 >0, \\lambda_2 > 0, \\cdots \\lambda_r > 0$, $\\lambda_i=0$, $i=r+1, r+2, \\cdots \\max\\{m,n\\}$\n","\n","Definamos dos nuevas matrices $U_r, V_r$ donde las\n","columnas son los primeros autovectores (que no corresponden a autovalores 0). Tambien $U_0, V_0$\n","son las matrices correspondientes a las columnas autovectores con autovalores 0.\n","\n","\n","\\begin{eqnarray}\n","A^* u_i &=& \\lambda_i v_i  \\quad , \\quad (2)  \\\\\n","A v_i &=& \\lambda_i u_i  \\quad , \\quad (3)\n","\\end{eqnarray}\n","\n","Recordando el teorema de multiplicacion de matrices\n","al principio de la clase\n","\n","De la (2) y la (3)\n","\n","\\begin{eqnarray}\n","A* U_r &=& V_r \\Lambda_r \\quad , \\quad A^* U_0 = 0 \\quad, \\quad \\text{de la (2)} \\\\\n","A V_r &=& U_r \\Lambda_r \\quad , \\quad A V_0 = 0 \\quad , \\quad \\text{de la (3) , esta  (4)} \n","\\end{eqnarray}\n","\n","Donde $\\Lambda_r$  es la matriz cuadrada diagonal con $\\lambda_i$ en la diagonal.\n","\n","Si definimos\n","\n","$$ \\Sigma = \\begin{pmatrix}  \\Lambda_r & | & 0 \\\\\n","-- & -- & -- \\\\\n","0 & | & 0  \\end{pmatrix}$$\n","De la (4)\n","\n","tenemos:\n","\n","$$ A V = U \\Sigma  \\quad , \\quad (5) $$\n","\n","La prueba de esto es la siguiente:\n","$$ AV = A[ V_r | V_0] =[ A V_r | A V_0] = [U_r \\Lambda_r | 0 ] = [ U_r \\Lambda_r + U_0 \\times 0 | U_r \\times 0 + U_0 \\times 0] = U \\Sigma $$\n","\n","Queda por revisar: el primero 0 tiene dimensiones $(m-r) \\times r$, el segundo $m \\times (n-r)$ y el tercero tiene dimensiones $(m-r) \\times (n-r)$.\n","\n","En la ecuacion (5) multiplicamos a ambos lados por $V^*$ y obtenemos\n","\n","$A = U \\Sigma V^*$. \n","\n","\n","\n"],"metadata":{"id":"9FK8yI3YNkRL"}},{"cell_type":"markdown","source":["Encuentre la descomposicion (factorizacion) SVD de la matriz\n","\n","$$ \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4 \\end{pmatrix} $$\n","\n","Para la solucion contemplamos los siguientes pasos:\n","\n","* Contruya $A^* A$, $A A^*$\n","* Encuentre los autovalores de $A^*A, A A^*$ (son compartidos. Como $A^*A$ y $A A^*$ no necesariamente tienen las mismas dimensiones, encuentre los de la matriz mas pequena y complete con 0. \n","\n","* Encuentre la matriz $U$  y $V$ que son los autovectores de las matrice $A A^*$ y $A^* A$ respectivamente.\n","\n","* La matriz $\\Sigma$ es la matriz con los autovalores.\n","\n","* Se ordenar de mayor a menor por que la representacion $U\\Sigma V^T$ no es uncia.\n","\n","Solucion:\n","\n","* Construyamos $A^* A $ y $A A^*$\n","\n","\\begin{eqnarray}\n","A A^* = \\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4 \\end{pmatrix} \\begin{pmatrix} 1 & 1 \\\\ \n","0 & 2 \\\\\n"," 3 & 4\\end{pmatrix} = \n"," \\begin{pmatrix}\n"," 10 & 13 \\\\\n"," 13 & 21\n"," \\end{pmatrix}\n","\\end{eqnarray}\n","\n","* Se hallan los autovalores de $A A^*$\n","El polinomio caracteristico es de una cuadratica\n","\n","$p(\\lambda) =\\det \\begin{pmatrix} 10 - \\lambda & 13 \\\\ 13 & 21 - \\lambda \\end{pmatrix} $\n","\n","Con ayuda de WolframAlpha \n","$$ \\lambda_1 = 29.61  \\quad , \\quad \\lambda_2=1.38 $$\n","\n","Con estos autovalores se hallan los autovectores.\n","\n","$$ U_1 = \\begin{pmatrix} 0.55 \\\\ 0.83 \\end{pmatrix} $$\n","$$ U_2 = \\begin{pmatrix} -0.83 \\\\ 0.55 \\end{pmatrix}$$\n","\n","Calculamos $A A^*$\n","\n","\\begin{eqnarray}\n","A* A \n","\\begin{pmatrix} 1 & 1 \\\\ 0 & 2 \\\\ 3 & 4 \\end{pmatrix}\n","\\begin{pmatrix} 1 & 0 & 3 \\\\ 1 & 2 & 4 \\end{pmatrix}\n","= \\begin{pmatrix} 2 & 2 & 7 \\\\\n","2 & 4 & 8 \\\\\n","7 & 8 & 25\n","\\end{pmatrix}\n","\\end{eqnarray}\n","Toca hallar los autovalores (que son los mismos) solo  que uno va a ser 0. \n","\n","Los autovalores son $\\lambda_1^2 = 29.61, \\lambda_2^2= 1.38, \\lambda_3^2 = 0$\n","\n","Ya tenemos como formar $\\Sigma$, \n","$\\sigma_1=\\sqrt{\\lambda_1^2} = 5.44$\n","$\\sigma_2 = \\sqrt{\\lambda_2^2} = \\sqrt{1.38}=1.17\n","$\\simg_3 = 0$\n","\n","De forma\n","\n","$$\n","\\Sigma = \\begin{pmatrix}  5.44 & 0 & 0 \\\\\n","0 & 1.17 & 0 \\\\\n","\\end{pmatrix}\n","$$\n","\n","Autovectores de $A^* A$.\n","\n","De WolframAlpha\n","\n","$$V_1 = \\begin{pmatrix} 0.25 \\\\ 0.31 \\\\ 0.92 \\end{pmatrix}$$\n","\n","\n","$$V_1 = \\begin{pmatrix} 0.23 \\\\ -0.94 \\\\ 0.25 \\end{pmatrix}$$\n","\n","$$V_1 = \\begin{pmatrix} -0.94 \\\\ -0.16 \\\\ 0.31 \\end{pmatrix}$$\n","\n","\n","La matriz $U$\n","\n","$$\n","U= \\begin{pmatrix} 0.55 & - 0.83 \\\\  0.83 & 0.55 \\end{pmatrix}\n","$$\n","\n","$$\n","V= \\begin{pmatrix} 0.25 & 0.23 &-0.94 \\\\\n","0.31 & -0.94 & -0.16 \\\\\n","0.92 & 0.25 & -0.31  \\end{pmatrix}\n","$$\n","\n","De forma que\n","\\begin{eqnarray}\n","A = \\begin{pmatrix} 0.55 & - 0.83 \\\\  0.83 & 0.55 \\end{pmatrix}  \\begin{pmatrix}  5.44 & 0 & 0 \\\\\n","0 & 1.17 & 0 \\\\ \n","\\end{pmatrix} \n","\\begin{pmatrix}\n","0.25 & 0.31 & 0.92 \\\\\n","0.23 & -0.94 & 0.25 \\\\\n","-0.94 & -.16 & -0.31\n","\\end{pmatrix}\n","\\end{eqnarray}"],"metadata":{"id":"pE9pXvULZHv2"}},{"cell_type":"markdown","source":["Una vez tenemos la matriz de valores singulares, podemos definir el numero de condicion. **condition number** $\\kappa$.\n","\n","$$ \\kappa = \\frac{\\sigma_1}{\\sigma_n} $$\n","\n","El mas grande dividido el mas pequeno.\n","Recuerde que estan organizados de mayor a menor. Si algun $\\sigma_i=0$, $\\sigma_n=0$\n","y $\\kappa = \\infty$.\n","\n","Que tan bien condicionado esta el problema $Ax = b$.\n","\n","El condition number mas pequeno es $1$. Este ocurre en las matrices ortogonales.\n","\n","E. Ward Cheney y David R. Kincaid: dicen:\n","la magnitud de el numero de condicion estima el numero de cifras decimales que se pierden en el calculo $\\kappa = 10^{k}$.  Una regla del dedo gordo (thumb rule)\n","\n","Si un sistema $Ax=b$ es tal que $\\kappa= \\kappa(A)  \\gg 1$, decimos que es ill posed (mal condicionado), si $\\kappa \\approx 1$ esta bien condicionado.\n","\n","### Cociente de Rayleigh.\n","\n","Dada una matriz autoadjunta $A$ definimos el cociente de Rayleigh como\n","\n","$$R(A, x) = \\frac{x^* A x}{x^* x} = \\frac{\\langle x, A x \\rangle }{\\| x \\|^2} = \\frac{\\langle Ax, x \\rangle }{\\| x \\|^2 }$$\n","\n","Esto lo podemos escribir tambien como:\n","\n","$$ R(A,x) = \\frac{x^*}{\\| x \\|} A \\frac{x}{\\| x\\|} = u^* A u $$\n","donde $u$ es unitario.\n","\n","Teorema: El valor mas grande del cociente de Rayleigh de una matriz $A$ , $n \\times n$ autoadjunta, es el autovalor mas grande de la matriz. \n","\n","Prueba: Asumamos un vector unitario $u$.\n","Este se puede representar en terminos de una base ortonormal (de autovectores). Teorema de descomposicion espectral)\n","\n","$$ u = \\sum_{i=1}^n \\alpha_i u_i $$\n","\n","$$R(A, u) = u^* A u = \\sum_{i=1}^n \\sum_{j=1}^j \\overline{\\alpha_i} \\alpha_j u_i^* A u_j $$\n","\n","$$u_i^* A u_j = u_i^* (\\lambda_j u_j)= \\lambda_j u_i^* u_j = \\lambda_j \\delta_{ij}  $$\n","\n","$$ R(A, u) = \\sum_{i=1}^n \\sum_{j=1}^j \\overline{\\alpha_i} \\alpha_j u_i^* A u_j=\n","\\sum_{i=1}^n \\sum_{j=1}^j \\overline{\\alpha_i} \\alpha_j \\lambda_j \\delta_{ij} = \\sum_{i=1}^n \\overline{\\alpha}_i \\alpha_i \\lambda_i = \\sum_{i=1}^n | \\alpha_i |^2 \\lambda_i  $$\n","\n","Sabemos $\\sum_{i=1}^n |\\alpha_i|^2 = 1$\n","Sin perdida de generalidad asumamos que $\\lambda_1$ es el mas grande de todos. Esta suma es mas grande\n","cuando $\\alpha_1=1$, $\\alpha_2 = \\alpha_3 = \\cdots = \\alpha_n=0$. Es decir que valor mas grande del \n","cociente de Rayleigh es $\\lambda_1$. \n","\n","\n","Descomposicion de **Cholesky**\n","Si $A$ es una matriz auto-adjunta, positiva definida, se puede escribir como el producto de dos matrices $L$, $L^*$ , donde $L$ es triangular inferior.\n","Es decir\n","$$A = L^* L  $$\n","Si $L$ es autoadjunta, entonces es diagonal ($L$ triangular inferior) y es real. \n","\n","$$ A = L L = L^2$$ y de esta forma decimos que\n","$L = \\sqrt{A}$. \n","\n","\n","\n","\n"],"metadata":{"id":"iyMsc7TSnbsK"}},{"cell_type":"code","source":[""],"metadata":{"id":"Y2S3_gXLmFcK"},"execution_count":null,"outputs":[]}]}