{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMppqSZ7vY70D1pH5q1hfDE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# El uso de multiplicadores de Lagrange para resolver el problema de SVD.\n","\n","Recordemos la funcion objetivo para SVM:\n","\n","$$\\text{minimizar} \\quad J(w) = \\frac{\\| w \\|^2}{2} $$\n","$$ \\text{ sujeto a } y_i ( w^T x_i - b) \\ge 1 \\quad , i=1,2, \\cdots, m$$\n","\n","O de otra forma\n","\n","$$\\text{minimizar} \\quad J(w) = \\frac{\\| w \\|^2}{2} $$\n","$$ \\text{ sujeto a } [ y_i ( w^T x_i - b) -1] \\ge 0 \\quad , i=1,2, \\cdots, m$$\n","\n","Un problema general de Multiplicadores de Lagrange\n","\n","$$\\min f(\\theta) \\quad , \\quad \\text{sujeto a} $$\n","$$ g_i(\\theta ) \\ge 0 $$.\n","\n","Es suficiente pensar en $g_i(\\theta)=0$, que es la frontera de la region $g_i(\\theta) \\ge 0$.\n","\n","El problema de multiplicadores de Lagrange es establecer la siguiente funcion de costo\n","\n","\n","$$L(\\theta, \\lambda) = f(\\theta) - \\sum_i \\lambda_i g_i(\\theta) \\tag{1} $$\n","\n","Para el problema que queremos resolver\n","Cambiano nombres $\\theta \\to w$.\n","$$ f(w) = \\frac12 \\| w \\|^2 $$\n","$$ g(w) = y_i[ ( w \\cdot x_i - b) - 1]$$\n","De forma que la Ecuacion (1) se convierte en\n","\n","$$L(w, b, \\lambda) = \\frac12 \\| w \\|^2 - \\sum_i \\lambda_i\n","[ y_i( w \\cdot x_i - b) - 1 ] \\tag{2} $$\n","\n","La funcion $L$ es la funcion objetivo, via multiplicadores de Lagrange que toca minimizar. Para esto toca hallar el gradiente de $L$. Es decir,  ( $\\nabla L$)\n","\n","\n","$$\\nabla_w L(w, b, \\lambda) =\\nabla_w \\left [  \\frac12 \\| w \\|^2 - \\sum_i \\lambda_i\n","[ y_i( w \\cdot x_i - b) - 1 ] \\right ] = 0 $$\n","\n","\n"],"metadata":{"id":"C2R6kw6kndm1"}},{"cell_type":"markdown","source":["Vamos por partes.\n","\n","$$\\frac{\\partial}{\\partial w_j} \\left ( \\frac12 \\| w \\|^2  \\right ) = \\frac{\\partial}{\\partial w_j} \\left ( \\frac12 \\sum_i w_i^2 \\right ) = \\sum_i \\frac{\\partial w_i}{\\partial w_j} w_i =\\sum_i \\delta_{ij} w_i =w_j. $$"],"metadata":{"id":"mjkjvRRywzyc"}},{"cell_type":"markdown","source":["\\begin{eqnarray}\n","\\frac{\\partial }{\\partial w_j} \\left (\n","    \\sum_i \\lambda_i [ y_i( w \\cdot x_i - b) -1]\n","    \\right ) &=& \\frac{\\partial}{\\partial w_j} \\left (\n","        \\sum_i \\lambda_i \\left [\n","            y_i \\left (\n","                \\sum_k w_k (x_i)_k - b\n","                \\right ) - 1\n","            \\right ]\n","        \\right ) \\\\\n","        &=& \\sum_i \\lambda_i \\left [\n","            y_i \\left (\n","                \\sum_k \\delta_{kj}(x_i)_k\n","                \\right )\n","            \\right ] \\\\\n","    &=&  \\sum_i \\lambda_i y_i (x_i)_j.\n","\\end{eqnarray}"],"metadata":{"id":"iDxwwMauxmh9"}},{"cell_type":"markdown","source":["Considerando ambos terminos tenemos:\n","\n","$$\\nabla_w L(w, b, \\lambda) = w_j - \\sum_i \\lambda_i y_i(x_i)_j = 0$$\n","\n","De forma vectorial podemos escribir\n","\n","$$w = \\sum_i \\lambda_i y_i x_i . \\tag{3} $$\n","donde $\\lambda_i, y_i$ son escalares y $x_i, w$ son vectores.\n","\n","Tomemos ahora la derivada con respecto a $b$\n","como $\\frac12 \\| w \\|^2$ no depende de $b$, solo\n","nos quedamos con el segundo termino de (2).\n","\n","$$\\frac{\\partial}{\\partial b} \\left (  \n","    \\sum_i \\lambda_i [ y_i(w \\cdot x_i - b) - 1]\n","    \\right ) = -\\sum_i \\lambda_i y_i = 0 $$\n","\n","Es decir, encontramos que\n","\n","$$\\sum_i \\lambda_i y_i = 0. $$\n","\n","A lo ultimo vamos a poner todo en terminos de $\\lambda$. Recordemos la Ecuacion (2)\n","\\begin{eqnarray}\n","L(w, b, \\lambda) &=& \\frac12 \\| w \\|^2 - \\sum_i \\lambda_i\n","[ y_i( w \\cdot x_i - b) - 1 ]  \\\\\n","&=& \\frac12\n","\\left ( \\sum_{i} \\lambda_i y_i x_i  \\right ) \\cdot\n","\\left ( \\sum_{i} \\lambda_i y_i x_i  \\right )\n","- \\sum_i \\lambda_i \\left [\n","    y_i  \\left (\n","        \\sum_j \\lambda_j y_j x_j \\cdot x_i -b\n","        \\right ) - 1\n","    \\right ] \\\\\n","    &=& \\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j - \\sum_i \\sum_j \\lambda_i y_i \\lambda_j y_j x_i \\cdot x_j +  \\sum_i \\lambda_i y_i b + \\sum_i \\lambda_i\n","    \\\\\n","    &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + b \\sum_i \\lambda_i y_i + \\sum_i \\lambda_i \\\\\n","    &=& -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_i \\lambda_i\n","\\end{eqnarray}\n","Queda lo siguiente\n","\n","$$L(\\lambda) = -\\frac12 \\sum_i \\sum_j \\lambda_i \\lambda_j y_i y_j x_i \\cdot x_j + \\sum_i \\lambda_i $$\n","sujeto a\n","\n","$$\\sum_i \\lambda_i y_i = 0 $$\n","\n","A este problema se conoce como el problema **dual**\n","Este problema es un problema de optimizacion cuadratica.\n","Se puede resolver por las tecnicas de\n","\n","[quadratic programming](https://en.wikipedia.org/wiki/Quadratic_programming).\n","\n","Para resolverlo tendriamos que hallar el gradiente de $L$ con respecto a $\\lambda$. Tenemos las siguientes ecuaciones\n","\n","$$ \\frac{\\partial L}{\\partial \\lambda_i} = 0 \\quad, \\quad i=1, \\cdots, m $$\n","\n","La pregunta es como hallamos $w$? De la ecuacion (3) ya conocemos\n","$\\lambda_i, x_i, y_i$ entonces conocemos $w$.\n","\n","Tarea, revisar como hallar $b$ con estos resultados.\n","\n","\n"],"metadata":{"id":"EyBBvF9EzWK7"}},{"cell_type":"markdown","source":["## Kernel Trick:\n","Sabemos que para separar datos no linealmente separables debemos mover los puntos a una hier-superficie en un espacio de mayor dimension que donde estan los datos originalmente.\n","La condicion para separar los puntos es\n","\n","$$y_i \\left (  \\sum_j \\lambda_j y_j x_j \\cdot x_i - b)  \\right ) \\ge 1$$"],"metadata":{"id":"BCHelnUD44z2"}},{"cell_type":"markdown","source":[],"metadata":{"id":"GTD5Il70AB2I"}},{"cell_type":"markdown","source":["Vemos que toca hacer los productos $x_i \\cdot x_j$ muchas veces. Estos puntos se van a mapear a un espacio de mayores dimensiones, por un \"kernel\"\n","$x_i \\mapsto \\Phi(x_i)$\n","Llamos kernel a la siguiente function\n","\n","$$k(x_i, x_j) = \\Phi(x_i) \\cdot \\Phi(x_j) $$\n","\n","Esto se va a tener que calcular muchas veces. Isabelle Guyon en 1991 invento una forma de calcular estos **kernels** de una forma rapida. A esto se llamo el **kernel trick**.\n","[enlace aca](https://en.wikipedia.org/wiki/Kernel_method).\n","Este **truco** permitio un computo mucho mas veloz de las operaciones que envuelve el metodo de SVM.\n","\n","La funcion $\\Phi$ se le llama el **feature map** (mapeo)\n","Vamos a ver una serie de ejemplos donde probamos que\n","mediante el kernel trick es mas rapido hacer los calculos.\n","\n","**Ejemplo 1**:\n","Tomemos $n=2$ (el espacio es de dos dimensiones)\n","Definamos el **feature map** como\n","\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^2 &\\to& \\mathbb{R}^3  \\\\\n","(x_1, x_2) &\\mapsto& \\Phi(x_1, x_2) = (x_1^2 , x_2^2, \\sqrt{2} x_1 x_2)\n","\\end{eqnarray}\n","De acuerdo a esto\n","\n","El truco es este:\n","$$k(x,y) = ( x \\cdot y)^2 $$\n","\n","Vamos a probar que en verdad $k(x,y) = \\Phi(x) \\cdot \\Phi(y)$\n","Veamos. Tomemos $x=(x_1, x_2)$, $y=(y_1, y_2)$, entonces\n","\n","$$k(x,y) = (x \\cdot y)^2 = \\left ( \\sum_{i=1}^2 x_i y_i   \\right )^2 =\n","x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 $$\n","\n","Comparamos esto con\n","\n","$$\\Phi(x) \\cdot \\phi(y) = (x_1^2, x_2^2, \\sqrt{2} x_1 x_2) \\cdot (y_1^2, y_2^2, \\sqrt{2} y_1 y_2) = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 $$\n","\n","\n","\n","\n","\n"],"metadata":{"id":"eXJDYkhq7FVY"}},{"cell_type":"markdown","source":["Contemos operaciones: (toy)\n","* $k(x,y)=(x \\cdot y)^2$\n","para $x \\cdot y$ se necesitan: 2 multiplicaciones y una suma.\n","Sumele un cuadrado y pare de contar.\n","* $\\Phi(x) \\cdot \\Phi(y)$. Entonces para $\\Phi(x)$, dos cuadrados y dos multiplicaciones y lo mismo para $\\Phi(y)$ y luego el producto punto se necesitan 3 multiplicaciones y 2 sumas.\n","Es mas del doble de operaciones. En la vida real no es 2 o 3 dimensiones sino muchisimas mas.\n","\n","\n","\n","    "],"metadata":{"id":"J9nqM3--_HVI"}},{"cell_type":"markdown","source":["**Ejemplo 2**:\n","Pensemos en $n=3$, y definamos\n","\n","\\begin{eqnarray}\n","\\Phi : \\mathbb{R}^3 &\\to& \\mathbb{R}^6 \\\\\n","(x_1, x_2, x_3) &\\mapsto& = (x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3)\n","\\end{eqnarray}\n","\n","El **kernel trick** es:\n","\n","$$ k(x,y)= (x \\cdot y)^2 $$\n","\n","Veamos como este computo es mucho mas rapido que el tradicional.\n","Si $x=(x_1, x_2, x_3)$, $y=(y_1, y_2, y_3)$.\n","\n","$$k(x,y) = (x \\cdot y)^2 = \\left ( \\sum_{i=1}^3 x_i y_i \\right )^2  = \\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j $$\n","\n","De otro lado\n","\n","$$\\Phi(x) \\cdot \\Phi(y) =\n","(x_1^2, x_2^2, x_3^2, \\sqrt{2} x_1 x_2, \\sqrt{2} x_1 x_3, \\sqrt{2} x_2 x_3) \\cdot\n","(y_1^2, y_2^2, y_3^2, \\sqrt{2} y_1 y_2, \\sqrt{2} y_1 y_3, \\sqrt{2} y_2 y_3) =\\sum_{i=1}^3 \\sum_{j=1}^3 x_i x_j y_i y_j\n"," $$"],"metadata":{"id":"TkgWUL10AFla"}},{"cell_type":"markdown","source":["Cuentas de calculo\n","* $k(x,y)$:\n","necesitamos 3 multiplicaciones y dos sumas para $x \\cdot y$ y un cuadrado y pare de contar.\n","\n","* $\\Phi(x) \\cdot \\Phi(y)$: 12 multiplicaciones y 6 cuadrados para $\\Phi(x)$ y $\\Phi(y)$. El product interno da 6 multiplicaciones y 5 sumas."],"metadata":{"id":"wINuUMnaBus1"}},{"cell_type":"code","source":[],"metadata":{"id":"dG-pMvgLDwT_"},"execution_count":null,"outputs":[]}]}